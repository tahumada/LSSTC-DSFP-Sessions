{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks\n",
    "========\n",
    "\n",
    "#### Version 0.1\n",
    "\n",
    "By B Nord 2018 Nov 09\n",
    "\n",
    "This notebook was developed within the [Google Collaboratory](https://colab.research.google.com/notebooks/welcome.ipynb#recent=true) framework. The original notebook can be run in a web browser, and is available [via Collaboratory](https://colab.research.google.com/drive/1dj8eLSCDbirQp6TzgD31iHXyIFqP_mKu). It has been recreated below, though we recommend you run the web-based version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Dyw4pbps6_9"
   },
   "source": [
    "# Install packages on the back end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_e3IFLX-ogPt"
   },
   "outputs": [],
   "source": [
    "# install software on the backend, which is located at \n",
    "# Google's Super Secret Sky Server in an alternate universe.\n",
    "# The backend is called a 'hosted runtime' if it is on their server.\n",
    "# A local runtime would start a colab notebook on your machine locally. \n",
    "# Think of google colab as a Google Docs version of Jupyter Notebooks\n",
    "\n",
    "# remove display of install details\n",
    "%%capture --no-display \n",
    "\n",
    "# pip install\n",
    "!pip install numpy matplotlib scipy pandas scikit-learn astropy seaborn ipython jupyter #standard install for DSFP\n",
    "!pip install keras tensorflow  # required for deep learning \n",
    "!pip install pycm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JYuP1qqMpyHZ"
   },
   "outputs": [],
   "source": [
    "# standard-ish imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import itertools\n",
    "\n",
    "# non-standard, but stable package for confusion matrices\n",
    "from pycm import ConfusionMatrix\n",
    "\n",
    "\n",
    "# neural network / machine learning packages\n",
    "from sklearn import metrics\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6U46EXbasNvb"
   },
   "source": [
    "# Let's experiment with neural networks!\n",
    "\n",
    "### Learning Objectives\n",
    "\n",
    "\n",
    "1.   Gain familiarity with \n",
    "      1.   Standard NN libraries: **Keras and Tensorflow**\n",
    "      2.   One standard NN architecture: **fully connected ('dense') networks**\n",
    "      3.   Two standard tasks performed with NNs: **Binary Classification**, **Multi-class Classification**\n",
    "      4.   Diagnostics of NNs\n",
    "            1.   History (Loss Function)\n",
    "            2.   Receiver Operator Curve and Area Under the Curve\n",
    "2.   Experience fundamental considerations, pitfalls, and strategies when training NNs\n",
    "      1.   Data set preparation (never underestimate the time required for this)\n",
    "      2.   Training set size\n",
    "      3.   Training speed and efficiency\n",
    "      5.   Model fitting (training)\n",
    "3.   Begin connecting NN functionality to data set structure and problem of interest\n",
    "\n",
    "\n",
    "### Topics not covered\n",
    "1. Class (im)balance\n",
    "2. Training set diversity\n",
    "\n",
    "\n",
    "### Notes\n",
    "Our main task will be to **Classify Handwritten Digits** (Is it a \"zero\" [0] or a \"one\" [1]?; ooooh, the suspense). This is very useful though, because it's an opportunity to separate yourself from the science of the data. [MNIST](http://yann.lecun.com/exdb/mnist/) is a benchmark data set for machine learning. Astronomy doesn't yet have machine learning-specific benchmark data sets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNzZZiHZwvI7"
   },
   "source": [
    "## Prepare the Data\n",
    "\n",
    "For this MNIST set, this is the fastest it will probably ever take you to prepare a data set. Ye have been warn-ed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ppacy9UTzAJT"
   },
   "source": [
    "### Download the data \n",
    "(ooh look it's all stored on Amazon's AWS!)\n",
    "(pssst, we're in the cloooud, it's the future!)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qwXuui6_yYBv"
   },
   "outputs": [],
   "source": [
    "# import MNIST data\n",
    "(x_train_temp, y_train_temp), (x_test_temp, y_test_temp) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "asIjm01UzUiK"
   },
   "source": [
    "### **Look** at the data\n",
    "(always do this so that you **know** what the structure is.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5L507V2Ayk3y"
   },
   "outputs": [],
   "source": [
    "# Print the shapes\n",
    "print(\"Train Data Shape:\", x_train_temp.shape)\n",
    "print(\"Test Data Shape:\", x_test_temp.shape)\n",
    "print(\"Train Label Shape:\", y_train_temp.shape)\n",
    "print(\"Test Label Shape:\", y_test_temp.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SquIrP2E0Pnz"
   },
   "source": [
    "\n",
    "*Do the shapes of 'data' and 'label' (for train and test, respectively) match? If they don't now, Keras/TF will kindly yell at you later.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "p9sOE3TI4nHR"
   },
   "outputs": [],
   "source": [
    "# Print an example\n",
    "print(\"Example:\")\n",
    "print(\"y_train[0] is the label for the 0th image, and it is a\", y_train_temp[0])\n",
    "print(\"x_train[0] is the image data, and you kind of see the pattern in the array of numbers\")\n",
    "print(x_train_temp[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FiBKC1_q4r34"
   },
   "source": [
    "*Can you see the pattern of the number in the array?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kcUsjxeSz9aF"
   },
   "outputs": [],
   "source": [
    "# Plot the data! \n",
    "f = plt.figure()\n",
    "f.add_subplot(1,2, 1)\n",
    "plt.imshow(x_train_temp[0])\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.imshow(x_train_temp[1])\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tnva-KuHjLw0"
   },
   "source": [
    "### Prepare the data\n",
    "\n",
    "Data often need to be re-shaped and normalized for ingestion into the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xPVp8HLR4LzC"
   },
   "source": [
    "#### Normalize the data\n",
    "\n",
    "The images are recast as float and normalized to one for the network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vTCyohBf4Nhp"
   },
   "outputs": [],
   "source": [
    "print(\"Before:\", np.min(x_train_temp), np.max(x_train_temp))\n",
    "x_train = x_train_temp.astype('float32')\n",
    "x_test = x_test_temp.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "y_train = y_train_temp\n",
    "y_test = y_test_temp\n",
    "print(\"After:\", np.min(x_train), np.max(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ashwVZDUAIJS"
   },
   "source": [
    "#### Reshape the data arrays: flatten for ingestion into a Dense layer.\n",
    "\n",
    "We're going to use a Dense Neural Network Architecture, which takes 1D data structures, not 2D images.\n",
    "So we need to make the input shape appropriate, so we 'flatten' it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6zzzMHpk_OgP"
   },
   "outputs": [],
   "source": [
    "# Flatten the images\n",
    "img_rows, img_cols = x_train[0].shape[0], x_train[0].shape[1] \n",
    "img_size = img_rows*img_cols\n",
    "x_train = x_train.reshape(x_train.shape[0], img_size)\n",
    "x_test = x_test.reshape(x_test.shape[0], img_size)\n",
    "print(\"New shape\", x_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3aTWoi_E6Ueh"
   },
   "source": [
    "#### Apply *one-hot encoding* to the data\n",
    "\n",
    "\n",
    "1.   Current encoding provides a literal label. For example, the label for \"3\"  is *3*.\n",
    "2.   One-hot encoding places a \"1\" in an array at the appropriate location for that datum. For example, the label \"3\" becomes *[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]*\n",
    "\n",
    "This increases the efficiency of the matrix algebra during network training and evaluation.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GIWLxZN099Bl"
   },
   "outputs": [],
   "source": [
    "# One-hot encoding\n",
    "num_classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4PO2kozw-czq"
   },
   "source": [
    "## Create a Neural Net and train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TbZq-vUT-hgd"
   },
   "source": [
    "### Decide model format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OAyMY-sG-bka"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gNOCNQft9ju4"
   },
   "source": [
    "*For later: check out the keras documentation to see what other kinds of model formats are available. What might they be useful for?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Po6px0oM-vdg"
   },
   "source": [
    "### Add layers to the model sequentially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXiW9aIx9_CM"
   },
   "outputs": [],
   "source": [
    "model.add(Dense(units=32, activation='sigmoid', input_shape=(img_size,))) # dense layer of 32 neurons\n",
    "model.add(Dense(units=num_classes, activation='softmax')) # dense layer of 'num_classes' neurons, because these are the number of options for the classification\n",
    "#model.add(BatchNormalization())\n",
    "model.summary() # look at the network output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eT9wuY042vaQ"
   },
   "source": [
    "1. *Review the model summary, noting the major features --- the meaning of each column, the flow downward, the number of parameters at the bottom.*\n",
    "2. *What's the math that leads to the number of parameters in each layer?* This is a little bit of fun math to play with. In the long run it can help understand how to debug model compilation errors, and to design custom networks better. No need to dwell here too long, but give it a try.The ingredients are \n",
    "    1. image size\n",
    "    2. number of units in the layer\n",
    "    3. number of bias elements per unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "huruvZgbDXb_"
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Select three key options\n",
    "1.   **optimizer**: the method for optimizing the weights. \"Stochastic Gradient Descent (SGD)\" is the canonical method.\n",
    "2.   **loss** function: the form of the function to encode the difference between the data's true label and the predict label.\n",
    "3.   **metric**: the function by which the model is evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HLaQvODcBzi2"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"sgd\", loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0br6HaB556O"
   },
   "source": [
    "*The optimizer is an element that I sometimes tune if I want more control over the training. Check the Keras docs for the optiosn available for the optimizer.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLDD3Iy5D-UB"
   },
   "source": [
    "### Fit (read: Train) the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Qu7XpVQFvvg"
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "batch_size = 32 # number of images per epoch\n",
    "num_epochs = 5 # number of epochs\n",
    "validation_split = 0.8 # fraction of the training set that is for validation only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SYAh_2hODQTd"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=num_epochs, \n",
    "                    validation_split=validation_split, \n",
    "                    verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vzLU9LOF4srp"
   },
   "source": [
    "*\"Exerciiiiiiisee the neurons! This Network is cleeeah!\"*\n",
    "\n",
    "(bonus points if you know the 90's movie, from which this is a near-quote.)\n",
    "\n",
    "* How well does the training go if we don't normalize the data to be between 0 and 1.0?\n",
    "* What is the effect of batch_size on how long it takes the network to achieve 80% accuracy?\n",
    "* What happens if you increase the validation set fraction (and thus reduce the training set size)? Try going really low on the training set size. Can you still get 80% accuracy? \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phnTXhesOdtT"
   },
   "source": [
    "## Diagnostics!\n",
    "\n",
    "Because neural networks are such complex objects, and in this era of experimentation, diagnostics (both standard and tailored) are critical for training NN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NLR2O6j3F6k-"
   },
   "source": [
    "### Evaluate overall model efficacy\n",
    "\n",
    "Evaluate model on training and test data and compare. This provides summary values that are equivalent to the final value in the accuracy/loss history plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBKTaHUADR0p"
   },
   "outputs": [],
   "source": [
    "loss_train, acc_train  = model.evaluate(x_train, y_train, verbose=False)\n",
    "loss_test, acc_test  = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(f'Train acc/loss: {acc_train:.3}, {loss_train:.3}')\n",
    "print(f'Test acc/loss: {acc_test:.3}, {loss_test:.3}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SPtsnJscR-oA"
   },
   "source": [
    "### Predict train and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OI7KWUnATIT2"
   },
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(x_train, verbose=True)\n",
    "y_pred_test = model.predict(x_test,verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqD61qqNOqWa"
   },
   "source": [
    "### Plot accuracy and loss as a function of epochs (equivalently training time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hH7SJntRDTtB"
   },
   "outputs": [],
   "source": [
    "# set up figure\n",
    "f = plt.figure(figsize=(12,5))\n",
    "f.add_subplot(1,2, 1)\n",
    "\n",
    "# plot accuracy as a function of epoch\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "\n",
    "# plot loss as a function of epoch\n",
    "f.add_subplot(1,2, 2)\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['training', 'validation'], loc='best')\n",
    "plt.show(block=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vEn4dyybOzy4"
   },
   "source": [
    "### Define Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r9fvhucyOyol"
   },
   "outputs": [],
   "source": [
    "# Function: Convert from categorical back to numerical value\n",
    "def convert_to_index(array_categorical):\n",
    "  array_index = [np.argmax(array_temp) for array_temp in array_categorical]\n",
    "  return array_index\n",
    "\n",
    "def plot_confusion_matrix(cm,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function modified to plots the ConfusionMatrix object.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \n",
    "    Code Reference : \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \n",
    "    This script is derived from PyCM repository: https://github.com/sepandhaghighi/pycm\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    plt_cm = []\n",
    "    for i in cm.classes :\n",
    "        row=[]\n",
    "        for j in cm.classes:\n",
    "            row.append(cm.table[i][j])\n",
    "        plt_cm.append(row)\n",
    "    plt_cm = np.array(plt_cm)\n",
    "    if normalize:\n",
    "        plt_cm = plt_cm.astype('float') / plt_cm.sum(axis=1)[:, np.newaxis]     \n",
    "    plt.imshow(plt_cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(cm.classes))\n",
    "    plt.xticks(tick_marks, cm.classes, rotation=45)\n",
    "    plt.yticks(tick_marks, cm.classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = plt_cm.max() / 2.\n",
    "    for i, j in itertools.product(range(plt_cm.shape[0]), range(plt_cm.shape[1])):\n",
    "        plt.text(j, i, format(plt_cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if plt_cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('Actual')\n",
    "    plt.xlabel('Predict')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UYWSXEsuYmui"
   },
   "source": [
    "### Plot Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PuMtsloj227g"
   },
   "outputs": [],
   "source": [
    "# apply conversion function to data\n",
    "y_test_ind = convert_to_index(y_test)\n",
    "y_pred_test_ind = convert_to_index(y_pred_test)\n",
    "\n",
    "# compute confusion matrix\n",
    "cm_test = ConfusionMatrix(y_test_ind, y_pred_test_ind)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "# plot confusion matrix result\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cm_test,title='cm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9XhDciBP6uI8"
   },
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wJMcw33I6hpc"
   },
   "source": [
    "---\n",
    "###  Problem 1: Binary data\n",
    "\n",
    "* Problem 3a: Generate a binary data sets from the MNIST data set.\n",
    "* Problem 3b: Prepare the data set in the same way as the multi-class scenario, but apprioriately for only two classes.\n",
    "* Problem 3c: Re-train the neural network for the binary classification problem.\n",
    "* Problem 3d: Perform all the diagnostics again.\n",
    "* Problem 3e: Use sklearn to generate a Receiver Operator Curve (ROC) and the associated AUC. \n",
    "* Problem 3f: Create a new diagnostic that shows examples of the images and how they were classified. For a given architecture and training, plot four columns of images, five rows down. Each row is just another example of something that is classified. The four columns are \n",
    "  * True positive\n",
    "  * False positive\n",
    "  * True negative\n",
    "  * True postiive\n",
    " \n",
    " So this will be a grid of images that are examples of classifications. This is important to give a sense of how well the network is classifying different types of objects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zfAK2PH0weG9"
   },
   "source": [
    "---\n",
    "### Problem 2: How does the following metrics scale with the data set size? (Constructing a learning curve)\n",
    "\n",
    "* time required to perform 5 epochs.\n",
    "* accuracy and loss at final step\n",
    "\n",
    "Some advice for this problem:\n",
    "  * create a function or functions that can wrap around the NN architecture, and then output key results.\n",
    "  * watch out for network initialization. If you don't redefine the network *model* each time, the weights will be preserved from the most recent training.\n",
    "\n",
    "Sub-problems \n",
    "1. Try training and evaluation for a number of well-spaced data set sizes and report on the pattern you see. Plot a \"learning curve\": the loss (or accuracy) as a function of the training set data size. This is an important step in judging what you need for your training set, and in assessing the feasibility of the application.\n",
    "2. Do you see any change a function data size? If not, go back and train the original network for more epochs. When does the loss plateau? Use that epoch for the \n",
    "3. Does keras have any model function fit parameters that automatically decide when it will stop? Try using that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vAOMdAmlGmg5"
   },
   "source": [
    "---\n",
    "\n",
    "### Problem 3: How do the following factors and metrics scale with the number of neurons in the first layer?\n",
    "\n",
    "1. time required to perform 10 epochs\n",
    "2. accuracy and loss at final step\n",
    "\n",
    "\n",
    "When developing an NN model, we think about the balance between *accuracy*, *computation time*, and *network complexity*. This exercise is intended to draw out some of the patterns in the confluence of those aspects. The goal of this problem is to produce plots that show general trends.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wOUoPD-m39ck"
   },
   "source": [
    "---\n",
    "### Problem 4: Apply this analysis to the Fashion MNIST data set\n",
    "\n",
    "*tip: you can use a very similar procedure to load the data as you did for MNIST*\n",
    "\n",
    "*hint: \"fashion_mnist\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KPFDLZ8tK4fb"
   },
   "source": [
    "---\n",
    "### Problem 6: Apply this to some mock strong lensing data\n",
    "\n",
    "We have mock strong lensing data made with [Lenspop](https://github.com/tcollett/LensPop)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dYr9tMQ7V2zs"
   },
   "source": [
    "#### Download from cloud storage to your remote runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YQcDcbB2--sG"
   },
   "outputs": [],
   "source": [
    "!gsutil cp -r gs://lsst-dsfp2018/stronglenses ./\n",
    "!ls ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EHJ6nq4oV8lh"
   },
   "source": [
    "#### Load the numpy binary files into numpy arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OdadIX9aWBxz"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8sLS2yU18l9R"
   },
   "source": [
    "---\n",
    "### Problem 7: Toying around: make data that looks like your problem.\n",
    "\n",
    "MNIST is great for learning and in some cases benchmarking, but it's not the best for physics and astronomy problems. In our questions, there's a physical underpinning, there are first principles that underly the images we make. So, when developing a neural network, it can be very useful to generate some data that morphologically looks like your astro data. This can help in developing the architecture before deploying on more complicated data, as well as diagnosing systematics in the more complicated data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LetsGetNetworking.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
