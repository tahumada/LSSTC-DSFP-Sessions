{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Supervised Machine Learning Model\n",
    "\n",
    "The objective of this hands-on activity is to create and evaluate a Real-Bogus classifier using ZTF alert data.  We will use the same data from the Day 2 clustering exercise (see [that notebook](https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/blob/master/Session7/Day2/Clustering-Astronomical-Sources.ipynb) to download the data).  \n",
    "\n",
    "1. Load data\n",
    "2. Examine features\n",
    "3. Curate a test and training set\n",
    "4. Train classifiers\n",
    "5. Compare the performance of different learning algorithms\n",
    "\n",
    "#### What's Not Covered\n",
    "\n",
    "There are many topics to cover, and due to time constraints, we cannot cover them all.  Omitted is a discussion of [cross validation](http://scikit-learn.org/stable/modules/cross_validation.html) and [hyperparameter tuning](http://scikit-learn.org/stable/modules/grid_search.html).  I encourage you to click through and read those articles by sklearn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0a. Imports\n",
    "\n",
    "These are all the imports that will be used in this notebook.  All should be available in the DSFP conda environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import Imputer\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0b. Data Location\n",
    "\n",
    "Please specify paths for the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F_META = '../Day2/dsfp_ztf_meta.npy'\n",
    "F_FEATS = '../Day2/dsfp_ztf_feats.npy'\n",
    "D_STAMPS = '../Day2/dsfp_ztf_png_stamps'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data\n",
    "\n",
    "Please note that I have made some adjustments to the data.  \n",
    "\n",
    "- I have created a dataframe (feats_df) from feats_np\n",
    "- I have also dropped columns from feats_df.  Some were irrelevant to classification, and some contained a lot of NaNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_np = np.load(F_META)\n",
    "feats_np = np.load(F_FEATS)\n",
    "\n",
    "COL_NAMES = ['diffmaglim', 'magpsf', 'sigmapsf', 'chipsf', 'magap', 'sigmagap',\n",
    "             'distnr', 'magnr', 'sigmagnr', 'chinr', 'sharpnr', 'sky',\n",
    "             'magdiff', 'fwhm', 'classtar', 'mindtoedge', 'magfromlim', 'seeratio',\n",
    "             'aimage', 'bimage', 'aimagerat', 'bimagerat', 'elong', 'nneg',\n",
    "             'nbad', 'ssdistnr', 'ssmagnr', 'sumrat', 'magapbig', 'sigmagapbig',\n",
    "             'ndethist', 'ncovhist', 'jdstarthist', 'jdendhist', 'scorr', 'label']\n",
    "\n",
    "# NOTE FROM Umaa: I've decided to eliminate the following features. Dropping them.\n",
    "#\n",
    "COL_TO_DROP = ['ndethist', 'ncovhist', 'jdstarthist', 'jdendhist', \n",
    "               'distnr', 'magnr', 'sigmagnr', 'chinr', 'sharpnr', \n",
    "               'classtar', 'ssdistnr', 'ssmagnr', 'aimagerat', 'bimagerat', \n",
    "               'magapbig', 'sigmagapbig', 'scorr']\n",
    "feats_df = pd.DataFrame(data=feats_np, index=meta_np['candid'], columns=COL_NAMES)\n",
    "print(\"There are {} columns left.\".format(len(feats_df.columns)))\n",
    "print(\"They are: {}\".format(list(feats_df.columns)))\n",
    "feats_df.drop(columns=COL_TO_DROP, inplace=True) \n",
    "feats_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: How many real and bogus examples are in this labeled set\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Plot Features\n",
    "\n",
    "Examine each individual feature. You may use the subroutine below, or code of your own devising. Note the features that are continuous vs categorial, and those that have outliers.  There are certain features that have sentinel values.  You may wish to view some features on a log scale.\n",
    "\n",
    "#### Question: Which features seem to have pathologies? Which features should we exclude?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Histogram a Single Feature\n",
    "#\n",
    "def plot_rb_hists(df, colname, bins=100, xscale='linear', yscale='linear'):\n",
    "    mask_real = feats_df['label'] == 1\n",
    "    mask_bogus = ~mask_real\n",
    "    plt.hist(df[colname][mask_real], bins, color='b', alpha=0.5, density=True)\n",
    "    plt.hist(df[colname][mask_bogus], bins, color='r', alpha=0.5, density=True)\n",
    "    plt.xscale(xscale)\n",
    "    plt.yscale(yscale)\n",
    "    plt.title(colname)\n",
    "    plt.show()\n",
    "\n",
    "# INSTRUCTION: Plot the individual features.  \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer: \n",
    "\n",
    "- 'nbad' and 'nneg' are discrete features\n",
    "- 'seeratio' has a sentinel value of -999\n",
    "- There are some features that have high ranges than others.  For classifiers that are sensitive to scaling, we will need to scale the data\n",
    "- There does not appear to be good separability between real and bogus on any individual features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Curate a Test and Training Set\n",
    "\n",
    "We need to reserve some of our labeled data for evaluation. This means we must split up the labeled data we have into the set used for training (training set), and the set used for evaluation (test set). Ideally, the distribution of real and bogus examples in both the training and test sets are roughly identical. One can use sklearn.model_selection.train_test_split and use the stratify option.\n",
    "\n",
    "For ZTF data, we split the training and test data by date. That way repeat observations from the same night (which might be nearly identical) cannot be split into the training and test set, and artificially inflate test performance. Also, due to the change in survey objectives, it's possible that the test set features have drifted away from the training sets.\n",
    "\n",
    "Provided is nid.npy which contains the Night IDs for ZTF. Split on nid=550 (July 5, 2018). This should leave you with roughly 500 examples in your test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats_plus_label = np.array(feats_df)\n",
    "nids = meta_np['nid']\n",
    "\n",
    "# INSTRUCTION: nid.npy contains the nids for this labeled data.\n",
    "# Split the data into separate data structures for train/test data at nid=500.\n",
    "# Verify that you have at least 500 reals in your test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train a Classifier\n",
    "\n",
    "#### Part 1: Separate Labels from the Features\n",
    "\n",
    "Now store the labels separately from the features.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Separate the labels from the features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 2. Scaling Data\n",
    "\n",
    "With missing values handled, you're closing to training your classifiers.  However, because distance metrics can be sensitive to the scale of your data (e.g., some features span large numeric ranges, but others don't), it is important to normalize data within a standard range such as (0, 1) or with z-score normalization (scaling to unit mean and variance).  Fortunately, sklearn also makes this quite easy.  Please review sklearn's [preprocessing](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module options, specifically StandardScaler which corresponds to z-score normalization and MinMaxScaler.  Please implement one.  \n",
    "\n",
    "FYI - Neural networks and Support Vector Machines (SVM) are sensitive to the scale of the data.  Decision trees (and therefore Random Forests) are not (but it doesn't hurt to use scaled data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: Re-scale your data using either the MinMaxScaler or StandardScaler from sklearn\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 3. Train Classifiers\n",
    "\n",
    "Import a few classifiers and build models on your training data.  Some suggestions include a [Support Vector Machine](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC), [Random Forest](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier), [Neural Net](http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html#sklearn.neural_network.MLPClassifier), [NaiveBayes](http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) and [K-Nearest Neighbor](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html#sklearn.neighbors.KNeighborsClassifier).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "knn3 = KNeighborsClassifier(3),\n",
    "svml = SVC(kernel=\"linear\", C=0.025, probability=True)\n",
    "svmr = SVC(gamma=2, C=1, probability=True)\n",
    "dtre = DecisionTreeClassifier()\n",
    "rafo = RandomForestClassifier(n_estimators=100)\n",
    "nnet = MLPClassifier(alpha=1)\n",
    "naiv = GaussianNB()\n",
    "\n",
    "# INSTRUCTION: Train at least three classifiers and run on your test data. Here's an example to get your started.  \n",
    "# \n",
    "\n",
    "\n",
    "rafo.fit(train_feats, train_labels)\n",
    "rafo_scores = rafo.predict_proba(test_feats)[:,1]\n",
    "\n",
    "\n",
    "# INSTRUCTION: Print out the following metrics per classifier into a table: accuracy, auc, f1_score, etc.\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Part 4. Plot Real and Bogus Score Distributions\n",
    "\n",
    "Another way to test performance is to plot a histogram of the test set RB scores, comparing the distributions of the labeled reals vs. boguses.  The scores of the reals should be close to 1, while the scores of the boguses should be closer to 0.  The more separated the distribution of scores, the better performing your classifier is.\n",
    "\n",
    "Compare the score distributions of the classifiers you've trained.  Trying displaying as a cumulative distribution rather than straight histogram.  \n",
    "\n",
    "*Optional:* What would the decision thresholds be at the  5% false negative rate (FNR)?  What would the decision threshold be at the 1% false positive rate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INSTRUCTION: create masks for the real and bogus examples of the test set\n",
    "\n",
    "\n",
    "# INSTRUCTION: First compare the classifiers' scores on the test reals only\n",
    "#\n",
    "\n",
    "scores_list = []\n",
    "legends = ['Classifier 1', 'Classifier 2', 'Classifier 3'] # REPLACE ME\n",
    "colors = ['g', 'b', 'r'] \n",
    "rbbins = np.arange(0,1,0.001)\n",
    "\n",
    "# Comparison on real examples\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "for i, scores in enumerate(scores_list):\n",
    "    # ax.hist() - COMPLETE ME\n",
    "ax.set_xlabel('RB Score')\n",
    "ax.set_ylabel('Count')\n",
    "ax.set_xbound(0, 1)\n",
    "ax.legend(legends, loc=4)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Now perform the same comparison on bogus examples\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refining\n",
    "\n",
    "Now it's time to go back and start experimenting with different parameters.  If you have time, create a validation set and perform a Grid Search over a set of possible hyperparameters.  Compare different classifier results using the histograms above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
