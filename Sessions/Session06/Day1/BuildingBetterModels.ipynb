{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Better Models for Inference:\n",
    "\n",
    "# How to construct practical models for existing tools\n",
    "\n",
    "In this notebook, we will walk through fitting an observed optical light curve from a tidal disruption event (TDE), the destruction and accretion of a star by a supermassive black hole, using two different approaches.\n",
    "\n",
    "As mentioned in the lecture, there are different kinds of models one can apply to a set of data. A code I have written, MOSFiT, is an attempt to provide a framework for building models that can be used within other optimizers/samplers. While MOSFiT can run independently with its own built-in samplers, in the notebook below we will simple be using it as a \"black box\" function for use in external optimization routines.\n",
    "\n",
    "Our first approach will be using the `tde` model in MOSFiT. This model uses both interpolation tables and integrations, making an analytical derivative not available. Our second approach will be to construct a simple analytical function to fit the same data. We will then be comparing performance, both in terms of the quality of the resulting solution, but also the speed by which the solution was computed, and in how we relate our solution to what transpired in this event.\n",
    "\n",
    "* * *\n",
    "\n",
    "By J Guillochon (Harvard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We will be mostly using the mosfit package and scipy routines. Both are available via conda.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import mosfit\n",
    "import time\n",
    "\n",
    "# Disable \"retina\" line below if your monitor doesn't support it.\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1) Fitting data with a blackbox model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first cell, we load the data of a particularly well-sampled tidal disruption event from the Pan-STARRS survey, PS1-10jh. This even is notable because it was caught on the rise, peak, and decay, with solid cadence.\n",
    "\n",
    "The datafile can be aquired from https://northwestern.app.box.com/s/ekwpbf8ufe1ivogpxq9yyex302zx0t96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the data from the Open Supernova Catalog.\n",
    "# Note: if loading the data doesn't work, I have a local copy.\n",
    "my_printer = mosfit.printer.Printer(quiet=True)  # just to avoid spamming from MOSFiT routines.\n",
    "my_fetcher = mosfit.fetcher.Fetcher()\n",
    "\n",
    "fetched = my_fetcher.fetch('PS1-10jh.json')[0]\n",
    "\n",
    "my_model = mosfit.model.Model(model='tde', printer=my_printer)\n",
    "fetched_data = my_fetcher.load_data(fetched)\n",
    "my_model.load_data(\n",
    "    fetched_data, event_name=fetched['name'],\n",
    "    exclude_bands=['u', 'r', 'i', 'z', 'F225W', 'NUV'],  # ignore all bands but g when computing ln_likelihood.\n",
    "    smooth_times=100,  # for plotting smooth fits later\n",
    "    user_fixed_parameters=['covariance'])  # don't use GP objective function.\n",
    "\n",
    "# Generate 100 random parameter realizations.\n",
    "x = np.random.rand(100, my_model.get_num_free_parameters())\n",
    "\n",
    "# Compute time per function call.\n",
    "start_time = time.time()\n",
    "ln_likes = [my_model.ln_likelihood(xx) for xx in x]\n",
    "stop_time = time.time()\n",
    "\n",
    "print('{}s per function call.'.format((stop_time - start_time)/100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1a**\n",
    "\n",
    "First, let's visualize the data we have downloaded. MOSFiT loads data in a format conforming to the OAC schema specification, which is a JSON dictionary where the top level of the structure is each event's name. The code snippet below will load a JSON dictionary for the event in question, plot the full time series of photometric data (with error bars) within the `photometry` key below.\n",
    "\n",
    "*Hint: The photometry is a mixture of different data types, and not every entry has the same set of keys. Optical/UV/IR photometry will always have a `band` key. Ignore upper limits (indicated with the `upperlimit` attribute). Use the `.get()` function liberally, and make sure everything is a `float`!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "times = []\n",
    "mags = []\n",
    "errs = []\n",
    "for x in fetched_data[fetched['name']]['photometry']:\n",
    "    # complete\n",
    "\n",
    "plt.errorbar(times, mags, yerr=errs, fmt='o')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1b**\n",
    "\n",
    "We know what the data looks like, and we've loaded a model that can be used to fit the data which computes a likelihood. Let's minimize the parameters of this model using various `scipy.optimize` routines. Note that since we are trying to **maximize** the likelihood, we have constructed a wrapper function around `ln_likelihood`, `my_func`, to reverse its sign, and to handle bad function evaluations.\n",
    "\n",
    "Most `optimize` routines in `scipy` require a derivative. Since we don't have this available, `scipy` must construct an approximate one, unless the method doesn't require a gradient to be computed (like `differential_evolution`). For this first sub-problem, optimize `my_func` using `differential_evolution`.\n",
    "\n",
    "*Hints: Each variable is bounded to the range (0, 1), but problems can arise if an optimizer attempts to compute values outside or right at the boundaries. Therefore, it is recommended to use a bounded optimizer in `scipy`, where the bounds do **not include** 0 or 1.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "def my_func(x):\n",
    "    try:\n",
    "        fx = -float(my_model.ln_likelihood(x))\n",
    "    except:\n",
    "        fx = np.inf\n",
    "    return fx\n",
    "\n",
    "eps = 0.00001\n",
    "bounds = # complete\n",
    "results = scipy.optimize.differential_evolution(  # complete\n",
    "best_x = results.x\n",
    "\n",
    "print('All done! Best score: `{}`.'.format(-results.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might take a while; try to limit the execution time of the above to ~5 minutes by playing with the `maxiter` and similar options of the `scipy` optimizers.\n",
    "\n",
    "Once the above has finished evaluating, compare the score you got to your neighbors. Is there a significant difference between your scores? Let's plot your result against the data.\n",
    "\n",
    "Model output is provided in the `output` object below, the format is a dictionary of arrays of the same length. The times of observation are in the `times` array, and magnitudes are in the `model_observations` array.\n",
    "\n",
    "*Hint: `times` is given relative to the time of the first detection, so add `min(times)` to your time to overplot onto the data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = my_model.run_stack(best_x, root='output')\n",
    "\n",
    "for ti, t in enumerate(output['times']):\n",
    "    # complete\n",
    "\n",
    "plt.errorbar(  # complete\n",
    "plt.plot(  # complete\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1c**\n",
    "\n",
    "Try optimizing the same function using **another** minimization routine in scipy that can take a derivative as an input (examples: `L-BFGS-B`, `SLSQP`, `basinhopping`, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = scipy.optimize.basinhopping( #complete\n",
    "best_x = results.x\n",
    "\n",
    "print('All done! Best score: `{}`.'.format(-results.fun))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, plot the results of the above minimization alongside your original `differential_evolution` solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete\n",
    "\n",
    "for ti, t in enumerate(output['times']):\n",
    "    # complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this process, **some** of you **might** have gotten a good solution with a runtime of a few minutes. In practice, guaranteed convergence to the best solution can take a very long time. Whats more, we only attempted to find the **best** solution available, usually we are interested in posterior distributions that (usually) include the best solution. These take even longer to compute (tens of thousands of function evaluations for a problem of this size)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "\n",
    "Now, we'll construct our own simpler model that is **analytically differentiable**. We'll partly motivate the shape of this function based upon our knowledge of how tidal disruption events are expected to behave theoretically, but there will be limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define a function that loosely mimics a tidal disruption event's temporal evolution. Tidal disruption events rise exponentially, then decay as a power-law. Canonically, the decay rate is -5/3, and the rise is very unconstrained, being mediated by complicated dynamics and accretion physics that have yet to be determined. So, we use the following agnostic form,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$L(t) = L_0 \\left(1-e^{-\\frac{t}{t_0}}\\right)^{\\alpha } \\left(\\frac{t}{t_0}\\right)^{-\\beta }.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tidal disruption observations are usually reported in magnitudes, thus the expression we'll actually compare against observations is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m(t) = m_0 - 2.5 \\log_{10}\\left[\\left(1-e^{-\\frac{t}{t_0}}\\right)^{\\alpha } \\left(\\frac{t}{t_0}\\right)^{-\\beta }\\right].$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the likelihood, we want to subtract the above from the observations. We'll make the gross assumption that the color of a tidal disruption is constant in time (which turns out to not be a terrible assumption) and thus $L_{\\rm g}(t) \\propto L(t)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our likelihood function will be defined as the product of the squares of differences between our model and observation,\n",
    "\n",
    "$$p = \\prod_i \\frac{1}{\\sqrt{2\\pi (\\sigma_i^2 + \\sigma^2)}} \\left[\\frac{\\left(m_{{\\rm g}, i} - \\bar{m}_{{\\rm g}, i}\\right)^2}{2\\left(\\sigma_i^2 + \\sigma^2\\right)}\\right],$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and thus our log likelihood is the sum of these squared differences, plus a separate sum for the variances,\n",
    "\n",
    "$$\\log p = -\\frac{1}{2} \\left\\{\\sum_i \\left[\\frac{\\left(m_{{\\rm g}, i} - \\bar{m}_{{\\rm g}, i}\\right)^2}{\\sigma_i^2 + \\sigma^2}\\right] + \\log 2\\pi \\left(\\sigma_i^2 + \\sigma^2\\right)\\right\\}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2a**\n",
    "\n",
    "Write the above expression as a python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analytic_f(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 2a **\n",
    "\n",
    "Compute the derivative for $\\log p$ (above expression) with respect to $m_0$ (Mathematica might be helpful here). Below are the derivatives for the other five free parameters $\\alpha$, $\\beta$, $\\tau$, $t_0$, and $\\sigma$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial\\log p}{\\partial \\alpha} &= \\sum_i -\\frac{5 \\log \\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right) \\left\\{\\log (100) (\\bar{m}-m_0)+5 \\log \\left[\\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right)^{\\alpha } \\left(\\frac{t+t_0}{\\tau }\\right)^{-\\beta }\\right]\\right\\}}{4 \\log ^2(10) \\left(\\sigma_i^2+\\sigma^2\\right)}\\\\\n",
    "\\frac{\\partial\\log p}{\\partial \\beta} &= \\sum_i \\frac{5 \\log \\left(\\frac{t+t_0}{\\tau }\\right) \\left\\{\\log (100) (\\bar{m}-m_0)+5 \\log \\left[\\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right)^{\\alpha } \\left(\\frac{t+t_0}{\\tau }\\right)^{-\\beta }\\right]\\right\\}}{4 \\log ^2(10) \\left(\\sigma_i^2+\\sigma^2\\right)}\\\\\n",
    "\\frac{\\partial\\log p}{\\partial \\tau} &= \\sum_i \\frac{5 \\left(\\alpha  (t+t_0)-\\beta  \\tau  \\left(e^{\\frac{t+t_0}{\\tau }}-1\\right)\\right) \\left(\\log (100) (\\bar{m}-m_0)+5 \\log \\left(\\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right)^{\\alpha } \\left(\\frac{t+t_0}{\\tau }\\right)^{-\\beta }\\right)\\right)}{4 \\tau ^2 \\log ^2(10) \\left(\\sigma_i^2 + \\sigma^2\\right) \\left(e^{\\frac{t+t_0}{\\tau }}-1\\right)}\\\\\n",
    "\\frac{\\partial\\log p}{\\partial t_0} &= \\sum_i \\frac{5 \\left(\\alpha  (t+t_0)-\\beta  \\tau  \\left(e^{\\frac{t+t_0}{\\tau }}-1\\right)\\right) \\left(\\log (100) (m_0-\\bar{m})-5 \\log \\left(\\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right)^{\\alpha } \\left(\\frac{t+t_0}{\\tau }\\right)^{-\\beta }\\right)\\right)}{4 \\tau  \\log ^2(10) \\left(\\sigma_i^2+\\sigma^2\\right) (t+t_0) \\left(e^{\\frac{t+t_0}{\\tau }}-1\\right)}\\\\\n",
    "\\frac{\\partial\\log p}{\\partial \\sigma} &= \\sum_i \\frac{\\sigma_i}{4 \\log ^2(10) \\left(\\sigma_i^2+\\sigma^2\\right)^2} \\left\\{5 \\log \\left[\\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right)^{\\alpha } \\left(\\frac{t+t_0}{\\tau }\\right)^{-\\beta }\\right]\\right.\\\\&\\times\\left.\\left(4 \\log (10) (\\bar{m}-m_0)+5 \\log \\left[\\left(1-e^{-\\frac{t+t_0}{\\tau }}\\right)^{\\alpha } \\left(\\frac{t+t_0}{\\tau }\\right)^{-\\beta }\\right]\\right)+4 \\log ^2(10) \\left((m_0-\\bar{m})^2-\\sigma_i^2-\\sigma^2\\right)\\right\\}\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2b**\n",
    "\n",
    "We now need to write each of these derivatives as python functions. These functions should accept a single vector argument `x` with length equal to the number of free parameters, plus a vector $t$ (the times of the observation) vector $m$ (the magnitudes of each observation), and finally errors $v$ (the measurement error of each observation). Again, 5 of the 6 parameters have already been written for you (you must provide the 6th)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dlogp_dalpha(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    derivs = np.sum(-5.0 * np.log(1.0 - np.exp(-(t + t0) / tau)) * (np.log(100.0) * (m - m0) + 5.0 * lf(\n",
    "        alpha, beta, tau, t0, t)) / (4.0 * np.log(10.0) ** 2 * (v ** 2 + sigma ** 2)))\n",
    "    \n",
    "    return derivs\n",
    "\n",
    "def dlogp_dbeta(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    derivs = np.sum(5.0 * np.log((t + t0) / tau) * (np.log(100.0) * (m - m0) + 5.0 * lf(\n",
    "        alpha, beta, tau, t0, t)) / (4.0 * np.log(10.0) ** 2 * (v ** 2 + sigma ** 2)))\n",
    "    \n",
    "    return derivs\n",
    "\n",
    "def dlogp_dtau(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    derivs = np.sum(5.0 * (alpha * (t + t0) - beta * tau * (np.exp((t + t0)/tau) - 1.0)) * (\n",
    "        np.log(100.0) * (m - m0) + 5.0 * lf(\n",
    "            alpha, beta, tau, t0, t)) / (4.0 * tau ** 2 * np.log(10.0) ** 2 * (v ** 2 + sigma ** 2) * (\n",
    "        np.exp((t + t0)/tau) - 1.0)))\n",
    "    \n",
    "    return derivs\n",
    "\n",
    "def dlogp_dt0(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    derivs = np.sum(-5.0 * (alpha * (t + t0) - beta * tau * (np.exp((t + t0)/tau) - 1.0)) * (\n",
    "        np.log(100.0) * (m - m0) + 5.0 * lf(\n",
    "            alpha, beta, tau, t0, t)) / (4.0 * tau * (t + t0) * np.log(10.0) ** 2 * (v ** 2 + sigma ** 2) * (\n",
    "        np.exp((t + t0)/tau) - 1.0)))\n",
    "    \n",
    "    return derivs\n",
    "\n",
    "def dlogp_dsigma(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    derivs = np.sum(sigma/(4.0 * np.log(10.0) ** 2 * (v**2 + sigma**2)**2) * (5.0 * lf(\n",
    "        alpha, beta, tau, t0, t) * (4.0 * np.log(10.0) * (m - m0) + 5.0 * lf(\n",
    "        alpha, beta, tau, t0, t)) + 4.0 * np.log(10.0) ** 2 * ((m0 - m) ** 2 - v ** 2 - sigma ** 2)))\n",
    "    \n",
    "    return derivs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2c**\n",
    "\n",
    "Make sure the derivatives for all the above function are consistent with the finite differences of the objective function. How large is the error for an `eps = 1e-8` (the default distance used when no Jacobian is provided)? Make a histogram for each derivative of these errors for 100 random parameter combinations drawn from the bounds (in other words, six plots with 100 samples each).\n",
    "\n",
    "*Hint: you will likely have to remove some nans.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up bounds/test parameters.\n",
    "abounds = [\n",
    "    [0.0, 30.0],\n",
    "    [0.1, 50.0],\n",
    "    [0.1, 10.0],\n",
    "    [0.1, 200.0],\n",
    "    [0.1, 200.0],\n",
    "    [0.001, 1.0]\n",
    "]\n",
    "\n",
    "test_times = [1.0, 20.0]\n",
    "test_mags = [23.0, 19.0]\n",
    "test_errs = [0.1, 0.2]\n",
    "\n",
    "# Draw a random parameter combo to test with.\n",
    "n = 100\n",
    "dm0_diff = np.zeros(n)\n",
    "for p in range(n):\n",
    "    test_x = [abounds[i][0] + x * (abounds[i][1] - abounds[i][0]) for i, x in enumerate(np.random.rand(6))]\n",
    "\n",
    "    # Check that every derivative expression is close to finite difference.\n",
    "    teps = 1e-10\n",
    "\n",
    "    xp = list(test_x)\n",
    "    xp[0] += teps\n",
    "    exactd = dlogp_dm0(test_x, test_times, test_mags, test_errs)\n",
    "    dm0_diff[p] = (exactd - (\n",
    "        analytic_f(test_x, test_times, test_mags, test_errs) - analytic_f(\n",
    "            xp, test_times, test_mags, test_errs)) / teps) / exactd\n",
    "\n",
    "    # complete for rest of parameters\n",
    "\n",
    "plt.subplot(321)\n",
    "plt.hist(dm0_diff[~np.isnan(dm0_diff)]);\n",
    "# complete for rest of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which derivatives seem to have the least accurate finite differences? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Now we have an analytical function with analytical derivatives that should be accurate to near-machine precision. \n",
    "\n",
    "** Problem 3a **\n",
    "\n",
    "First, let's optimize our function using `differential_evolution`, as we did above with the MOSFiT output, without using the derivatives we have constructed (as `differential_evolution` does not use them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "times0 = np.array(times) - min(times)\n",
    "results = scipy.optimize.differential_evolution(# complete\n",
    "best_x = results.x\n",
    "\n",
    "print('All done! Best score: `{}`, took `{}` function evaluations.'.format(results.fun, results.nfev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How good is the approximation?\n",
    "\n",
    "** Problem 3b **\n",
    "\n",
    "Let's minimize using the `basinhopping` algorithm now, again not using our derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times0 = np.array(times) - min(times)\n",
    "results = scipy.optimize.basinhopping(  # complete\n",
    "best_x = results.x\n",
    "\n",
    "print('All done! Best score: `{}`, took `{}` function evaluations.'.format(results.fun, results.nfev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm, which depends on finite differencing, seems to have taken more function evaluations than `differential_evolution`. Let's give it some help: construct a jacobian using the derivative functions defined above.\n",
    "\n",
    "*Hint: mind the sign of the Jacobian since we are **minimizing** the function.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jac(x, tt, mm, vv):\n",
    "    m0, alpha, beta, tau, t0, sigma = tuple(x)\n",
    "    t = np.array(tt)\n",
    "    m = np.array(mm)\n",
    "    v = np.array(vv)\n",
    "    \n",
    "    # complete\n",
    "    \n",
    "    return jac\n",
    "\n",
    "results = scipy.optimize.basinhopping(  # complete\n",
    "best_x = results.x\n",
    "\n",
    "print('All done! Best score: `{}`, took `{}` function evaluations.'.format(results.fun, results.nfev))\n",
    "\n",
    "# plot the resulting fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all went well, the jacobian version of the optimization should have taken ~8x fewer function evaluations. But is it faster?\n",
    "\n",
    "**Problem 3c**\n",
    "\n",
    "Compute how many times the Jacobian was called, and estimate how expensive the Jacobian is to compute relative to the objective function. How does this compare to the run that only used finite differencing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global jcount\n",
    "jcount = 0\n",
    "\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you think of a reason why using the Jacobian version may be preferable, even if it is slower?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem(s)\n",
    "\n",
    "**Select one (or more) of the following:**\n",
    "\n",
    "- Fit a different event using either MOSFiT or the analytical formula. Any supernova can be loaded by name from the internet via the `fetch` method of the `Fetcher` class. Examples: SN1987A, SN2011fe, PTF09ge. If you are using the analytical model, exclude all but one of the bands in the dataset.\n",
    "- Optimize the Jacobian function to reuse common functions that are shared between each derivative component (example: $1 - e^{((t + t_0)/\\tau)}$ appears frequently in the expressions, it only needs to be computed once).\n",
    "- Sample the posterior in a Monte Carlo framework (using priors of your choice). Samplers like `emcee` are versatile and work even when derivatives aren't available, but we **do** have derivatives, so more powerful methods like Hamiltonian MCMC are available to us. A simple HMC for our purposes is available via `pip install pyhmc`, see the README for instructions on how to construct the input function: https://github.com/rmcgibbo/pyhmc. Plot the resulting samples using the `corner` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyhmc import hmc\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concluding questions\n",
    "\n",
    "- What did we learn from fitting the analytic model about the **physics** of the disruption?\n",
    "- Does the analytic function have utility for generating simulated data?\n",
    "- Where else might the analytic function have a use?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
