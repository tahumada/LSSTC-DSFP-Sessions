{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoencodersBlank.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a Simple Autoencoder\n",
        "\n",
        "By: V. Ashley Villar (PSU)\n",
        "\n",
        "In this problem set, we will use Pytorch to learn a latent space for the same galaxy image dataset we have previously played with."
      ],
      "metadata": {
        "id": "33ZuUfnAun8W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9TS8c_-lg55"
      },
      "outputs": [],
      "source": [
        "!pip install astronn\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from astroNN.datasets import load_galaxy10\n",
        "from astroNN.datasets.galaxy10 import galaxy10cls_lookup\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1a: Understanding our dataset...again\n",
        "\n",
        "Our data is a little too big for us to train an autoencoder in ~1 minute. Let's lower the resolution of our images and only keep one filter. Plot an example of the lower resolution galaxies.\n",
        "\n",
        "Next, flatten each image into a 1D array. Then rescale the flux of the images such that the mean is 0 and the standard deviation is 1. "
      ],
      "metadata": {
        "id": "UjgT74iXu-KP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Readin the data\n",
        "images, labels = load_galaxy10()\n",
        "labels = labels.astype(np.float32)\n",
        "images = images.astype(np.float32)\n",
        "images = torch.tensor(images)\n",
        "labels = torch.tensor(labels)\n",
        "# Cut down the resolution of the images!!! What is this line doing in words?\n",
        "images = images[:,::6,::6,1]\n",
        "\n",
        "#Plot an example image here\n",
        "\n",
        "#Flatten images here\n",
        "\n",
        "#Normalize the flux of the images here\n"
      ],
      "metadata": {
        "id": "M3Gg9-a_lp-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 1b. \n",
        "Split the training and test set with a 66/33 split."
      ],
      "metadata": {
        "id": "X6-GabIrvqUc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "xXVYGbcMlzed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 2: Understanding the Autoencoder\n",
        "\n",
        "Below is sample of an autoencoder, built in Pytorch. Describe the code line-by-line with a partner. Add another hidden layer before and after the encoded (latent) layer (this will be a total of 2 new layers). Choose the appropriate activation function for this regression problem. Make all of the activation functions the same."
      ],
      "metadata": {
        "id": "PoOK468Nv4dA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Autoencoder(torch.nn.Module):\n",
        "      # this defines the model\n",
        "        def __init__(self, input_size, hidden_size, hidden_inner, encoded_size):\n",
        "            super(Autoencoder, self).__init__()\n",
        "            print(input_size,hidden_size,encoded_size)\n",
        "            self.input_size = input_size\n",
        "            self.hidden_size  = hidden_size\n",
        "            self.encoded_size = encoded_size\n",
        "            self.hidden_inner = hidden_inner\n",
        "            self.hiddenlayer1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
        "            # ADD A LAYER HERE\n",
        "            self.encodedlayer = torch.nn.Linear(self.hidden_inner, self.encoded_size)\n",
        "            self.hiddenlayer3 = torch.nn.Linear(self.encoded_size, self.hidden_inner)\n",
        "            # ADD A LAYER HERE\n",
        "            self.outputlayer = torch.nn.Linear(self.hidden_size, self.input_size)\n",
        "            # some nonlinear options\n",
        "            self.sigmoid = torch.nn.Sigmoid()\n",
        "            self.softmax = torch.nn.Softmax()\n",
        "            self.relu = torch.nn.ReLU()\n",
        "        def forward(self, x):\n",
        "            layer1 = self.hiddenlayer1(x)\n",
        "            activation1 = self.ACTIVATION?(layer1)\n",
        "            layer2 = self.hiddenlayer2(activation1)\n",
        "            activation2 = self.ACTIVATION?(layer2)\n",
        "            layer3 = self.encodedlayer(activation2)\n",
        "            activation3 = self.ACTIVATION?(layer3)\n",
        "            layer4 = self.hiddenlayer3(activation3)\n",
        "            activation4 = self.ACTIVATION?(layer4)\n",
        "            layer5 = self.hiddenlayer4(activation4)\n",
        "            activation5 = self.ACTIVATION?(layer5)\n",
        "            layer6 = self.outputlayer(activation5)\n",
        "            output = self.ACTIVATION?(layer6)\n",
        "\n",
        "            # Why do I have two outputs?\n",
        "            return output, layer3"
      ],
      "metadata": {
        "id": "oR5y59MfnFF2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 3. Training\n",
        "\n",
        "This is going to be a lot of guess-and-check. You've been warned. In this block, we will train the autoencoder. Add a plotting function into the training.\n",
        "\n",
        "Note that instead of cross-entropy, we use the \"mean-square-error\" loss. Switch between SGD and Adam optimized. Which seems to work better? Optimize the `learning-rate` parameter and do *not* change other parameters, like momentum.\n",
        "\n",
        "Write a piece of code to run train_model for 10 epochs. Play with the size of each hidden layer and encoded layer. When you feel you've found a reasonable learning rate, up this to 100 (or even 500 if you're patient) epochs. Hint: You want to find MSE~0.25."
      ],
      "metadata": {
        "id": "jSr4XllBX-Nb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "def train_model(training_data,test_data, model):\n",
        "  # define the optimization\n",
        "  criterion = torch.nn.MSELoss()\n",
        "\n",
        "  # Choose between these two optimizers\n",
        "  #optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
        "  #optimizer = torch.optim.Adam(model.parameters(), lr=0.1,weight_decay=1e-6)\n",
        "\n",
        "  for epoch in range(500):\n",
        "    # clear the gradient\n",
        "    optimizer.zero_grad()\n",
        "    # compute the model output\n",
        "    myoutput, encodings_train = model(training_data)\n",
        "    # calculate loss\n",
        "    loss = criterion(myoutput, training_data)\n",
        "    # credit assignment\n",
        "    loss.backward()\n",
        "    # update model weights\n",
        "    optimizer.step()\n",
        "    # Add a plot of the loss vs epoch for the test and training sets here\n",
        "\n",
        "#Do your training here!!\n",
        "hidden_size_1 = 100\n",
        "hidden_size_2 = 50\n",
        "encoded_size = 10 \n",
        "model = Autoencoder(np.shape(images_train[0])[0],hidden_size_1,hidden_size_2,encoded_size)\n",
        "train_model(images_train, images_test, model)"
      ],
      "metadata": {
        "id": "1qYsd2nlo8Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4a. Understand our Results\n",
        "\n",
        "Plot an image (remember you will need to reshape it to a 14x14 grid) with imshow, and plot the autoencoder output for the same galaxy. Try plotting the difference between the two. What does your algorithm do well reconstructing? Are there certain features which it fails to reproduce?"
      ],
      "metadata": {
        "id": "1zWYWVOuYvP4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Make an image of the original image\n",
        "\n",
        "#Make an image of its reconstruction\n",
        "\n",
        "#Make an image of (original - reconstruction)"
      ],
      "metadata": {
        "id": "08bqiTkdB1Xx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem 4b. \n",
        "\n",
        "Make a scatter plot of two of the 10 latent space dimensions. Do you notice any interesting correlations between different subsets of the latent space? Any interesting clustering?\n",
        "\n",
        "Try color coding each point by the galaxy label using `plt.scatter`"
      ],
      "metadata": {
        "id": "X5YQIsfC7C9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Scatter plot between two dimensions of the latent space\n",
        "#Try coloring the points"
      ],
      "metadata": {
        "id": "LZcz3_OwBvAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Problem 5a Playing with the Latent Space\n",
        "\n",
        "Create a random forest classifier to classiy each galaxy using only your latent space."
      ],
      "metadata": {
        "id": "BL5yNDE37TCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "clf = RandomForestClassifier(...)\n",
        "\n",
        "clf.fit(...)\n",
        "new_labels = clf.predict(...)\n",
        "\n",
        "cm = confusion_matrix(labels_test,new_labels,normalize='true')\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "disp.plot()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BtBA7LDkznuO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bonus Problem 5b Playing with the Latent Space\n",
        "\n",
        "Create an isolation forest to find the most anomalous galaxies. Made a cumulative distribution plot showing the anomaly scores of each class of galaxies. Which ones are the most anomalous? Why do you think that is?"
      ],
      "metadata": {
        "id": "Xeo_dn8j7fNd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf = IsolationForest(...).fit(encodings)\n",
        "scores = -clf.score_samples(encodings) #I am taking the negative because the lowest score is actually the weirdest, which I don't like...\n",
        "\n",
        "#Plot an image of the weirdest galazy!\n",
        "\n",
        "#This plots the cumulative distribution\n",
        "def cdf(x, label='',plot=True, *args, **kwargs):\n",
        "    x, y = sorted(x), np.arange(len(x)) / len(x)\n",
        "    return plt.plot(x, y, *args, **kwargs, label=label) if plot else (x, y)\n",
        "\n",
        "ulabels = np.unique(labels)\n",
        "for ulabel in ulabels:\n",
        "  gind = np.where(labels==ulabel)\n",
        "  cdf(...)\n"
      ],
      "metadata": {
        "id": "SjWqRJjgOTKf"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}