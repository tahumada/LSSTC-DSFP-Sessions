{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to scikit-learn\n",
    "\n",
    "##### Version 0.1\n",
    "\n",
    "***\n",
    "By AA Miller (Northwestern/CIERA)\n",
    "\n",
    "18 Sep 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Broadly speaking, machine-learning methods constitute a diverse collection of data-driven algorithms designed to classify/characterize/analyze sources in multi-dimensional spaces. The topics and studies that fall under the umbrella of machine learning is growing, and there is no good catch-all definition. We cannot cover all possible algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Today we will focus on the [`scikit-learn`](https://scikit-learn.org/stable/index.html) python library, which provides a nice interface to build a wide variety of machine learning models. As we will see, `scikit-learn` makes machine learning \"easy.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 1) Data with `scikit-learn`\n",
    "\n",
    "In 4 lines we can build a complex classifier for the famous [iris data set](https://en.wikipedia.org/wiki/Iris_flower_data_set).\n",
    "\n",
    "    from sklearn import datasets\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    iris = datasets.load_iris()\n",
    "    RFclf = RandomForestClassifier().fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Those 4 lines of code have constructed a model that is superior to any system of hard cuts that we could have encoded while looking at the multidimensional space. \n",
    "\n",
    "It's also fast â€“ execute the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# execute example code here\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "iris = datasets.load_iris()\n",
    "RFclf = RandomForestClassifier().fit(iris.data, iris.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generally speaking, the procedure for `scikit-learn` is uniform across all machine-learning algorithms. Models are accessed via the various modules (`ensemble`, `SVM`, `neighbors`, etc), with user-defined tuning parameters. The features (or data) for the models are stored in a 2D array, `X`, with rows representing individual sources and columns representing the corresponding feature values.$^\\dagger$  In cases where there is a known classification or scalar value (typically supervised methods), this information is stored in a 1D array `y`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$^\\dagger$In a minority of cases, `X`, represents a similarity or distance matrix where each entry represents the distance to every other source in the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Unsupervised models are fit by calling `.fit(X)` and supervised models are fit by calling `.fit(X, y)`. In both cases, predictions for new observations, `Xnew`, can be obtained by calling `.predict(Xnew)`. Those are the basics and beyond that, the details are algorithm specific, so ...\n",
    "\n",
    "read the docs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1a** What is the pythonic type of `iris`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You likely haven't encountered a `scikit-learn Bunch` before. It's essentially the same as a dictionary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1b** What are the keys of iris?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Most importantly, iris contains `data` and `target` values. These are all you need for `scikit-learn`, though the feature and target names and description are useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1c** What is the shape and content of the `iris` data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(np.shape(# complete\n",
    "print(# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The data is a 2d array with shape 150 x 4.\n",
    "\n",
    "We said earlier that each row represents a source and each column a \"feature.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1d**\n",
    "\n",
    "What is the first feature in the `iris` data set? What units is it measured in?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1e** \n",
    "\n",
    "What is the shape and content of the `iris` target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(np.shape( # complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1f**\n",
    "\n",
    "What are the names of class 0, 1, and 2 in the `iris` target?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "An important lesson from this week - worry about the data!\n",
    "\n",
    "If you are worried about the data, then you should look at the data. This is actually an important aspect of applying machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1g** \n",
    "\n",
    "Make a scatter plot showing sepal length vs. sepal width for the iris data set. Color the points according to their respective classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(# complete\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will return to the `iris` data set later in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 2) Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At its core, `scitkit-learn` is designed to help you, the user, easily fit models to data. \n",
    "\n",
    "To demonstrate this we will start with a familiar example - linear regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2a**\n",
    "\n",
    "Simulate data drawn from a linear relationship with Gaussian scatter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(2012)\n",
    "n_obs = 25\n",
    "x = np.random.uniform(0,100, n_obs)\n",
    "y_true = 2.3*x + 14\n",
    "y_obs = y_true + np.random.normal(0, 15, n_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2b**\n",
    "\n",
    "Plot the simulated data. Overplot the true relation from which the data are drawn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The [`sklearn.linear_model`](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.linear_model) has a [`LinearRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) class.\n",
    "\n",
    "The \"standard\" procedure in `scikit-learn` is to generate the model by creating an instance of the relevant class object (e.g., `reg_model = LinearRegression()`). \n",
    "\n",
    "The model is then trained using the aforementioned training and label arrays, `X` and `y`, with the `.fit()` method. \n",
    "\n",
    "Finally, predictions can be made using the `.predict()` method (note - the precise syntax for this can vary depending on the model being used). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2c** \n",
    "\n",
    "Generate an instance of the `LinearRegression` class. Call this instance `reg_model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "reg_model = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2d** \n",
    "\n",
    "Fit the model to the training data `x` and `y_obs`. \n",
    "\n",
    "*Hint* - the standard feature array in `scikit-learn` is 2D, so you will need to convert `x` to a 2D array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = # complete\n",
    "reg_model.fit( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2e**\n",
    "\n",
    "Output the best-fit parameters from the model (these are stored in the `.coef_` and `.intercept_` attributes). \n",
    "\n",
    "Overplot the best-fit line on top of the observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(f'The best fit is y = ' # complete\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot( # complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2f**\n",
    "\n",
    "Fit for the model parameters using some other method (e.g., `numpy`, `scipy`, linear algebra), and compare the model parameters to those found with `scikit-learn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "\n",
    "print('The best-fit model is: y = ' # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The results are identical!\n",
    "\n",
    "Under the hood, polynomial regression is just linear algebra, so it doesn't matter which library you use to solve the problem. Both `numpy` and `scikit-learn` can handle multi-dimensional input and even account for uncertainties on the observations via input weights. For this problem, it does not matter which library you use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Problem 3) Linear Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now examine why linear regression does not work for classification. \n",
    "\n",
    "Suppose we have a method to measure the mass of a galaxy (but this method has noise). We would like to separate the galaxies into two classes \"normal\" and \"dwarf\", where dwarf galaxies are those with a mass below 60. We could simulate such a dataset as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Note* - this is very much a \"toy\" data set to be used purely for illustration. While we use some familiar terms, nothing about this is truly physical, which is why we do not include units, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(1938)\n",
    "n_obs = 200\n",
    "mass = np.random.normal(65, 20, size=n_obs)\n",
    "mass[mass > 70] *= (mass[mass > 70]-60)/60*5\n",
    "y = (mass > 60).astype(bool)\n",
    "obs_mass = mass + np.random.normal(0, 7, size=n_obs)\n",
    "X = obs_mass.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the previous cell we have simulated 200 galaxies, most of which are normal with masses > 60. We use this information to define our class vector `y`, such that all normal galaxies have a class of 1 and all dwarf galaxies have a class of 0. Finally, we simulate our observations `X`, which adds Gaussian noise to the true mass measurements. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3a**\n",
    "\n",
    "Build a classifier using `LinearRegression()`. Find the best fit line for the data. All sources with an extrapolated best-fit value > 0.5 are classified as normal, and everything else is considered a dwarf. \n",
    "\n",
    "From the training data, how accurate is this model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "reg_model = # complete\n",
    "reg_model.fit( # complete\n",
    "\n",
    "y_pred = # complete\n",
    "\n",
    "n_incorrect = np.sum((y_pred - y)**2)\n",
    "\n",
    "accuracy = (n_obs - n_incorrect)/n_obs\n",
    "\n",
    "print(f'This model has an accuracy of {accuracy:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3b**\n",
    "\n",
    "To understand the short-comings of this model, plot the data. Overplot the best-fit linear regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot( # complete\n",
    "# complete\n",
    "# complete\n",
    "# complete\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As we can see in the above plot, every galaxy with $\\mathrm{mass} \\gtrsim 78$ is classified as normal. \n",
    "\n",
    "It is also clear from the plot that linear regression is not a particularly good approach to this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3c**\n",
    "\n",
    "Build a classifier via \"hard cut\" and assess it's accuracy. \n",
    "\n",
    "Find the maximum mass for dwarf galaxies in the data set. Classify all sources with a mass less than this as dwarf galaxies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mass_cut = # complete\n",
    "\n",
    "y_pred = # complete\n",
    "# complete\n",
    "\n",
    "\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This represents a slight improvement over the linear regression model. (Note - if you did the reverse and used the normal galaxies to determine the mass cut, you would get similar results)\n",
    "\n",
    "At this point you must be thinking - \"There has to be a better way!\"\n",
    "\n",
    "Fortunately, there is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Logistic Regression](https://en.wikipedia.org/wiki/Logistic_regression) - the \"hello, world\" of machine learning classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "(and also one of the absolutely worst named algorithms ever, as logistic regression is used for *classification* and not *regression*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Briefly, logistic regression is used to understand the relationship between a dependent variable (normal vs. dwarf) and one or more independent variables (mass, but could also be mass, star formation rate, metallicity, and so on) by estimating probabilities via the logistic function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The logistic function: \n",
    "\n",
    "$$ p(x) = \\frac{1}{1 + e^{-(x - \\mu)/s}}$$\n",
    "\n",
    "can be rewritten as: \n",
    "\n",
    "$$ p(x) = \\frac{1}{1 + e^{-(B_0 + B_1 x)}}$$\n",
    "\n",
    "which recasts the problem as having a slope and intercept, similar to the linear regression problem that we worked on previously. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3d**\n",
    "\n",
    "Execute the cell below to see the logistic function. \n",
    "\n",
    "How might this be useful for our galaxy classification problem? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-10,10,1000)\n",
    "p_x = 1/(1 + np.exp(-x))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, p_x)\n",
    "ax.set_xlabel('x', fontsize=14)\n",
    "ax.set_ylabel('p(x)', fontsize=14)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "*write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3e**\n",
    "\n",
    "Create an instance of the [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) class from the `sklearn.linear_model` module. \n",
    "\n",
    "Fit the model to the simulated galaxy data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = # complete\n",
    "logreg.fit( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3f**\n",
    "\n",
    "Assess the accuracy of the Logistic Regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = # complete\n",
    "\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This model shows significant improvement over our alternative methods! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3g**\n",
    "\n",
    "Overplot the best fit logistic regression model on the data. \n",
    "\n",
    "*Hint* â€“ the `.intercept_` and `.coef_` attributes can be used to determine the argument for the logistic function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot( # complete\n",
    "\n",
    "# complete\n",
    "    \n",
    "# complete\n",
    "    \n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can see that the logistic regression model is clearly superior to the linear regression model. The model is not perfect, but no model could be in the presence of noise. \n",
    "\n",
    "We also see that the probability cut ($p \\approx 0.5 \\;\\mathrm{at\\;mass} \\approx 61.5$) is much closer to the true answer of 60. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The previous example used a simple toy data set that was easy to visualize. But logistic regression can be extended to include multiple features, and more than two classes (in which case the problem is a multinomial one rather than a binomial one). \n",
    "\n",
    "This optimization happens entirely under the hood with `scikit-learn`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3f**\n",
    "\n",
    "Fit a logistic regression model to the iris data set. \n",
    "\n",
    "Assess the accuracy of this model. \n",
    "\n",
    "*Hint* â€“ [`accuracy_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) from `sklearn.metrics` will simplify your calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "logreg = LogisticRegression( # complete\n",
    "# complete\n",
    "# complete\n",
    "    \n",
    "print('The accuracy of this model is {}' # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In conclustion, today we have learned the basics for the `scikit-learn` library. \n",
    "\n",
    "We have also learned how the logistic regression algorithm can be very useful for classification problems. Logistic regression is often used as a starting point when building machine learning models: when faced with a new data set, logistic regression can be applied with minimal tuning to get a sense of whether machine learning could be helpful in classifying the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "I also want to end with two caveats: \n",
    "\n",
    "1. Logistic regression is very powerful, but like all machine learning techniques, it also has limitations. For example, logistic regression can lead to \"interpretable\" results (such as the mean value where the curve transitions from 0 to 1), but these interpretations will not work if the features are correlated. \n",
    "\n",
    "2. The error rates (1 - accuracy) measured throughout this notebook are the \"training error.\" This is not a particularly good way to measure the efficacy of a model as we will discuss in further detail later today. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Challenge Problem\n",
    "\n",
    "Pick any classification model from the `scikit-learn` library and measure its accuracy when applied to the iris data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
