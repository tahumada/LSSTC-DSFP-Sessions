{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f46f24f",
   "metadata": {},
   "source": [
    "# Priors, Likelihoods, Posteriors, and All That: Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d14e99",
   "metadata": {},
   "source": [
    "## Problem 1: The Curse of Dimensionality; or why self driving cars are hard.\n",
    "\n",
    "### 1a) Sampling in low dimensions\n",
    "\n",
    "Generate a sample of 100 randomly distributed points inside a 2D square. \n",
    "\n",
    "### 1b) Distances in low dimensions\n",
    "\n",
    "Calculate the pairwise distances between all of the points, in d-dimensions, for two points x_i and y_i, this quantity is\n",
    "\n",
    "$$ |x - y| = \\sqrt{\\Sigma^d (x_i - y_i)^2} $$\n",
    "\n",
    "### 1c) Distribution of distances in low dimensions \n",
    "\n",
    "Plot the distribution of these distances. Do you notice anything interesting? Is a \"special value\" picked out?\n",
    "\n",
    "### 1d) Extending to d-dimensions\n",
    "\n",
    "Now, let's consider the difference between picking a point located in a d-dimensional sphere vs in a d-dimensional cube. To do this, calculate and plot the the difference between the volume of a cube with unit-length sides and the volume of a unit-radius sphere as the dimension d of the space increases. Possibly helpful formula: \n",
    "\n",
    "$$ Sphere: V_d = \\frac{\\pi^{d/2}}{\\frac{d}{2} \\Gamma(\\frac{d}{2})} $$\n",
    "\n",
    "$$ Cube: V_d = L^d $$\n",
    "\n",
    "The proof of this formula is left as an exercise to the reader. (actually, a link to the proof for the curious will be provided in the solutions version of this notebook.\n",
    "\n",
    "### 1e) Sampling in d-dimensions\n",
    "\n",
    "Now, consider the problem of sampling in high-dimensions. If we use a uniform prior on every parameter, our expectation is that we will well sample the parameter space simply by picking in a \"uniform way\". Calculate the ratio of volume contained in an annulus between $R$ and $R - \\epsilon$ for d = 1-20 and plot this. Interpret your result in terms of a sampling problem. Are uniform priors a good choice in higher dimensions? Why or why not?\n",
    "\n",
    "### 1f) Further reading: \"Soap bubbles\" or \"moldy spheres\". \n",
    "\n",
    "A recent blogpost that discusses an extension of this idea to a Gaussian distribution (rather than uniform unit ball distributions) is here:  https://www.inference.vc/high-dimensional-gaussian-distributions-are-soap-bubble/ \n",
    "\n",
    "give it a short read. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4344f4d",
   "metadata": {},
   "source": [
    "## Problem 2: Measuring distances - when priors matter\n",
    "\n",
    "A famous example where your choice of prior matters is found in parallax measurements. In this problem, we will explore this. \n",
    "\n",
    "(Thank you Adrian Price-Whelan for contributing this problem!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efe4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "\n",
    "import astropy.units as u\n",
    "import astropy.table as at\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "\n",
    "import arviz as az\n",
    "from astroquery.gaia import Gaia\n",
    "import pymc as pm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86dbbe75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query Gaia \n",
    "# Cohort 5 students should confirm that they understand the database syntax here based on \n",
    "# the material from Session 15.\n",
    "# Cohort 6 students - we will cover this in Session 21 : ) \n",
    "\n",
    "job = Gaia.launch_job(\n",
    "    \"SELECT TOP 1 * FROM gaiadr3.gaia_source WHERE parallax_over_error > 3 and parallax_over_error < 4\"\n",
    ")\n",
    "data = at.QTable(job.get_results().filled())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60690f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the data - make sure you understand its structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca2d049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's consider two choices of prior. First, a uniform prior, and then a truncated normal. \n",
    "\n",
    "def uniform_space_density_logp(L):\n",
    "    def lpdf(r):\n",
    "        return (2*pm.math.log(r)) - 3*pm.math.log(L) - pm.math.log(2) - r/L\n",
    "    return lpdf\n",
    "\n",
    "with pm.Model() as model:\n",
    "    r = pm.DensityDist(\n",
    "        'r', \n",
    "        logp=uniform_space_density_logp(4.), \n",
    "        initval=1.,\n",
    "        transform=pm.distributions.transforms.LogTransform()\n",
    "    )\n",
    "    plx = pm.Normal(\n",
    "        'plx',\n",
    "        mu=1/r,\n",
    "        sigma= \n",
    "        observed=\n",
    "    )\n",
    "    \n",
    "    samples_usd = pm.sample(tune=2000, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6933b2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    r = pm.TruncatedNormal(\n",
    "        'r', \n",
    "        mu=2.,\n",
    "        sigma=1.,\n",
    "        lower=0,\n",
    "        initval=1.,\n",
    "        transform=pm.distributions.transforms.LogTransform()\n",
    "    )\n",
    "    plx = pm.Normal(\n",
    "        'plx',\n",
    "        mu=1/r,\n",
    "        sigma=\n",
    "        observed=\n",
    "    )\n",
    "    \n",
    "    samples_truncnorm = pm.sample(tune=2000, draws=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe69ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9872bf9",
   "metadata": {},
   "source": [
    "## Problem 3: Not everything is Gaussian\n",
    "\n",
    "In the lecture and challenge problems, we've emphasized the importance of the Gaussian distribution. This distribution is very common, but not universal. In a (perhaps apocryphal story) this problem was given to first year students at Cambridge in the 1980s. It concerns estimating the distance to an off-shore lighthouse based on the timing of pulses. A canonical statement of the problem is, \n",
    "\n",
    "\"A lighthouse is situated at unknown coordinates $x_0,y_0$ with respect to a straight coastline y=0. It sends a series of N flashes in random directions, and these are recorded on the coastline at positions $x_i$.\" \n",
    "\n",
    "### Problem 3-0) Draw the picture. \n",
    "\n",
    "With your partner, draw the picture.\n",
    "\n",
    "### Problem 3a) Prior \n",
    "\n",
    "Write down a prior for the $x_0, y_0$ position of the lighthouse. Implement a python function that returns a uniform probability (or if you'd like, another prior) for $x_0, y_0$. Hint: It is easier to write the prior in terms of the angle $\\theta$ between the line connecting the lighthouse to the shore and the direction in which the pulse is emitted. \n",
    "\n",
    "### Problem 3b) Likelihood \n",
    "\n",
    "Now, we need to determine the form of the likelihood. If you following the hint in 3a), we want to turn a function of the data (shoreline positions, x) in terms of the angle ($\\theta$). First, write down the relationship between the $x_0$ position of the lighthouse, the data x, and the angle ($\\theta$). \n",
    "\n",
    "### Problem 3c) Generate some data\n",
    "\n",
    "In order to produce a Bayesian estimate of the x-y position, we'll need some data. \n",
    "\n",
    "### Problem 3d) Posterior\n",
    "\n",
    "Using Bayes' theorem, write down the posterior. Implement a grid search function to calculate the posterior for the x-y position of the lighthouse. \n",
    "\n",
    "\n",
    "### Problem 3e) [Optional] Challenge: Is your likelihood a Gaussian? What is special about your likelihood?\n",
    "\n",
    "One property of a Gaussian that makes it \"special\" is that it is the maximum entropy distribution for for finite first and second moments. Calculate the first and second moments of your likelihood distribution. What makes this distribution special? Generate some example plots of your likelihood function and compare to a Gaussian distribution over the same range. Are these curves the same? What is different about them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cec290e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.random import uniform \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c380434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea0f57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unknown true values of the position \n",
    "\n",
    "x_0 = 10\n",
    "y_0  = 30\n",
    "\n",
    "# generate data\n",
    "\n",
    "def make_data(x, y, thetas): \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f13e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_posterior(x_0, y_0, x):\n",
    "    \n",
    "    #you write this \n",
    "    \n",
    "    # why can you jump to the posterior?\n",
    "    \n",
    "    return log_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a63a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(x_0_grid, y_0_grid, datum): \n",
    "    # You write this\n",
    "    return posterior_grid "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad9a03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the data and calculate the posterior on the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8feeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for data in xs: \n",
    "    posterior += grid_search(x_0_grid, y_0_grid, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab974b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize your results \n",
    "\n",
    "plt.imshow(np.exp(), extent = (, , ,), origin = 'lower', cmap = 'plasma')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968ee7b5",
   "metadata": {},
   "source": [
    "## [Pen & Paper] Challenge Problem 1: The Gaussian Derivation of the Central Distribution\n",
    "\n",
    "In the lecture, we see two derivations of the Central distribution based on physical arguments. The first, spatial homogeneity, arose to solve a problem in astronomy, while the second based on convolutions of distributions (and \"additive physical processes\" arose in condensed matter physics and electrical engineering. In this problem, we'll study another derivation that also arose in the fitting of the orbits of the planets. \n",
    "\n",
    "### Maximum Likelihood Estimates\n",
    "\n",
    "Begin by reviewing the discussion in Adam's lecture yesterday of the \"maximum likelihood estimate\" of a parameter. Then, write a general expression for the maximum of the log-likelihood. Then re-express your MLE statmeent in terms of a function $g'(\\hat{\\theta} - x$), where $\\hat{\\theta}$ is your MLE estimate for the parameter $\\theta$ and $g(\\theta-x) = log f(x_i | \\theta)$ is your log-likelihood.\n",
    "\n",
    "### The arithmetic mean and roots of the MLE\n",
    "\n",
    "Now assume that the MLE must be given by the arithmetic mean of the observations, \n",
    "\n",
    "$$ \\hat{\\theta} = \\bar{x} = \\frac{1}{n+1} \\Sigma_{i=0}^n x_i $$ and consider a simple sample. This sample should have one observation $x_0 = (n+1)(\\theta - x)$. Now compute $\\hat{\\theta}$ and $\\hat{\\theta} - x_0$. What is the value of $g'(\\hat{\\theta} - x$)? Is this symmetric or anti-symmetric?\n",
    "\n",
    "### Functional equations\n",
    "\n",
    "Given your expression for $g'(\\hat{\\theta} - x)$, do some functional analysis. What are the possible functional forms of $g(u)$? Then plug them into your original MLE expression. You will find that \n",
    "\n",
    "$$ f(x|\\theta) = \\sqrt{\\frac{\\alpha}{2\\pi}} \\exp \\left[ -  \\frac{1}{2} \\alpha (x-\\theta)^2 \\right]$$\n",
    "\n",
    "As noted in the lecture, the historical importance of this result is that the assumption that $\\hat{\\theta} = \\bar{x}$ provides a theoretical basis for the intuitive notion that errors cancel. This put to rest a long running argument about the nature of additive errors and justifies much of what we assume as a matter of course."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca00147",
   "metadata": {},
   "source": [
    "## [Hybrid] Challenge Problem 2: Deriving the Central Limit Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0ba69e",
   "metadata": {},
   "source": [
    "If you consult the wikipedia page on the central limit theorem, you will find the following definition,\n",
    "\n",
    "If $X_1, X_2,...,X_n$ are random samples drawn from a population with mean $\\mu$ and variance $\\sigma^2$, and if $\\bar{X}_n$ is the sample mean of the first n samples, then the limiting form of the distribution $Z = \\frac{(\\bar{X}_n - \\mu)}{\\sigma_{\\bar{X}_n}}$ with standard error of the mean, $\\sigma_{\\bar{X}_n} = \\sigma/n$ is normally distributed.\n",
    "\n",
    "However, in fact there are many central limit theorems for different distributions. A more general way to approach CLTs is via a form that may be more intuitive as a Bayesian who thinks more about distributions rather than estimators. \n",
    "\n",
    "In this form, a statement of the CLT is that, \n",
    "\n",
    "If you have a large number of distributions, labelled $f_i = (f_1, f_2, ... f_n)$ and take their convolution $F^{*} = f_1 * f_2 * ... * f_n$, that $F^{*} \\rightarrow \\mathscr{N}$ as $n \\rightarrow \\infty$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3fd126",
   "metadata": {},
   "source": [
    "### [Computational] Challenge Problem 2a) Convolutions of distributions\n",
    "\n",
    "Begin by computing and visualizing the convolution of n-arbitrary distributions. Look up statistical tests for normality and prove to yourself that the resulting distribution really is normal (and becomes increasingly normal as $n \\rightarrow \\infty$. Try some pathological cases. Can you break the CLT? What caveats should we add to our version of the CLT?\n",
    "\n",
    "### [Pen & Paper] Challenge Problem 2b) Proof with Fourier Transforms and Cumulants\n",
    "\n",
    "Now that you've convinced yourself of the CLT by convolving distributions, prove the CLT using pen & paper. One way to do this is by considering\n",
    "\n",
    "$$ \\phi(\\alpha) = \\int_{-\\infty}^{\\infty} f(x) \\exp \\left[ i \\alpha x \\right] $$ \n",
    "\n",
    "then repeating convolution of the function f(x) yields,\n",
    "\n",
    "$$ h_n (y) = f * f * ... * f = \\frac{1}{2\\pi} \\int_{-\\infty}^{\\infty} dy \\phi(y)^n \\exp \\left[ i \\alpha y \\right] $$ \n",
    "\n",
    "Now, complete the proof by first considering the quantity $\\phi(\\alpha)^n$ and performing a cumulant expansion of this quantity. After plugging in, perform the integral. You will find tha the resulting distribution is,\n",
    "\n",
    "$$ h_n (y) = \\frac{1}{\\sqrt{2 \\pi n \\sigma^2}} exp \\left( - \\frac{(y - n <x> x)^2}{2 n \\sigma^2} \\right)$$\n",
    "\n",
    "which proves the CLT. Confirm that the pen and paper derivation respects the conditions you worked out in 2a) for the CLT to be valid."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
