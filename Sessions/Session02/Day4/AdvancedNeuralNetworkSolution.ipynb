{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing convolutional and recurrent neural networks\n",
    "By Brett Naul (UC Berkeley)\n",
    "\n",
    "In this exercise we'll explore how to implement basic convolutional and recurrent neural networks for classifying image and sequence data. The networks we'll see here differ from state-of-the-art classification techniques mostly in scale: we'll be training smaller networks on small datasets, whereas more powerful classifiers are much deeper, contain more complicated connections between layers, and are trained using enormous quantities of data.\n",
    "\n",
    "*Based on various notebooks from the [`keras` examples](https://github.com/fchollet/keras/tree/master/examples) repository.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports / plotting configuration\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_context('poster')\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "np.random.seed(13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Change this to `'tensorflow'` if you prefer\n",
    "backend = 'theano'\n",
    "config = {'image_dim_ordering': 'tf', 'epsilon': 1e-07, \n",
    "          'floatx': 'float32', 'backend': backend}\n",
    "!mkdir -p ~/.keras\n",
    "with open(os.path.expanduser('~/.keras/keras.json'), 'w') as f:\n",
    "    json.dump(config, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!pip install -q keras-tqdm  # Install Jupyter-friendly progress bar "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Fully-connected network for digit recognition\n",
    "Handwritten digit recognition is one of the most famous neural network applications; even before the recent advances in \"deep learning,\" neural networks were able to achieve excellent performance for this problem and have been used in real-world applications for many years.\n",
    "\n",
    "The canonical digit recognition example uses the so-called MNIST dataset, which consists of 60,000 training and 10,000 test examples. Each consists of a 28x28 black and white image of a handwritten digit. First we'll load the MNIST dataset and visualize the first few training examples (this may take a minute or two the first time you download the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "\n",
    "nrow = 28; ncol = 28; nb_classes = 10  # MNIST data parameters\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = X_train.astype('float32'); X_test = X_test.astype('float32')  # int -> float\n",
    "X_train = X_train.reshape(-1, nrow * ncol); X_test = X_test.reshape(-1, nrow * ncol)  # flatten\n",
    "X_train /= 255; X_test /= 255  # normalize pixels to between 0 and 1\n",
    "\n",
    "# convert class vectors to binary class matrices (i.e., one-hot encoding)\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 8))\n",
    "for i in range(10):\n",
    "    plt.sca(ax.ravel()[i])\n",
    "    plt.imshow(X_train[i].reshape(nrow, ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll start with our old friend the single-layer perceptron that we implemented in the \"Basic Neural Network Exercise.\" The perceptron consists of a single fully-connected (a.k.a. dense) layer with some activation function, plus an output that we pass to the softmax function. An example of a `keras` implementation of such a network is given below; you'll want to use this as the template for the rest of your models in this exercise.\n",
    "\n",
    "The steps below should mostly be self-explanatory, but here's a quick breakdown:\n",
    "- `Sequential` is a `keras` class that allows us to add new layers one at a time\n",
    "- `Dense` is a fully-connected layer; the `input_shape` argument is only needed in the first layer\n",
    "- `Activation` passes the output of the previous layer through a specified activation function\n",
    "- `compile` prepares the underlying `tensorflow` or `theano` graph corresponding to the requested model\n",
    "- `rmsprop` is a variant of gradient descent that converges more quickly; vanilla gradient descent is rarely used for training neural networks\n",
    "- \"Categorical crossentropy\" is the same standard loss function we used in the previous example for evaluating predicted class probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model architecture\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(8, input_dim=nrow * ncol))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Fit model to training data and check accuracy\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "\n",
    "batch_size = 128\n",
    "nb_epoch = 20\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score: {}; test accuracy: {}'.format(score, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1a: Fit model and examine predictions\n",
    "Look through the above code carefully and make sure you understand each step. Identify a couple of test cases using that are incorrectly classified by our model; the `model.predict_classes` function will be useful (`model.predict` in this case is more like `scikit-learn`'s `predict_proba`). Are these examples particularly difficult or is our model underperforming?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred_test = model.predict_classes(X_test, verbose=0)\n",
    "misclassified = y_test != pred_test\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(15, 8))\n",
    "for i in range(10):\n",
    "    plt.sca(ax.ravel()[i])\n",
    "    plt.imshow(X_test[misclassified][i].reshape(nrow, ncol))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1b: Multi-layer classifier\n",
    "Write a function that takes parameters `hidden_size` and `num_layers` and returns a classifier like the one above but with multiple fully-connected layers. Compare the performance of your multi-layer classifier with that of the single-layer network above. Also, check the output of `model.summary()` and see how the number of parameters varies with `hidden_size` and `num_layers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fully_connected(hidden_size, num_layers):\n",
    "    model = Sequential()\n",
    "    for i in range(num_layers):\n",
    "        model.add(Dense(hidden_size, input_dim=nrow * ncol if i == 0 else None))\n",
    "        model.add(Activation('relu'))\n",
    "    model.add(Dense(nb_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "\n",
    "model = fully_connected(64, 3)\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score: {}; test accuracy: {}'.format(score, accuracy))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Convolutional network\n",
    "In the above example, we treated our images as one-dimensional vectors and input them to a simple feed-forward network. This has the major disadvantage of ignoring the local structure of the image: each pixel is considered separately from its surroundings. Convolutional networks, on the other hand, apply a number of filters to small regions of each image; these filters are trained to recognize common shapes that appear in the image and are useful for distinguishing classes. Here we'll train a basic convolutional network to perform the same image classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First we'll reshape the data back into two-dimensional form\n",
    "X_train = X_train.reshape(X_train.shape[0], nrow, ncol, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], nrow, ncol, 1)\n",
    "input_shape = (nrow, ncol, 1)  # only 1 channel since the images are black and white"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2a: Simple convolutional network\n",
    "First, implement a simple convolutional network consisting of a single convolutional layer, a single dense ReLU layer, and a softmax output. The structure should look basically like the single-layer network from earlier, but with a couple of additions.\n",
    "\n",
    "A couple of hints:\n",
    "- The `Conv2D` layer takes parameters `nb_filter` for the number of filters, and `nb_row` and `nb_col` for the filter dimensions; try a small number (e.g. 12) of small (e.g. 3 x 3) filters to start.\n",
    "- Since a convolutional layer expects 2D inputs and a fully-connected layer expects 1D inputs, you'll want to add a `Flatten()` layer inbetween; this is just like the flattening we performed the MNIST inputs themselves before passing them to our fully-connected network, except applied to the intermediate values of our network.\n",
    "- This is a more computationally-intensive training procedure, so try reducing `nb_epoch` to something like 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Flatten\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(12, 3, 3, input_shape=input_shape))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 5\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score: {}; test accuracy: {}'.format(score, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2b: Deep-ish convolutional network\n",
    "Part of the reason the above network is so slow is because of the connection between the convolutional and fully-connected layers: the convolutional layer's weights consist of just a few small filters, but its output is large (roughly 28x28x{# filters}), so the following fully-connected layer needs roughly 28x28x{# filters}x{hidden layer size} parameters...in other words, a lot. For this reason, it's common to insert a pooling layer after a convolutional layer, which reduces the output size considerably. The most common type of pooling is max pooling, primarily because of its simplicity. For more about pooling see, e.g., the [CS 231n lecture notes](http://cs231n.github.io/convolutional-networks/#pool) about convolutional architectures.\n",
    "\n",
    "Extend the network above by first replacing the first dense layer with a max pooling layer; the `MaxPooling2D` layer takes a 2-dimensional tuple `pool_size` which controls the size of regions that are pooled together. Experiment with this parameter and see how the training time changes; then try adding additional convolutional+pooling layers to form a deeper network. Check the output of `model.summary()` to see how the number of parameters changes depending on whether you apply pooling after a convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, 3, input_shape=input_shape))\n",
    "model.add(MaxPooling2D((3, 3)))\n",
    "model.add(Conv2D(32, 3, 3))\n",
    "model.add(MaxPooling2D((3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 5\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score: {}; test accuracy: {}'.format(score, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2c: Regularization\n",
    "Because of the extremely high number of parameters contained in deep convolutional networks, the chances of overfitting your model to the test data are rather high. The term \"regularization\" is something of a catch-all for techniques that try to mitigate this overfitting behavior. Here we'll try adding a couple of approaches to our existing network: dropout and weight penalization.\n",
    "\n",
    "Dropout is a technique that randomly sets some fraction neuron activations to zero during training; the idea is that the network will learn to cope with this by developing multiple different representations for the various patterns in the data, and thereby increase its robustness to unseen data. For validation/test data, dropout is then disabled to generate the best predictions possible.\n",
    "\n",
    "Weight penalization is a standard idea from linear regression (there it's called ridge regression/LASSO/elastic net, among other things). For each set of weights, a penalty is added to the loss function that is proportional to the magnitude of the coefficients; the result is that the network prefers a combination of many smaller weights rather than some extremely large weights, which again (hopefully) should lead to a more robust model.\n",
    "\n",
    "Extend your network by adding dropout layers between the convolutional layers, and/or by adding $\\ell_2$ regularization to the weights in an existing layer. Does this help reduce the gap between the training and test error? Dropout in `keras` is simply another layer; regularization can be passed into an existing layer with the keyword argument `W_regularizer=l2({some small value})`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.regularizers import l2\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, 3, 3, input_shape=input_shape))\n",
    "model.add(MaxPooling2D((3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(32, 3, 3))\n",
    "model.add(MaxPooling2D((3, 3)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 5\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score: {}; test accuracy: {}'.format(score, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Recurrent neural network\n",
    "Recurrent neural networks are typically used to process sequence data, such as text data, time series, etc. In this case, we'll treat each image as a one-dimensional sequence of pixels and process them sequentially using recurrent layers. This is generally not the preferred way to handle image data as it somewhat distorts the spatial structure, but as we'll see the results aren't too bad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, reshape the data into one-dimensional sequences\n",
    "X_train = X_train.reshape(X_train.shape[0], -1, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], -1, 1)\n",
    "input_shape = X_train.shape[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular type of recurrent cell is called a \"Long Short-Term Memory\" cell, or LSTM. Follow the same structure as above and implement a simple recurrent neural network classifier using the `LSTM` layer. \n",
    "\n",
    "- Note that a recurrent cell takes sequences as inputs, but can output either sequences (i.e. a new value each time it processes a value from the input), or individual values (i.e. only one value after the entire sequence is processed). In `keras` this is controlled using the `return_sequences` keyword argument.\n",
    "- Recurrent networks generally use different activations than convolutional networks; you can omit the `Activation` layers here and just use the `LSTM` cell's default, which is tanh.\n",
    "- Training LSTMs is computationally-intensive so for now keep your hidden layer sizes small and only use the first 250 training examples; this won't be enough to train an effective model but you'll at least see the steps needed to do so given more time/computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(16, input_shape=input_shape, return_sequences=True))\n",
    "model.add(LSTM(16, return_sequences=False))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "model.summary()\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "nb_epoch = 5\n",
    "history = model.fit(X_train[:250], Y_train[:250],\n",
    "                    batch_size=batch_size, nb_epoch=nb_epoch,\n",
    "                    validation_data=(X_test, Y_test),\n",
    "                    verbose=0, callbacks=[TQDMNotebookCallback()])\n",
    "score, accuracy = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score: {}; test accuracy: {}'.format(score, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CIFAR10 dataset (optional)\n",
    "Another standard neural network test case is the CIFAR10 image dataset, which consists of 60,000 32x32 color images from 10 classes. This dataset is available in `keras` as `keras.datasets.cifar10`. Use one of the above network architectures and see how it performs on this (more difficult) problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Here's a larger network from the Keras examples page that performs decently on CIFAR10\n",
    "# https://github.com/fchollet/keras/blob/master/examples/cifar10_cnn.py\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.datasets import cifar10\n",
    "\n",
    "batch_size = 32\n",
    "nb_classes = 10\n",
    "nb_epoch = 200\n",
    "\n",
    "# input image dimensions\n",
    "img_rows, img_cols = 32, 32\n",
    "# The CIFAR10 images are RGB.\n",
    "img_channels = 3\n",
    "\n",
    "# The data, shuffled and split between train and test sets:\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='same',\n",
    "                        input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(32, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Convolution2D(64, 3, 3, border_mode='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size,\n",
    "          nb_epoch=nb_epoch,\n",
    "          validation_data=(X_test, Y_test),\n",
    "          shuffle=True, verbose=0,\n",
    "          callbacks=[TQDMNotebookCallback()])"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {
    "007e46f4f1bf400fa99c918f774a5000": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "00b15220ec564a6985d3e826c207a460": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "018e62560d1f42b1b4d44038998dfcad": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "0230f20af0544b9ca7125b532ad30117": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "0aa487861b274aa48ce72f392251f939": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "0b4d6b8cf0be4a3a9603daa88d2d12b2": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "0e145bd4ce0644e6bc89748303b32043": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "0f890b5255324daab3d58f79f5cf8889": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "0fb5e390aa10410d82900a95dfaae768": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "10bca7aacaff4cd0b0199609b32ff7d5": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "11de9434a623432a946758579e584b49": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "126f83c8f46f47cc85af1e1fd27360d1": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "134da487a5014f7698f4bfbfc938666e": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "189148fe64a047959eeb0be825e65e1b": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "1b8e38ae67624f15bc428300b2734363": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "1bb89b8b8ece49459e37914f32bf7cc3": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "1ef61160061d432a9e0110787fc96b22": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "2a80ccdfa2384bf9813b6d96e7d02693": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "2b293f68f53e47739a60ec3bc35721eb": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "2b4efc73a57d4c0a9c3c08820f923809": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "2b9d43c9ac3a42cdaf2bf68b851e573f": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "2ba75013446744319f72f6104329edac": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "2d95f26b8f3a4543bf0a5fb8d415c6b1": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "2deac0ebf0894e388545b86a52817041": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "2e9e6b650ac54838af96a3a08932a289": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "36f6f5a5f7774d2594830dca418f74c7": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "37e54ebd389f4980b3f4f18c90506769": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "3843449abbb94820a46b4bfb3cd8608d": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "3b325001a26d411badf965512c45a997": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "3ee9ba82876c411e84afb2c5dc8fe5ab": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "44c1dc3261974d70803e2dc50325c0e4": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "4671c8797aa64aa7830c8d6b77e40511": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "4f57ca0a95474e82934a4dc7d2c61fa2": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "52d7daf4b4d740cebf6cece0dfc9602d": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "538f27f7ef764e55904fd2d3cccd34bd": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "53fa81ed2efa49e6a66479fc05d70203": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "547abc13fc074d0b8c062d4f49c0406c": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "5658f53cfef74b108db7f2f8e309bf00": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "58452e9208fd4bfd937d56857949873b": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "58d31e536c2441d39df0ad48370f1f88": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "5af39d92d902413d9f28efc91b2416f9": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "5bb8fa2d57ea474ba4e6e07f02a8a255": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "5febc0648f024cdba5bf6d5dc864b8a0": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "5ff5b6c2a9c0491ab5fc01c64eea7fa7": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "61e4bcffa9da496cb68ef945a384e8aa": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "61f735ab010a46debcf6ceaa9e703695": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "658b30a8e98b441dac7ac3d64861d41a": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "694a04722def41dbadb349f311eb84f7": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "699b9b999ee74df09b37c03948f7237b": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "6a6360cb662c461b8ced9bf8904a1f4b": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "6c2a6db0c55e460ca4bb8f0b2f7feca1": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "6cbe8ad5f0474ae6a75b98383eb124cf": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "6dfbf3c017334123a6e7f8087f32386b": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "6fbfeb6c15764ca8aa658a6cfae4c957": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "71640b33bcaa4d72b70dbfb85089fff5": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "72361dc1132e4e88be025f6e3fdf144a": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "72ef2f18ef3c45b58c1eb91b33e7487c": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "7560fc1001f242a4bd358713f4275c3a": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "75c0631cf4924d5c8504c97151f8072d": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "75e25637541d4f2481c4ab0859020eac": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "7813b0bc11f740849e895f96bdb5b0a7": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "7a3ec63995ea4cd588ecda729d783416": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "7bfd55016ea14948b9ac7a14fc7e9d3c": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "7cb28a58b9f04e9c9924a265d93fd578": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "81d35b31ba02472d945982cb51e3427e": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "82b1a8cc593a4a989e1a221906251b1c": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "84a872b706734927a53e7d717d455a32": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "84c632c12ac74b819d102122009316ff": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "866d96ef231e43f9b66cbe6036d8a99a": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "8c303647813d4971983c3b180bd3e0f4": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "8c4c45d84217485fac3f5252828900bb": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "8d92d7ebcc80447cbef8915afaf2aaa7": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "8ddcead6c2174710b934b32d3b62d38b": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "8e8d86e4deb94d248dc3525c392c0fc9": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "9103c870edae4047936e331f409d9f1f": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "9200e001c375495c9772ca6ef28a9957": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "923f49fcb6924b40b1597ded26450676": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "928aff2d054e4025ad66bfe6df128a0b": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "93f85eef394d4f5691e0392c99fc5018": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "95126b75ee174d4990f6efd26f8b8397": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "961aadb15f014333bcefbd01b587d35b": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "968ded38b38048d0880777123c385fe7": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "9b1935afbebb49eaabc006fd902462ad": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "9b3c861e50e740cabfb70492c0c7c463": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "9c9853d965cb4c09b203e60397eaacc1": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "9e09aaaf8c5d4aee919caf827d6c2f13": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "a133919fa216457da01c46880adad434": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "a1607e9bbca54938bbdb7c6fdafff78c": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "a456176bc84946f3bd16f802838c1dfa": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "acf0129297934b7d80ce631f3abf5d33": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "ad7d07a0386946e8abb22d424c269e33": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "ae2a9c2216f343c0b1f0c549dd7b7fc3": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "afe528a092404edb87b37a69c124dad0": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "b012c57aaffd4a4fa21b4d1022aec2b1": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "b02938afab19404297bdfbb608475bb6": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    },
    "b0656c7e52594a5695a47406cf47174b": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "b4d9fee8fbe1464ba404a98812f0d855": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "b8ce8e33441149a7bacc91a4698d0790": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "b8d4a060feb940f1b0b98330bfab8d41": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "baf0894b32c84433803bfed9f72cb4f3": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "bd4fa8482c244063b3f9d05d4489f947": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "be86153fcfaf49d3b4134cf24cd21fbc": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "c23ef64cdddc47a383465a092605ae83": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "c2748cbaebfb40eebf7bc4e066aa0df3": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "c649a6dbca474689822d2adc39d54812": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "c7ad977fcc8a42ab85266f925e44f681": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "c7ca698388144a3e8144797b2e15182d": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "ccb3cedde1a846038c39e83f47f8d73b": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d0d1ad051e4b493dae7b05a5874ed949": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "d17c06b64ab143838d1284af9fb5a10d": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d2476a41bd874ccfaa3a7e163f5a749f": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "d2b67d62257d43148b7c0a23d51b67c9": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "d2f516ef93474f9d9f5c9bb250ad8fa6": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d30711e9c320473f9e2f4ca8f9eae34b": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "d323828b67764409ab9d7f278abace4d": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "d9e268778e9c4269bc877c3f5d82a7d0": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "dd33080d941b41c7925145f54655b737": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "e0983b4afb4547cc8cdf979dfa0e45b9": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "e1eaec00ed5145bc978eb4e919a4617f": {
     "views": [
      {
       "cell_index": 12
      }
     ]
    },
    "e84be1db56d049428de5caa16b6cac73": {
     "views": [
      {
       "cell_index": 20
      }
     ]
    },
    "e89a305029d142d9b2eca1c07e33f056": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ea80ef43d4cf42b49910f4834f6cd866": {
     "views": [
      {
       "cell_index": 18
      }
     ]
    },
    "ed8f2bc80eca411886497cd047b3d134": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "ee52e039fe3542bfa59f3ed294a76901": {
     "views": [
      {
       "cell_index": 16
      }
     ]
    },
    "ef6934f56b04494d9581cb5e6ed6f229": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "f8199303ce1c4d7ba29f51fc334d4bbc": {
     "views": [
      {
       "cell_index": 8
      }
     ]
    },
    "fada1a5731984465acfccd66fcef9e5e": {
     "views": [
      {
       "cell_index": 24
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
