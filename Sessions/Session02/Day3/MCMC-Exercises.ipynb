{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCMC Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### David Kirkby, UC Irvine / LSSTC Data Science Fellows Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will need to `pip install` these two packages, if you haven't already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import emcee\n",
    "import corner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This first section is a walk through to introduce you to the terminology and define some useful functions. Read it carefully, evaluate each cell, then move to the following sections where you will be repeating this basic study with variations.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will study one of the simplest models, the straight line (which still has many [subtleties](https://arxiv.org/abs/1008.4686)):\n",
    "$$\n",
    "f(x) = m x + b\n",
    "$$\n",
    "This exercise is based on a more [complicated example](http://dan.iel.fm/emcee/current/user/line/) in the [emcee documentation](http://dan.iel.fm/emcee/current/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, specify our true parameters.  A dictionary is overkill with only two parameters, but is useful when tracking a larger number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth = dict(m=-1, b=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the true model over the range [0, 10]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_lo, x_hi = 0, 10\n",
    "x_plot = np.linspace(x_lo, x_hi, 100)  # Really only need 2 points here\n",
    "y_plot = truth['m'] * x_plot + truth['b']\n",
    "\n",
    "plt.plot(x_plot, y_plot, 'r-', label='Truth')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, generate a \"toy Monte Carlo\" sample of `n_data = 50` observations using the true model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reproducible \"randomness\"\n",
    "gen = np.random.RandomState(seed=123)\n",
    "\n",
    "# Random x values, in increasing order.\n",
    "n_data = 50\n",
    "x_data = np.sort(gen.uniform(low=0, high=10, size=n_data))\n",
    "# True y values without any noise.\n",
    "y_true = truth['m'] * x_data + truth['b']\n",
    "\n",
    "# Add Gaussian noise.\n",
    "rms_noise = 0.5\n",
    "y_error = np.full(n_data, rms_noise)\n",
    "y_data = y_true + gen.normal(scale=y_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the generated sample with the model superimposed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x_data, y_data, yerr=y_error, fmt='.k', label='Toy MC')\n",
    "plt.plot(x_plot, y_plot, 'r-', label='Truth')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least Squares Cross Check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We chose a linear problem so that we would have an alternate method (weighted least squares linear regression) to calculate the expected answer, for cross checks.  For details on this method, see:\n",
    "\n",
    " https://en.wikipedia.org/wiki/Linear_least_squares_(mathematics)#Weighted_linear_least_squares\n",
    " \n",
    "The appropriate weights here are \"inverse variances\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights = y_error ** -2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn provides a convenient class for (weighted) linear least squares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sklearn.linear_model\n",
    "\n",
    "linear_model = sklearn.linear_model.LinearRegression()\n",
    "linear_model.fit(x_data.reshape(-1, 1), y_data, sample_weight=weights)\n",
    "\n",
    "fit = dict(m=linear_model.coef_[0], b=linear_model.intercept_)\n",
    "print fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weighted linear least squares method also provides a covariance matrix for the best-fit parameters, but unfortunately sklearn does not calculate this for you.  However, we will soon have something even better using MCMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add this fit to our plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_fit = fit['m'] * x_plot + fit['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x_data, y_data, yerr=rms_noise, fmt='.k', label='Toy MC')\n",
    "plt.plot(x_plot, y_plot, 'r-', label='Truth')\n",
    "plt.plot(x_plot, y_fit, 'b--', label='Fit')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our priors $P(\\theta)$ on the two fit parameters $\\theta$ with a function to calculate $logP(\\theta)$. The function should return `-np.inf` for any parameter values that are forbidden.\n",
    "\n",
    "Start with simple \"top hat\" priors on both parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_prior(theta):\n",
    "    m, b = theta\n",
    "    if -5 < m < 0.5 and 0 < b < 10:\n",
    "        return 0 # log(1) = 0\n",
    "    else:\n",
    "        return -np.inf # some parameter is outside its allowed range."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined the likelihood $P(D|\\theta)$ of our data $D$ given the parameters $\\theta$ with a function that returns $\\log P(D|\\theta)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_likelihood(theta):\n",
    "    m, b = theta\n",
    "    # Calculate the predicted value at each x.\n",
    "    y_pred = m * x_data + b\n",
    "    # Calculate the residuals at each x.\n",
    "    dy = y_data - y_pred\n",
    "    # Assume Gaussian errors on each data point.\n",
    "    return -0.5 * np.sum(dy ** 2) / rms_noise ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the priors and likelihood to calculate the log of the posterior $\\log P(\\theta|D)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_posterior(theta):\n",
    "    lp = log_prior(theta)\n",
    "    if np.isfinite(lp):\n",
    "        # Only calculate likelihood if necessary.\n",
    "        lp += log_likelihood(theta)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a Markov chain sampler using the `emcee` package:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ndim = 2  # the number of parameters\n",
    "nwalkers = 100  # bigger is better\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Give each \"walker\" a unique starting point in parameter space.  The advice on this in the [docs](http://dan.iel.fm/emcee/current/user/faq/#how-should-i-initialize-the-walkers) is:\n",
    "> The best technique seems to be to start in a small ball around the a priori preferred position. Donâ€™t worry, the walkers quickly branch out and explore the rest of the space.\n",
    "\n",
    "Pick something close but exactly the truth for our a priori preferred position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_priori = np.array([-1.1, 5.5])\n",
    "ball_size = 1e-4\n",
    "initial = [a_priori + ball_size * gen.normal(size=ndim) for i in range(nwalkers)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate 500 samples for each walker (and ignore the sampler return value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.run_mcmc(initial, 500);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `sampler` object has a `chain` attribute with our generated chains for each walker:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.chain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the chains from the first 3 walkers for each parameter. Notice how they all start from the same small ball around our a priori initial point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sampler.chain[0, :, 0])\n",
    "plt.plot(sampler.chain[1, :, 0])\n",
    "plt.plot(sampler.chain[2, :, 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(sampler.chain[0, :, 1])\n",
    "plt.plot(sampler.chain[1, :, 1])\n",
    "plt.plot(sampler.chain[2, :, 1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all the walkers into a single array of generated samples.  You could trim off some initial \"burn-in\" here, but I don't recommend it (see lecture notes):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "samples = sampler.chain[:, :, :].reshape((-1, ndim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consecutive samples are certainly correlated, but the ensemble of all samples should have the correct distribution for inferences.  Make a corner plot to see this distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = corner.corner(samples, labels=['m', 'b'], truths=[truth['m'], truth['b']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each sample represents a possible model.  Pick a few at random to superimpose on our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "shuffled = samples[gen.permutation(len(samples))]\n",
    "shuffled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.errorbar(x_data, y_data, yerr=rms_noise, fmt='.k', label='Toy MC')\n",
    "plt.plot(x_plot, y_plot, 'r-', label='Truth')\n",
    "\n",
    "for m, b in shuffled[:100]:\n",
    "    y_sample = m * x_plot + b\n",
    "    plt.plot(x_plot, y_sample, 'b', alpha=0.05)\n",
    "\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Marginal Distributions and Confidence Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function that uses the `samples` array to plot the distribution of predicted values for $y(x)^8$ given $x$ and prints the (central) 68% and 95% confidence limits on this prediction.  (Hint: `np.percentile`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def marginal(x):\n",
    "    # ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that you can use this technique to make inferences about any non-linear function of the parameters.  I chose $y(x)^8$ here so that the result would have an asymmetric distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Known Heteroscedastic Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example above, the RMS error on each point is known and the same for each point.  In this exercise, you will repeat the whole walk-through study above, but with **known but non-constant (aka heteroscedastic) errors** for each point. Copy each cell into this section then edit it, rather than editing the cells above directly, so you can compare later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your new errors use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_error = gen.uniform(low=0.1, high=1.0, size=n_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting, take a moment to review the results above and predict how they will change..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Uncertain Heteroscedastic Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, the errors reported for each point are uncertain: the true errors are different from the reported errors by an unknown constant scale factor.  This introduces a new parameter, the scale factor, that we will call `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "truth = dict(m=-1, b=5, s=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regenerate the data using this larger error, while keeping the reported error the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_data = y_true + gen.normal(scale=y_error * truth['s'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model now has three parameters instead of two.  Repeat the previous exercise with this additional parameter. Before starting, take a moment to review your results above and predict how they will change..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Exercise: PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the walk through above (constant known error) using the [pymc3](http://pymc-devs.github.io/pymc3/) package instead of `emcee`.  Be sure to use `pymc3` and not `pymc2` (since they are quite different).  You will find that `pymc3` has a different approach that takes some getting used to, but it is very helpful to have both packages in your toolkit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
