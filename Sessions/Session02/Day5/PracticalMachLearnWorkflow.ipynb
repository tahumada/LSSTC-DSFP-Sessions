{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function, absolute_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#A Practical Guide to the Machine Learning Workflow:\n",
    "Separating Stars and Galaxies from SDSS \n",
    "========\n",
    "\n",
    "#####Version 0.1\n",
    "\n",
    "***\n",
    "By AA Miller 2017 Jan 22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will now follow the steps from the machine learning workflow lecture to develop an end-to-end machine learning model using actual astronomical data. As a reminder the workflow is as follows:\n",
    "\n",
    "1. Data Preparation\n",
    "2. Model Building\n",
    "3. Model Evaluation\n",
    "4. Model Optimization\n",
    "5. Model Predictions\n",
    "\n",
    "Some of these steps will be streamlined to allow us to fully build a model within the alloted time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Science background*: Many (nearly all?) of the science applications for LSST data will rely on the accurate separation of stars and galaxies in the LSST imaging data. As an example, imagine measuring galaxy clustering without knowing which sources are galaxies and which are stars. \n",
    "\n",
    "During this exercise, we will utilize supervised machine-learning methods to separate extended (galaxies) and point sources (stars, QSOs) in imaging data. These methods are highly flexible, and as a result can classify sources at higher fidelity than methods that simply make cuts in a low-dimensional space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from astropy.table import Table\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Problem 1) Obtain and Examine Training Data\n",
    "\n",
    "As a reminder, for supervised-learning problems we use a training set, sources with known labels, i.e. they have been confirmed as normal stars, QSOs, or galaxies, to build a model to classify new observations where we do not know the source label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The training set for this exercise uses Sloan Digital Sky Survey (SDSS) data. For features, we will start with each $r$-band magnitude measurement made by SDSS. This yields 8 features (twice that of the Iris data set, but significantly fewer than the 454 properties measured for each source in SDSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Step 1 in the ML workflow is data preparation - we must curate the training set. As a reminder: \n",
    "\n",
    "**A machine-learning model is only as good as its training set.** \n",
    "\n",
    "This point cannot be emphasized enough. Machine-learning models are data-driven, they do not capture any physical theory, and thus it is essential that the training set satisfy several criteria. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Two of the most important criteria for a good training set are: \n",
    "\n",
    "+ the training set should be unbiased [this is actually really hard to achieve in astronomy since most surveys are magnitude limited]\n",
    "+ the training set should be representative of the (unobserved or field) population of sources [a training set with no stars will yield a model incapable of discovering point sources]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So, **step 1** (this is a must), we are going to examine the training set to see if anything suspicious is going on. We will use [`astroquery`](https://astroquery.readthedocs.io/en/latest/) to directly access the SDSS database, and store the results in an [`astropy` Table](http://astropy.readthedocs.org/en/latest/table/). \n",
    "\n",
    "**Note** The SDSS API for `astroquery` is not standard for the package, which leads to a warning. This is not, however, a problem for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from astroquery.sdss import SDSS  # enables direct queries to the SDSS database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While it is possible to look up each of the names of the $r$-band magnitudes in the [SDSS `PhotoObjAll` schema](https://skyserver.sdss.org/dr12/en/help/browser/browser.aspx?cmd=description+PhotoObjAll+U#&&history=description+PhotoObjAll+U), the schema list is long, and thus difficult to parse by eye. Fortunately, we can identify the desired columns using the database itself:\n",
    "\n",
    "    select COLUMN_NAME\n",
    "    from INFORMATION_SCHEMA.Columns\n",
    "    where table_name = 'PhotoObjAll' AND \n",
    "    COLUMN_NAME like '%Mag/_r' escape '/'\n",
    "\n",
    "which returns the following list of columns: `psfMag_r, fiberMag_r, fiber2Mag_r, petroMag_r, deVMag_r, expMag_r, modelMag_r, cModelMag_r`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We now select these magnitude measurements for 10000 stars and galaxies from SDSS. Additionally, we join these photometric measurements with the `SpecObjAll` table to obtain their spectroscopic classifications, which will serve as labels for the machine-learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note** - the SDSS database contains duplicate observations, flagged observations, and non-detections, which we condition the query to exclude (as explained further below). We also exclude quasars, as the precise photometric classification of these objects is ambiguous: low-$z$ AGN have resolvable host galaxies, while high-$z$ QSOs are point-sources. Query conditions:\n",
    "\n",
    "* `p.mode = 1` select only the primary photometric detection of a source\n",
    "* `s.sciencePrimary = 1` select only the primary spectroscopic detection of a source (together with above, prevents duplicates)\n",
    "* `p.clean = 1` the SDSS [`clean`](http://www.sdss.org/dr12/algorithms/photo_flags_recommend/) flag excludes flagged observations and sources with non-detections\n",
    "* `s.class != 'QSO'` removes potentially ambiguous QSOs from the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sdss_query = \"\"\"SELECT TOP 10000\n",
    "             p.psfMag_r, p.fiberMag_r, p.fiber2Mag_r, p.petroMag_r, \n",
    "             p.deVMag_r, p.expMag_r, p.modelMag_r, p.cModelMag_r, \n",
    "             s.class\n",
    "             FROM PhotoObjAll AS p JOIN specObjAll s ON s.bestobjid = p.objid\n",
    "             WHERE p.mode = 1 AND s.sciencePrimary = 1 AND p.clean = 1 AND s.class != 'QSO'\n",
    "             ORDER BY p.objid ASC\n",
    "               \"\"\"\n",
    "sdss_set = SDSS.query_sql(sdss_query)\n",
    "sdss_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To reiterate a point from above: data-driven models are only as good as the training set. Now that we have a potential training set, it is essential to inspect the data for any peculiarities.\n",
    "\n",
    "**Problem 1a**\n",
    "\n",
    "Can you easily identify any important properties of the data from the above table?\n",
    "\n",
    "If not - is there a better way to examine the data?\n",
    "\n",
    "*Hint* - emphasis on easy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 1a**\n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 1b**\n",
    "\n",
    "Visualize the 8 dimensional feature set [this is intentionally open-ended...] \n",
    "\n",
    "Does this visualization reveal anything that is not obvious from the table?\n",
    "\n",
    "Can you identify any biases in the training set? \n",
    "\n",
    "**Remember - always worry about the data**\n",
    "\n",
    "*Hint* `astropy Tables` can be converted to `pandas DataFrames` with the `.to_pandas()` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 1b**\n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally, to finish off our preparation of the data - we need to create an independent test that will be used to evalute the accuracy/generalization properies of the model after everything has been tuned. Often, independent test sets are generated by witholding a fraction of the training set. No hard and fast rules apply for the fraction to be withheld, though typical choices vary between $\\sim{0.2}-0.5$. For this problem we will adopt 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[`sklearn.model_selection`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) has a handy function [`train_test_split`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), which will simplify this process.\n",
    "\n",
    "**Problem 1c** Split the 10k spectroscopic sources 70-30 into training and test sets. Save the results in arrays called: `train_X, train_y, test_X, test_y`, respectively. Use `rs` for the `random_state` in `train_test_split`.\n",
    "\n",
    "*Hint - recall that `sklearn` utilizes X, a 2D `np.array()`, and y as the features and labels arrays, respecitively.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "rs = 2  # we are in second biggest metropolitan area in the US\n",
    "\n",
    "# complete\n",
    "\n",
    "X = np.array( # complete\n",
    "y = np.array( # complete\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split( X, y,  # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Problem 2) An Aside on the Importance of Feature Engineering\n",
    "\n",
    "It has been said that all machine learning is an exercise in feature engineering. \n",
    "\n",
    "Feature engineering - the process of creating new features, combining features, removing features, collecting new data to supplement existing features, etc. is essential in the machine learning workflow. As part of the data preparation stage, it is useful to apply domain knowledge to engineer features prior to model construction. [Though it is important to know that feature engineering may be needed at any point in the ML workflow if the model does not provide desired results.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Due to a peculiarity of our SDSS training set, we need to briefly craft a separate problem to demonstrate the importance of feature engineering. \n",
    "\n",
    "For this aside, we will train the model on bright ($r' < 18.5$ mag) sources and test the model on faint ($r' > 19.5$ mag) sources. As you might guess the model will not perform well. Following some clever feature engineering, we will be able to improve this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*aside-to-the-aside*\n",
    "\n",
    "This exact situation happens in astronomy all the time, and it is known as sample selection bias. In brief, any time a larger aperture telescope is built, or instrumentation is greatly improved, a large swath of sources that were previously undetectable can now be observed. These fainter sources, however, may contain entirely different populations than their brighter counterparts, and thus any models trained on the bright sources will be biased when making predictions on the faint sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We train and test the model with 10000 sources using an identical query to the one employed above, with the added condition restricting the training set to bright sources and the test set to faint sources. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bright_query = \"\"\"SELECT TOP 10000\n",
    "             p.psfMag_r, p.fiberMag_r, p.fiber2Mag_r, p.petroMag_r, \n",
    "             p.deVMag_r, p.expMag_r, p.modelMag_r, p.cModelMag_r, \n",
    "             s.class\n",
    "             FROM PhotoObjAll AS p JOIN specObjAll s ON s.bestobjid = p.objid\n",
    "             WHERE p.mode = 1 AND s.sciencePrimary = 1 AND p.clean = 1 AND s.class != 'QSO'\n",
    "             AND p.cModelMag_r < 18.5\n",
    "             ORDER BY p.objid ASC\n",
    "               \"\"\"\n",
    "bright_set = SDSS.query_sql(bright_query)\n",
    "bright_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "faint_query = \"\"\"SELECT TOP 10000\n",
    "             p.psfMag_r, p.fiberMag_r, p.fiber2Mag_r, p.petroMag_r, \n",
    "             p.deVMag_r, p.expMag_r, p.modelMag_r, p.cModelMag_r, \n",
    "             s.class\n",
    "             FROM PhotoObjAll AS p JOIN specObjAll s ON s.bestobjid = p.objid\n",
    "             WHERE p.mode = 1 AND s.sciencePrimary = 1 AND p.clean = 1 AND s.class != 'QSO'\n",
    "             AND p.cModelMag_r > 19.5\n",
    "             ORDER BY p.objid ASC\n",
    "               \"\"\"\n",
    "faint_set = SDSS.query_sql(faint_query)\n",
    "faint_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2a** \n",
    "\n",
    "Train a $k$ Nearest Neighbors model with $k = 11$ neighbors on the 10k source training set. Note - for this particular problem, the number of neighbors does not matter much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "feats = # complete\n",
    "\n",
    "\n",
    "bright_X = # complete\n",
    "bright_y = # complete\n",
    "\n",
    "KNNclf = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 2b** \n",
    "\n",
    "Evaluate the accuracy of the model when applied to the sources in the faint test set. \n",
    "\n",
    "Does the model perform well?\n",
    "\n",
    "*Hint - you may find [`sklearn.metrics.accuracy_score`](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) useful for this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "faint_X = # complete\n",
    "faint_y = # complete\n",
    "\n",
    "faint_preds = # complete\n",
    "\n",
    "print(\"The raw features produce a KNN model with accuracy ~{:.4f}\".format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 2b** \n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Leveraging the same domain knowledge discussed above, namely that galaxies cannot be modeled with a PSF, we can \"normalize\" the magnitude measurements by taking their difference relative to `psfMag_r`. This normalization has the added advantage of removing any knowledge of the apparent brightness of the sources, which should help when comparing independent bright and faint sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 2c** \n",
    "\n",
    "Normalize the feature vector relative to `psfMag_r`, and refit the $k$NN model to the 7 newly engineered features.\n",
    "\n",
    "Does the accuracy improve when predicting the class of sources in the faint test set? \n",
    "\n",
    "*Hint - be sure you apply the eaxct same normalization to both the training and test set* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "bright_Xnorm = # complete\n",
    "\n",
    "KNNclf = # complete\n",
    "\n",
    "faint_predsNorm = # complete\n",
    "\n",
    "print(\"The normalized features produce an accuracy ~{:.4f}\".format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 2c** \n",
    "\n",
    "Wow! Normalizing the features produces a huge ($\\sim{35}\\%$) increase in accuracy. Clearly, we should be using normalized magnitude features moving forward."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In addition to demonstrating the importance of feature engineering, this exercise teaches another important lesson: contextual features can be dangerous. \n",
    "\n",
    "Contextual astronomical features can provide very strong priors: stars are more likely close to the galactic plane, supernovae occur next to/on top of galaxies, bluer stars have have lower metallicity, etc. Thus, including contextual information may improve overall model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, all astronomical training sets are heavily biased. Thus, the strong priors associated with contextual features can lead to severely biased model predictions.\n",
    "\n",
    "Generally, I (AAM) remove all contextual features from my ML models for this reason. If you are building ML models, consider contextual information as it may help overall performance, but... be weary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Worry about the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Problem 3) Model Building\n",
    "\n",
    "After the data have been properly curated, the next important choice in the ML workflow is the selection of ML algorithm. With experience, it is possible to develop intuition for the best ML algorithm given a specific problem.\n",
    "\n",
    "Short of that? Try three (or four, or five) different models and choose whichever works the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For the star-galaxy problem, we will use the Random Forest (RF) algorithm [(Breiman 2001)](http://link.springer.com/article/10.1023/A:1010933404324) as implemented by `scikit-learn`.\n",
    "\n",
    "[`RandomForestClassifier`](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) is part of the [`sklearn.ensemble`](http://scikit-learn.org/stable/modules/classes.html#module-sklearn.ensemble) module."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RF has a number of nice properties for working with astronomical data:\n",
    "\n",
    "+ relative insensitivity to noisy or useless features\n",
    "+ invariant response to highly non-gaussian feature distributions\n",
    "+ fast, flexible and scales well to large data sets\n",
    "\n",
    "which is why we will adopt it here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3a** \n",
    "\n",
    "Build a RF model using the normalized features from the training set.\n",
    "\n",
    "Include 25 trees in the forest using the `n_estimators` paramater in `RandomForestClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import # complete\n",
    "rs = 626 # aread code for Pasadena\n",
    "\n",
    "train_Xnorm = # complete\n",
    "\n",
    "RFclf = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "`scikit-learn` really makes it easy to build ML models.\n",
    "\n",
    "Another nice property of RF is that it naturally provides an estimate of the most important features in the model. \n",
    "\n",
    "[Once again - feature engineering comes into play, as it may be necessary to remove correlated features or unimportant features during the model construction in order to reduce run time or allow the model to fit in the available memory.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this case we don't need to remove any features [RF is relatively immune to correlated or unimportant features], but for completeness we measure the importance of each feature in the model. \n",
    "\n",
    "RF feature importance is measured by randomly shuffling the values of a particular feature, and measuring the decrease in the model's overall accuracy. The relative feature importances can be accessed using the `.feature_importances_` attribute associated with the `RandomForestClassifer()` class. The higher the value, the more important feature. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 3b** \n",
    "\n",
    "Calculate the relative importance of each feature. \n",
    "\n",
    "Which feature is most important? Can you make sense of the feature ordering? \n",
    "\n",
    "*Hint - do not dwell too long on the final ordering of the features.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    " # complete\n",
    "    \n",
    "\n",
    "print(\"The relative importance of the features is: \\n{:s}\".format(  # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 3b** \n",
    "\n",
    "`psfMag_r - deVMag_r` is the most important feature. This makes sense based on the separation of stars and galaxies in the `psfMag_r`-`deVMag_r` plane (see the visualization results above). \n",
    "\n",
    "*Note* - the precise ordering of the features can change due to their strong correlation with each other, though the `fiberMag` features are always the least important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Problem 4) Model Evaluation\n",
    "\n",
    "To evaluate the performance of the model we establish a baseline (or figure of merit) that we would like to exceed. This in essence is the essential \"engineering\" step of machine learning [and why I (AAM) often caution against ML for scientific measurements and advocate for engineering-like problems instead]. \n",
    "\n",
    "If the model does not improve upon the baseline (or reach the desired figure of merit) then one must iterate on previous steps (feature engineering, algorithm selection, etc) to accomplish the desired goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The SDSS photometric pipeline uses a simple parametric model to classify sources as either stars or galaxies. If we are going to the trouble of building a complex ML model, then it stands to reason that its performance should exceed that of the simple model. Thus, we adopt the SDSS photometric classifier as our baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tthe SDSS photometric classifier uses a [single hard cut](http://www.sdss.org/dr12/algorithms/classify/#photo_class) to separate stars and galaxies in imaging data:\n",
    "\n",
    "$$\\mathtt{psfMag} - \\mathtt{cmodelMag} > 0.145.$$\n",
    "\n",
    "Sources that satisfy this criteria are considered galaxies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4a** \n",
    "\n",
    "Determine the baseline for the ML model by measuring the accuracy of the SDSS photometric classifier on the training set. \n",
    "\n",
    "*Hint - you may need to play around with array values to get `accuracy_score` to work.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete\n",
    "\n",
    "print(\"The SDSS phot model produces an accuracy ~{:.4f}\".format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The simple SDSS model sets a high standard! A $\\sim{96}\\%$ accuracy following a single hard cut is a phenomenal  performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 4b** Using 10-fold cross validation, estimate the accuracy of the RF model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import # complete\n",
    "\n",
    "RFpreds = # complete\n",
    "\n",
    "print(\"The CV accuracy for the training set is {:.4f}\".format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Phew! Our hard work to build a machine learning model has been rewarded, by creating an improved model: $\\sim{96.9}\\%$ accuracy vs. $\\sim{96.4}\\%$.\n",
    "\n",
    "[But - was our effort worth only a $0.5\\%$ improvement in the model?]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Problem 5) Model Optimization\n",
    "\n",
    "While the \"off-the-shelf\" model provides an improvement over the SDSS photometric classifier, we can further refine and improve the performance of the machine learning model by adjusting the model tuning parameters. A process known as model optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All machine-learning models have tuning parameters. In brief, these parameters capture the smoothness of the model in the multidimentional-feature space. Whether the model is smooth or coarse is application dependent -- be weary of over-fitting or under-fitting the data. Generally speaking, RF (and most tree-based methods) have 3 flavors of tuning parameter:\n",
    "\n",
    "1. $N_\\mathrm{tree}$ - the number of trees in the forest `n_estimators` (default: 10) in `sklearn`\n",
    "2. $m_\\mathrm{try}$ - the number of (random) features to explore as splitting criteria at each node `max_features` (default: `sqrt(n_features)`) in `sklearn`\n",
    "3. Pruning criteria - defined stopping criteria for ending continued growth of the tree, there are [many choices](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for this in `sklearn` (My preference is `min_samples_leaf` (default: 1) which sets the minimum number of sources allowed in a terminal node, or leaf, of the tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Just as we previously evaluated the model using CV, we must optimize the tuning paramters via CV. Until we \"finalize\" the model by fixing all the input parameters, we cannot evalute the accuracy of the model with the test set as that would be \"snooping.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On Tuesday we were introduced to [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), which is an excellent tool for optimizing model parameters. \n",
    "\n",
    "Before we get to that, let's try to develop some intuition for how the tuning parameters affect the final model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 5a** \n",
    "\n",
    "Determine the 5-fold CV accuracy for models with $N_\\mathrm{tree}$ = 1, 10, 100. \n",
    "\n",
    "How do you expect changing the number of trees to affect the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rs = 1936 # year JPL was founded\n",
    "\n",
    "CVpreds1 = # complete\n",
    "\n",
    "# complete\n",
    "\n",
    "# complete\n",
    "\n",
    "print(\"The CV accuracy for 1, 10, 100 trees is {:.4f}, {:.4f}, {:.4f}\".format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "While (in this case) the affect is small, it is clear that $N_\\mathrm{tree}$ affects the model output. \n",
    "\n",
    "Now we will optimize the model over all tuning parameters. How does one actually determine the optimal set of tuning parameters? \n",
    "\n",
    "*Brute force.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This data set and the number of tuning parameters is small, so brute force is appropriate (alternatives exist when this isn't the case). We can optimize the model via a grid search that performs CV at each point in the 3D grid. The final model will adopt the point with the highest accuracy.\n",
    "\n",
    "It is important to remember two general rules of thumb: (i) if the model is optimized at the edge of the grid, refit a new grid centered on that point, and (ii) the results should be stable in the vicinity of the grid maximum. If this is not the case the model is likely overfit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5b** \n",
    "\n",
    "Use [`GridSearchCV`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) to perform a **3-fold** CV grid search to optimize the RF star-galaxy model. Remember the rules of thumb. \n",
    "\n",
    "What are the optimal tuning parameters for the model?\n",
    "\n",
    "*Hint 1 - think about the computational runtime based on the number of points in the grid. Do not start with a very dense or large grid.*\n",
    "\n",
    "*Hint 2 - if the runtime is long, don't repeat the grid search even if the optimal model is on an edge of the grid*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "rs = 64 # average temperature in Los Angeles\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "grid_results = # complete\n",
    "\n",
    "\n",
    "print(\"The optimal parameters are:\")\n",
    "for key, item in grid_results.best_params_.items(): # warning - slightly different meanings in Py2 & Py3\n",
    "    print(\"{}: {}\".format(key, item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that the model is fully optimized - we are ready for the moment of truth!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 5c**\n",
    "\n",
    "Using the optimized model parameters, train a RF model and estimate the model's generalization error using the test set.\n",
    "\n",
    "How does this compare to the baseline model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "RFopt_clf = # complete\n",
    "\n",
    "test_preds = # complete\n",
    "\n",
    "print('The optimized model produces a generalization error of {:.4f}'.format( # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 5c**\n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will now examine the performance of the model using some alternative metrics. \n",
    "\n",
    "*Note* - if these metrics are essential for judging the model performance, then they should be incorporated to the workflow in the evaluation stage, prior to examination of the test set. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Problem 5d**\n",
    "\n",
    "Calculate the confusion matrix for the model, as determined by the test set.\n",
    "\n",
    "Is there symmetry to the misclassifications?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import # complete\n",
    "\n",
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 5d**\n",
    "\n",
    "*Write your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5e**\n",
    "\n",
    "Calculate and plot the ROC curves for both stars and galaxies.\n",
    "\n",
    "*Hint - you'll need probabilities in order to calculate the ROC curve.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "test_preds_proba = # complete\n",
    "# complete\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve( # complete\n",
    "plt.plot( # complete\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "test_preds_proba = RFopt_clf.predict_proba(test_Xnorm)\n",
    "test_y_stars = np.zeros(len(test_y), dtype = int)\n",
    "test_y_stars[np.where(test_y == \"STAR\")] = 1\n",
    "test_y_galaxies = test_y_stars*-1. + 1\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y_stars, test_preds_proba[:,1])\n",
    "plt.plot(fpr, tpr, label = r'$\\mathrm{STAR}$', color = \"MediumAquaMarine\")\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y_galaxies, test_preds_proba[:,0])\n",
    "plt.plot(fpr, tpr, label = r'$\\mathrm{GALAXY}$', color = \"Tomato\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Problem 5f**\n",
    "\n",
    "Suppose you want a model that only misclassifies 1% of stars as galaxies. \n",
    "\n",
    "What classification threshold should be adopted for this model?\n",
    "\n",
    "What fraction of galaxies does this model miss?\n",
    "\n",
    "Can you think of a reason to adopt such a threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Solution 5f**\n",
    "\n",
    "When building galaxy 2-point correlation functions it is very important to avoid including stars in the statistics as they will bias the final measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finally - always remember: \n",
    "\n",
    "### **worry about the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "##Challenge Problem) Taking the Plunge\n",
    "###Applying the model to field data\n",
    "\n",
    "QSOs are unresolved sources that look like stars in optical imaging data. We will now download photometric measurements for 10k QSOs from SDSS and see how accurate the RF model performs for these sources. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Challenge 1** \n",
    "\n",
    "Calculate the accuracy with which the model classifies QSOs based on the 10k QSOs selected with the above command. How does that accuracy compare to that estimated by the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Challenge 2** \n",
    "\n",
    "Can you think of any reasons why the performance would be so much worse for the QSOs than it is for the stars? \n",
    "\n",
    "Can you obtain a ~.97 accuracy when classifying QSOs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Challenge 3** \n",
    "\n",
    "Perform an actual test of the model using \"field\" sources. The SDSS photometric classifier is nearly perfect for sources brighter than $r = 21$ mag. Download a random sample of $r < 21$ mag photometric sources, and classify them using the optimized RF model. Adopting the photometric classifications as ground truth, what is the accuracy of the RF model?\n",
    "\n",
    "*Hint - you'll need to look up the parameter describing photometric classification in SDSS*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# complete"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
