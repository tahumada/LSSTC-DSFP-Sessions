{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection For Machine Learning\n",
    "\n",
    "In this exercise, we will explore methods to do model selection in a machine learning context, in particular cross-validation and information criteria. At the same time, we'll learn about `scikit-learn`'s class structure and how to build a pipeline.\n",
    "\n",
    "## Why Model Selection?\n",
    "\n",
    "There are several reasons why you might want to perform model selection:\n",
    "\n",
    "* You might not be sure which machine learning algorithm is most appropriate.\n",
    "* The algorithm you have chosen might have a regularization parameter whose value you want to determine.\n",
    "* The algorithm you have chosen might have other parameters (e.g. the depth of a decision tree, the number of clusters in `KMeans`, ...) you would like to determine.\n",
    "* You might not be sure which of your features are the most useful/predictive.\n",
    "\n",
    "**Question**: Can you think of other reasons and contexts in which model selection might be important?\n",
    "\n",
    "Your decisions for how to do model selection depend very strongly (like everything else in machine learning) on the purpose of your machine learning procedure. Is your main purpose to accurately **predict** outcomes for new samples? Or are you trying to **infer** something about the system? \n",
    "\n",
    "Inference generally restricts the number of algorithms you can reasonably use, and also the number of model selection procedures you can apply. In the following, assume that everything below works for prediction problems; I will point out methods for inference where appropriate. Additionally, assume that everything below works for *supervised machine learning*. We will cover *unsupervised* methods further below.\n",
    "\n",
    "## Imports\n",
    "\n",
    "Let's first import some stuff we're going to need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# comment out this line if you don't have seaborn installed\n",
    "import seaborn as sns\n",
    "sns.set_palette(\"colorblind\")\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we're going to need some data. We'll work with the star-galaxy data from the first session. This uses the `astroquery` package and then queries the top 10000 observations from SDSS (see [this exercise](https://github.com/LSSTC-DSFP/LSSTC-DSFP-Sessions/blob/master/Session1/Day4/StarGalaxyRandomForest.ipynb) for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# execute this line:\n",
    "from astroquery.sdss import SDSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TSquery = \"\"\"SELECT TOP 10000 \n",
    "             p.psfMag_r, p.fiberMag_r, p.fiber2Mag_r, p.petroMag_r, \n",
    "             p.deVMag_r, p.expMag_r, p.modelMag_r, p.cModelMag_r, \n",
    "             s.class\n",
    "             FROM PhotoObjAll AS p JOIN specObjAll s ON s.bestobjid = p.objid\n",
    "             WHERE p.mode = 1 AND s.sciencePrimary = 1 AND p.clean = 1 AND s.class != 'QSO'\n",
    "             ORDER BY p.objid ASC\n",
    "               \"\"\"\n",
    "SDSSts = SDSS.query_sql(TSquery)\n",
    "SDSSts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1**: Visualize this data set. What representation is most appropriate, do you think?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2**: Let's now do some machine learning. In this exercise, you are going to use a random forest classifier to classify this data set. Here are the steps you'll need to perform:\n",
    "* Split the column with the classes (stars and galaxies) from the rest of the data\n",
    "* Cast the features and the classes to numpy arrays\n",
    "* Split the data into a *test* set and a *training* set. The training set will be used to train the classifier; the test set we'll reserve for the very end to test the final performance of the model (more on this on Friday). You can use the `scikit-learn` function `test_train_split` for this task\n",
    "* Define a `RandomForest` object from the `sklearn.ensemble` module. Note that the `RandomForest` class has three parameters:\n",
    "    - `n_estimators`: The number of decision trees in the random forest\n",
    "    - `max_features`: The maximum number of features to use for the decision trees\n",
    "    - `min_samples_leaf`: The minimum number of samples that need to end up in a terminal leaf (this effectively limits the number of branchings each tree can make)\n",
    "* We'll want to use *cross-validation* to decide between parameters. You can do this with the `scikit-learn` class `GridSearchCV`. This class takes a classifier as an input, along with a dictionary of the parameter values to search over.\n",
    "\n",
    "In the earlier lecture, you learned about four different types of cross-validation:\n",
    "* hold-out cross validation, where you take a single validation set to compare your algorithm's performance to\n",
    "* k-fold cross validation, where you split your training set into k subsets, each of which holds out a different portion of the data\n",
    "* leave-one-out cross validation, where you have N different subsets, each of which leaves just one sample as a validation set\n",
    "* random subset cross validation, where you pick a random subset of your data points k times as your validation set.\n",
    "\n",
    "**Exercise 2a**: Which of the four algorithms is most appropriate here? And why?\n",
    "\n",
    "**Answer**: In this case, k-fold CV or random subset CV seem to be the most appropriate algorithms to use.\n",
    "* Using hold-out cross validation leads to a percentage of the data not being used for training at all. \n",
    "* Given that the data set is not too huge, using k-fold CV probably won't slow down the ML procedure too much.\n",
    "* LOO CV is particularly useful for small data sets, where even training on a subset of the training data is difficult (for example because there are only very few examples of a certain class). \n",
    "* Random subset CV could also yield good results, since there's no real ordering to the training data. Do not use this algorithm when the ordering matters (for example in Hidden Markov Models)\n",
    "\n",
    "**Important:** One important thing to remember that cross-validation crucially depends on your *samples* being **independent** from each other. Be sure that this is the case before using it. For example, say you want to classify images of galaxies, but your data set is small, and you're not sure whether your algorithm is rotation independent. So you might choose to use the same images multiple times in your training data set, but rotated by a random degree. In this case, you *have* to make sure all versions of the same image are included in the **same** data set (either the training, the validation or the test set), and not split across data sets! If you don't, your algorithm will be unreasonably confident in its accuracy (because you are training and validating essentially on the same data points). \n",
    "\n",
    "Note that `scikit-learn` can actually deal with that! The class [`GroupKFold`](http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GroupKFold.html#sklearn.model_selection.GroupKFold) allows k-fold cross validation using an array of indices for your training data. Validation sets will only be split among samples with *different* indices. \n",
    "\n",
    "But this was just an aside. Last time, you used a random forest and used k-fold cross validation to effectively do model selection for the different parameters that the random forest classifier uses. \n",
    "\n",
    "**Exercise 2b**: Now follow the instructions above and implement your random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# set the random state\n",
    "rs = 23 \n",
    "\n",
    "# extract feature names, remove class\n",
    "\n",
    "\n",
    "# cast astropy table to pandas and then to a numpy array, remove classes\n",
    "\n",
    "\n",
    "# our classes are the outcomes to classify on\n",
    "\n",
    "\n",
    "# let's do a split in training and test set:\n",
    "# we'll leave the test set for later.\n",
    "\n",
    "\n",
    "# instantiate the random forest classifier:\n",
    "\n",
    "\n",
    "# do a grid search over the free random forest parameters:\n",
    "pars = \n",
    "\n",
    "\n",
    "grid_results = \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 2c**: Take a look at the different validation scores for the different parameter combinations. Are they very different or are they similar? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the scores are very similar, and have very small variance between the different cross validation instances. It can be useful to do this kind of representation to see for example whether there is a large variance in the cross-validation results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validating Multiple Model Components\n",
    "\n",
    "In most machine learning applications, your machine learning algorithm might not be the only component having free parameters. You might not even be sure which machine learning algorithm to use! \n",
    "\n",
    "For demonstration purposes, imagine you have many features, but many of them might be correlated. A standard dimensionality reduction technique to use is [Principal Component Analysis](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html). \n",
    "\n",
    "**Exercise 4**: The number of features in our present data set is pretty small, but let's nevertheless attempt to reduce dimensionality with PCA. Run a PCA decomposition in 2 dimensions and plot the results. Colour-code stars versus galaxies. How well do they separate along the principal components?\n",
    "\n",
    "*Hint*: Think about whether you can run PCA on training and test set separately, or whether you need to run it on both together *before* doing the train-test split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# instantiate the PCA object\n",
    "pca =\n",
    "\n",
    "# fit and transform the samples:\n",
    "X_pca = \n",
    "\n",
    "# make a plot of the PCA components colour-coded by stars and galaxies\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12,8))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 5**: Re-do the classification on the PCA components instead of the original features. Does it work better or worse than the classification on the original features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Train PCA on training data set\n",
    "\n",
    "\n",
    "# apply to test set\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# instantiate the random forest classifier:\n",
    "\n",
    "\n",
    "# do a grid search over the free random forest parameters:\n",
    "pars = \n",
    "\n",
    "grid_results ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: In general, you should (cross-)validate both your data transformations and your classifiers!\n",
    "\n",
    "But how do we know whether two components was really the right number to choose? perhaps it should have been three? Or four? Ideally, we would like to include the feature engineering in our cross validation procedure. In principle, you can do this by running a complicated for-loop. In practice, this is what `scikit-learn`s [Pipeline](http://scikit-learn.org/stable/modules/pipeline.html) is for! A `Pipeline` object takes a list of tuples of `(\"string\", ScikitLearnObject)` pairs as input and strings them together (your feature vector `X` will be put first through the first object, then the second object and so on sequentially).\n",
    "\n",
    "**Note**: `scikit-learn` distinguishes between *transformers* (i.e. classes that transform the features into something else, like PCA, t-SNE, StandardScaler, ...) and *predictors* (i.e. classes that produce predictions, such as random forests, logistic regression, ...). In a pipeline, all but the last objects must be transformers; the last object can be either.\n",
    "\n",
    "**Exercise 6**: Make a pipeline including (1) a PCA object and (2) a random forest classifier. Cross-validate both the PCA components and the parameters of the random forest classifier. What is the best number of PCA components to use?\n",
    "\n",
    "*Hint*: You can also use the convenience function [`make_pipeline`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.make_pipeline.html#sklearn.pipeline.make_pipeline) to creatue your pipeline. \n",
    "\n",
    "*Hint*: Check the documentation for the precise notation to use for cross-validating parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# make a list of name-estimator tuples\n",
    "estimators = \n",
    "\n",
    "# instantiate the pipeline\n",
    "pipe = \n",
    "\n",
    "# make a dictionary of parameters\n",
    "params = \n",
    "\n",
    "\n",
    "# perform the grid search\n",
    "grid_search = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing Algorithms\n",
    "\n",
    "So far, we've just picked PCA because it's common. But what if there's a better algorithm for dimensionality reduction out there for our problem? Or what if you'd want to compare random forests to other classifiers? \n",
    "\n",
    "In this case, your best option is to split off a separate validation set, perform cross-validation for each algorithm separately, and then compare the results using hold-out cross validation and your validation set (**Note**: Do *not* use your test set for this! Your test set is *only* used for your final error estimate!)\n",
    "\n",
    "Doing CV across algorithms is difficult, since the `KFoldCV` object needs to know which parameters belong to which algorithms, which is difficult to do. \n",
    "\n",
    "**Exercise 7**: Pick an algorithm from the [manifold learning](http://scikit-learn.org/stable/modules/manifold.html#manifold) library in `scikit-learn`, cross-validate a random forest for both, and compare the performance of both.\n",
    "\n",
    "**Important**: Do *not* choose t-SNE. The reason is that t-SNE does not generalize to new samples! This means while it's useful for data visualization, you cannot train a t-SNE transformation (in the `scikit-learn` implementation) on one part of your data and apply it to another!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# First, let's redo the train-test split to split the training data \n",
    "# into training and hold-out validation set\n",
    "\n",
    "\n",
    "# make a list of name-estimator tuples\n",
    "estimators =\n",
    "\n",
    "# instantiate the pipeline\n",
    "pipe = \n",
    "\n",
    "# make a dictionary of parameters\n",
    "params = \n",
    "\n",
    "\n",
    "# perform the grid search\n",
    "grid_search = \n",
    "\n",
    "# complete the print functions\n",
    "print(\"Best score: \")\n",
    "print(\"Best parameter set: \" )\n",
    "print(\"Validation score for model with PCA: \")\n",
    "\n",
    "# Now repeat the same procedure with the second algorithm you've picked.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Interpreting Results\n",
    "\n",
    "Earlier today, we talked about interpreting machine learning models. Let's see how you would go about this in practice.\n",
    "\n",
    "* Repeat your classification with a logistic regression model.\n",
    "* Is the logistic regression model easier or harder to interpret? Why?\n",
    "* Assume you're interested in which features are the most relevant to your classification (because they might have some bearing on the underlying physics). Would you do your classification on the original features or the PCA transformation? Why?\n",
    "* Change the subset of parameters used in the logistic regression models. Look at the weights. Do they change? How? Does that affect your interpretability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "lr = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even More Challenging Challenge Problem: Implementing Your Own Estimator\n",
    "\n",
    "Sometimes, you might want to use algorithms, for example for feature engineering, that are not implemented in scikit-learn. But perhaps these transformations still have free parameters to estimate. What to do? \n",
    "\n",
    "`scikit-learn` classes inherit from certain base classes that make it easy to implement your own objects. Below is an example I wrote for a machine learning model on time series, where I wanted to re-bin the time series in different ways and and optimize the rebinning factor with respect to the classification afterwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class RebinTimeseries(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, n=4, method=\"average\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Initialize hyperparameters\n",
    "\n",
    "        :param n: number of samples to bin\n",
    "        :param method: \"average\" or \"sum\" the samples within a bin?\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        self.n = n ## save number of bins to average together\n",
    "        self.method = method\n",
    "\n",
    "        return\n",
    "\n",
    "\n",
    "    def fit(self,X):\n",
    "        \"\"\"\n",
    "        I don't really need a fit method!\n",
    "        \"\"\"\n",
    "        \n",
    "        ## set number of light curves (L) and \n",
    "        ## number of samples per light curve (k)\n",
    "        return self\n",
    "        \n",
    "        \n",
    "    def transform(self, X):\n",
    "        self.L, self.K = X.shape\n",
    "\n",
    "    \n",
    "        ## set the number of binned samples per light curve\n",
    "        K_binned = int(self.K/self.n)\n",
    "        \n",
    "        ## if the number of samples in the original light curve\n",
    "        ## is not divisible by n, then chop off the last few samples of \n",
    "        ## the light curve to make it divisible\n",
    "        #print(\"X shape: \" + str(X.shape))\n",
    "\n",
    "        if K_binned*self.n < self.K:\n",
    "            X = X[:,:self.n*K_binned]\n",
    "        \n",
    "        ## the array for the new, binned light curves\n",
    "        X_binned = np.zeros((self.L, K_binned))\n",
    "        \n",
    "        if self.method in [\"average\", \"mean\"]:\n",
    "            method = np.mean\n",
    "        elif self.method == \"sum\":\n",
    "            method = np.sum\n",
    "        else:\n",
    "            raise Exception(\"Method not recognized!\")\n",
    "        \n",
    "        #print(\"X shape: \" + str(X.shape))\n",
    "        #print(\"L: \" + str(self.L))\n",
    "        for i in xrange(self.L):\n",
    "            t_reshape = X[i,:].reshape((K_binned, self.n))\n",
    "            X_binned[i,:] = method(t_reshape, axis=1)\n",
    "        \n",
    "        return X_binned\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass\n",
    "    \n",
    "    def score(self, X):\n",
    "        pass\n",
    "\n",
    "    def fit_transform(self, X, y=None):\n",
    "\n",
    "        self.fit(X)\n",
    "        X_binned = self.transform(X)\n",
    "\n",
    "        return X_binned\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the important things about writing transformer objects for use in scikit-learn:\n",
    "* The class must have the following methods:\n",
    "    - `fit`: fit your training data\n",
    "    - `transform`: transform your training data into the new representation\n",
    "    - `predict`: predict new examples\n",
    "    - `score`: score predictions\n",
    "    - `fit_transform` is optional (I think)\n",
    "* The `__init__` method *only* sets up parameters. Don't put any relevant code in there (this is convention more than anything else, but it's a good one to follow!)\n",
    "* The `fit` method is always called in a `Pipeline` object (either on its own or as part of `fit_transform`). It usually modifies the internal state of the object, so returning `self` (i.e. the object itself) is usually fine.\n",
    "* For transformer objects, which don't need scoring and prediction methods, you can just return `pass` as above.\n",
    "\n",
    "**Exercise 8**: Last time, you learned that the SDSS photometric classifier uses a single hard cut to separate stars and galaxies in imaging data:\n",
    "$$\\mathtt{psfMag} - \\mathtt{cmodelMag} \\gt 0.145,$$\n",
    "sources that satisfy this criteria are considered galaxies.\n",
    "\n",
    "* Implement an object that takes $\\mathtt{psfMag}$ and $\\mathtt{cmodelMag}$ as inputs and has a free parameter `p` that sets the value above which a source is considered a galaxy. \n",
    "* Implement a `transform` methods that returns a single binary feature that is one if $$\\mathtt{psfMag} - \\mathtt{cmodelMag} \\gt p$$ and zero otherwise. \n",
    "* Add this feature to your optimized set of features consisting of either the PCA or your alternative representation, and run a random forest classifier on both. Run a CV on all components involved.\n",
    "\n",
    "*Hint*: $\\mathtt{psfMag}$ and $\\mathtt{cmodelMag}$ are the first and the last column in your feature vector, respectively.\n",
    "\n",
    "*Hint*: You can use [`FeatureUnion`](http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.FeatureUnion.html#sklearn.pipeline.FeatureUnion) to combine the outputs of two transformers in a single data set. (Note that using pipeline with all three will *chain* them, rather than compute the feature union, followed by a classifier). You can input your `FeatureUnion` object into `Pipeline`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class PSFMagThreshold(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self, p=1.45,):\n",
    "\n",
    "\n",
    "    def fit(self,X):\n",
    "   \n",
    "\n",
    "        \n",
    "    def transform(self, X):\n",
    "\n",
    "        \n",
    "\n",
    "    def predict(self, X):\n",
    "        \n",
    "        \n",
    "    def score(self, X):\n",
    "        \n",
    "        \n",
    "    def fit_transform(self, X, y=None):\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's make a feature set that combines this feature with the PCA features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "transformers = \n",
    "\n",
    "feat_union = \n",
    "\n",
    "X_transformed ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can build the pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# combine the transformers\n",
    "transformers = \n",
    "\n",
    "# make the feature union\n",
    "feat_union =\n",
    "\n",
    "# combine estimators for the pipeline\n",
    "estimators = \n",
    "\n",
    "# define the pipeline object\n",
    "pipe_c = \n",
    "\n",
    "# make the parameter set\n",
    "params = \n",
    "\n",
    "# perform the grid search\n",
    "grid_search_c = \n",
    "\n",
    "# complete the print statements:\n",
    "print(\"Best score: \")\n",
    "print(\"Best parameter set: \")\n",
    "print(\"Validation score: \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choosing The Right Scoring Function\n",
    "\n",
    "As a standard, the algorithms in `scikit-learn` use `accuracy` to score results. The accuracy is basically the raw fraction of correctly classified samples in your validation or test set. \n",
    "\n",
    "**Question**: Is this scoring function always the best method to use? Why (not)? Can you think of alternatives to use?\n",
    "\n",
    "Let's make a heavily biased data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# all stars\n",
    "star_ind = np.argwhere(y == b\"STAR\").T[0]\n",
    "# all galaxies\n",
    "galaxy_ind = np.argwhere(y == b\"GALAXY\").T[0]\n",
    "\n",
    "np.random.seed(100)\n",
    "# new array with much fewer stars\n",
    "star_ind_new = np.random.choice(star_ind, replace=False, size=int(len(star_ind)/80.0))\n",
    "\n",
    "X_new = np.vstack((X[galaxy_ind], X[star_ind_new]))\n",
    "y_new = np.hstack((y[galaxy_ind], y[star_ind_new]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now made a really imbalanced data set with many galaxies and only a few stars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(y_new[y_new == b\"GALAXY\"]))\n",
    "print(len(y_new[y_new == b\"STAR\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 10**: Run a logistic regression classifier on this data, for a very low regularization (0.0001) and a\n",
    "very large regularization (10000) parameter. Print the accuracy and a confusion matrix of the results for each run. How many mis-classified samples are in each? Where do the mis-classifications end up? If you were to run a cross validation on this, could you be sure to get a good model? Why (not)?\n",
    "\n",
    "As part of this exercise, you should plot a *confusion matrix*. A confusion matrix takes the true labels and the predicted labels, and then plots a grid for all samples where true labels and predicted labels match versus do not match. You can use the `scikit-learn` function `confusion_matrix` to create one. `pyplot.matshow` is useful for plotting it, but just printing it on the screen works pretty well, too (at least for the two classes considered here).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_new, y_new, \n",
    "                                                        test_size = 0.3, \n",
    "                                                        random_state = 20)\n",
    "\n",
    "C_all = \n",
    "\n",
    "for C in C_all:\n",
    "    \n",
    "    lr = \n",
    "    \n",
    "    # ... insert code here ...\n",
    "\n",
    "    # make predictions for the validation set\n",
    "    y_pred = \n",
    "\n",
    "    # print accuracy score for this regularization:\n",
    "    \n",
    "    # make and print a confusion matrix\n",
    "    cm = \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 11**: Take a look at the [metrics](http://scikit-learn.org/stable/modules/model_evaluation.html) implemented for model evaluation in `scikit-learn`, in particular the different versions of the [F1 score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score). Is there a metric that may be more suited to the task above? Which one? \n",
    "\n",
    "*Hint*: Our imbalanced class, the one we're interested in, is the STAR class. Make sure you set the keyword `pos_label` in the `f1_score` function correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for C in C_all:\n",
    "    \n",
    "    lr = \n",
    "    \n",
    "    # ... insert code here ...\n",
    "    \n",
    "    # predict the validdation set\n",
    "    y_pred = lr.predict(X_test2)\n",
    "\n",
    "    # print both accuracy and F1 score for comparison:\n",
    "    \n",
    "\n",
    "    # create and plot a confusion matrix:\n",
    "    cm = \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
