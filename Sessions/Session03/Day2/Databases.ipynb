{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, division, absolute_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Management Part 1\n",
    "\n",
    "For April 25, 2017\n",
    "* * *\n",
    "Yusra AlSayyad (Princeton University)\n",
    "\n",
    "This excerise demonstrates the power of spatial indexes in Databases and non-database SQL implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.spatial import Delaunay\n",
    "# from matplotlib.collections import LineCollection\n",
    "import os\n",
    "import copy\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1) Spatial Indexes in SkyServer\n",
    "\n",
    "In this problem, will to query SkyServer spatially (based on RA and Dec). You may enter the queries into http://skyserver.sdss.org/dr13/en/tools/search/sql.aspx or use astroquery (demonstrated in the database re-introduction yesterday). We are interested in the _approximate_ timing of the queries. Don't take query times too seriouslys since there are many factors in reponse time with a public database with many users. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We're interested in getting all the photometry for objects within 1 arcminute of \n",
    "(18.27, 0.94).  SkyServer has angular distance functions, for example:\n",
    "\n",
    "`dbo.fDistanceArcMinEq(ra1, dec1, ra2, dec2)`\n",
    "\n",
    "One might naively filter on all sources for which the distance from the source to (18.27, 0.94) is < 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ```-- BAD! Don't actually do this in real life! --\n",
    "SELECT COUNT(objid)\n",
    " FROM PhotoObjAll p\n",
    " WHERE dbo.fDistanceArcMinEq(18.27, 0.94, p.ra, p.dec) < 1;```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feel free to try out this query (you can cancel when you get tired of waiting.) It will compute the distance of each row in PhotoObjAll (N=1231051050\n",
    ") to the search coordinate. What are some ways to utilize the indices to cut down on the query time?\n",
    "\n",
    "_Hint: You can assume that the table has indices on ra, dec, and htmID._\n",
    "\n",
    "**Problem 1a** \n",
    "\n",
    "One way to improve query time is to pre-filter the data on an indexed column. Fill in an additional WHERE clause to filter the data on RA before computing the distance. How long does this take?\n",
    "\n",
    "```SELECT COUNT(objid)\n",
    " FROM PhotoObjAll p\n",
    " WHERE -- COMPLETE THIS LINE\n",
    " AND  dbo.fDistanceArcMinEq(18.27, 0.94, p.ra, p.dec) < 1```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1b**\n",
    "\n",
    "This probably improved runtime dramatically. But the database still had to compute the distance to a large number of rows. Write a query to find out how times it computed the distance in your query in problem 1a. Would be the effect prefiltering clause based on both ra and decl?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1c**\n",
    "\n",
    "We can do better by using the HTMID. SkyServer defines Table Valued Functions for spatial queries, [fGetNearbyObjEq](http://skyserver.sdss.org/dr13/en/help/browser/browser.aspx?cmd=description+fGetNearbyObjEq+F#&&history=description+fGetNearbyObjEq+F) and [fGetObjFromRectEq](http://skyserver.sdss.org/dr13/en/help/browser/browser.aspx?cmd=description+fGetObjFromRectEq+F#&&history=description+fGetObjFromRectEq+F).\n",
    "\n",
    "These functions return a list of the `ObjId` within the search area you defined. Like the demo it is behind the scenes it is joining on HTMID ranges and performing the join on the PhotoObjAll table. \n",
    "\n",
    " Inspect the results of:\n",
    " \n",
    "```Select * FROM fGetNearbyObjEq(18.27, 0.94, 1)```\n",
    " \n",
    " \n",
    "Complete this query to get the photometry from PhotoObjAll utilizing `fGetNearbyObjEq(18.27, 0.94, 1)`:\n",
    "\n",
    "```SELECT p.objid,\n",
    "   p.run, p.rerun, p.camcol, p.field, p.obj,\n",
    "   p.type, p.ra, p.dec, p.u, p.g, p.r, p.i, p.z,\n",
    "   p.Err_u, p.Err_g, p.Err_r, p.Err_i, p.Err_z\n",
    "   FROM PhotoObjAll p\n",
    "   INNER JOIN fGetNearbyObjEq(18.27, 0.94, 1) n\n",
    "   ON -- COMPLETE```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 1d - revisiting the challenge problem from yesterday**\n",
    "\n",
    "The challenge problem yesterday relied on the table `ROSAT` which is the _result_ of a  spatial cross match between `PhotoPrimary` and the positions of ROSAT sources.  Perform your own spatial cross match between the RA/DEC in the ROSAT table and the PhotoPrimary view. \n",
    "\n",
    "How many rows in the ROSAT table, have a source in the `PhotoPrimary` table within 1 arcmin?\n",
    "\n",
    "_HINT: Use SkyServer function: dbo.fGetNearestObjIdEq() answer is 47070 for DR13 and should take < 10_ seconds\n",
    "\n",
    "Finally, augment this query to return the TOP 10 of these matches with their ROSAT.SOURCENAME and the ugriz photometry from PhotoPrimary. Write your query below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2) SQL operations with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas supports SQL operations on relations or tables called DataFrames. If your data can fit in memory, or it can be broken into chunks that fit in memory, this is a great way to manipulate your tables in python. :)\n",
    "\n",
    "For this problem, you may want to make use of the pandas documentation: http://pandas.pydata.org/pandas-docs/stable/ for syntax\n",
    "\n",
    "\n",
    "We're going to go through some basic operations on DataFrames: Select, Join and Group By. First lets load a dataset of i-band photometry performed on a patch of a coadd from Stripe 82 re-processed with the LSST DM stack. \n",
    "\n",
    "** Setup **\n",
    "\n",
    "Download the datafile and load it into a DataFrame\n",
    "\n",
    "```\n",
    "$ curl -O https://lsst-web.ncsa.illinois.edu/~yusra/survey_area/DeepSourceAll_i_lt300_narrow_32610.csv.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Please forgive my LSSTDM/C++ style variable names\n",
    "\n",
    "# CHANGE IF YOU NEED:\n",
    "DATA_DIR = '.'  # Path to datafile wherever you put the datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getSourceDataFrame():\n",
    "    TYPE = {'deepSourceId': 'int64',\n",
    "            'parentDeepSourceId': str,\n",
    "            'deepCoaddId': int,\n",
    "            'ra': float,\n",
    "            'decl': float,\n",
    "            'psfMag': float,\n",
    "            'psfMagSigma': float,\n",
    "            'tract': int,\n",
    "            'patch': str,\n",
    "            'detect_is_primary': int\n",
    "            }\n",
    "    df = pd.read_csv(os.path.join(DATA_DIR, 'DeepSourceAll_i_lt300_narrow_32610.csv.gz'),\n",
    "                     index_col=0, dtype=TYPE, compression='gzip')\n",
    "    # replaced the NULLs with -1. pandas can't represent NULLS in integer columns\n",
    "    # (only NaNs in float columns or Nones in Object columns)\n",
    "    df['deepSourceId'] = df['deepSourceId'].astype('int64')\n",
    "    df['parentDeepSourceId'].loc[df['parentDeepSourceId'].isnull()] = -1\n",
    "    df['parentDeepSourceId'] = df['parentDeepSourceId'].astype(float).astype(int)\n",
    "    df.index = df['deepSourceId']\n",
    "    return df\n",
    "\n",
    "df = getSourceDataFrame()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 2a Projection and Selection**\n",
    "\n",
    "Recall that in SQL, we can SELECT particular _columns_ and particular _rows_. \n",
    "\n",
    "In pandas there are different interfaces for this: http://pandas.pydata.org/pandas-docs/stable/indexing.html#\n",
    "\n",
    "This includes some syntax thats intuitive for numpy. For example, you can select columns like:\n",
    "```\n",
    "df['psfMag']\n",
    "df[['psfMag', 'psfMagSigma']] # or even\n",
    "df.psfMag\n",
    "```\n",
    "\n",
    "and rows like:\n",
    "```df[df.psfMag < 18]```\n",
    "\n",
    "Make a scatter plot of `ra` and `dec` with two separate colors for `detect_is_primary` 0 or 1. \n",
    "\n",
    "Note: The photometry pipeline deblended all multipeak detections (parents) into its component peaks (children). If a source was deblended, it was marked `detect_is_primary = 0`. The children flux measurements were entered into the table, `detect_is_primary` marked 1, and the `deepSourceId` of their parent put in the `parentDeepSourceId` column. All `deepSourceIds` in the `parentDeepSourceId` column are also in `deepSourceId` column (but with `detect_is_primary`=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# e.g.\n",
    "plt.scatter( #\n",
    "plt.scatter( #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 2b Selection and aggregation**\n",
    "\n",
    "How many \"primary\" sources (that is rows  with `detect_is_primary = 1`) have a psfMag between 19 and 20? \n",
    "Can you do this in one line?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df[ #COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 2c Group By ** \n",
    "\n",
    "Pandas also provides grouping: http://pandas.pydata.org/pandas-docs/stable/groupby.html\n",
    "\n",
    "Count the number of primary vs. non-primary sources using `groupby`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "grouped = df.groupby(#COMPLETE\n",
    "grouped.#COMPLETE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use `groupby` again to make a histogram of sources in integer bins of `psfMag` for only the primary sources. Plot the log number counts vs. psfMag you get. What's the approximate depth of this dataset in mags? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "grouped = df[# COMPLETE\n",
    "counts = grouped.deepSourceId.# COMPLETE\n",
    "plt.plot(counts.index, # COMPLETE\n",
    "\n",
    "plt.xlabel(\"PsfMag\")\n",
    "plt.ylabel(\"log10 Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 2d Group By with UDF** \n",
    "One particularly power feature of pandas is the ease with which you can apply any function to a group (an aggregate user defined function).  Remember all your data is in memory, so there's no need to limit your self to commuting operations like count, mean, sum, first and last. \n",
    "\n",
    "Write a function to be applied to the `psfMag` bins. Using your same groups from above, use your new function as the aggregate. \n",
    "\n",
    "_Your function doesn't have to make sense. One idea would be the interquartile range._\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_function(arr):\n",
    "    # where arr is array-like\n",
    "    # your code here\n",
    "    return value # as single value\n",
    "\n",
    "grouped. # Apply your new aggregate UDF new_fuction to your groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Problem 2d Join ** \n",
    "\n",
    "Backstory: analysis has revealed that the photometry is bad for any children of parent sources that are brighter than 18th magnitude.  Too much scattered light from the background. We want to remove all sources that have a parent brighter than 18th mag. \n",
    "\n",
    "Fortunately, pandas provides joins. Pandas `DataFrames` have two methods `merge` and `join` which both implement joins. There is also a static `pd.merge()`.  Use any of these to join the DataFrame `df` to itself, in order to add the `psfMag` of the parent to the row. Call it \"parent_psfMag\". The result should be a new DataFrame `dfJoined`.\n",
    "\n",
    "_What type of Join? We want the resulting table to have the same number of rows as `df` regardlesss if a source has a parent or not._\n",
    "\n",
    "_HINT: In pandas, deepSourceId isn't a column, but an \"index.\" As a result, the arguments to merge or join are not as symmetric as writing SQL. If you use pd.merge, user `right_index=True`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfJoined = # complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now select just the rows  where `detect_is_primary=1` and `parent_psfMag` is either null or fainter than 18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfGood = \n",
    "dfGood.count() #should return 4088"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem 2e (BONUS)**\n",
    "If you're done with the challenge problem, I encourage you to download Topcat and reproduce your solutions to Problem 2 with topcat: http://www.star.bris.ac.uk/~mbt/topcat/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Problem: Graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal in this problem is to convert the histogram you made above into a density in units of deg$^{-2}$mag$^{-1}$. But to do this we need to compute the survey area.\n",
    "The scatter plot of the positions show some pretty significant gaps in the area accessed by the survey, because of large galaxies in the field of view.  We want to compute this area, not including the gaps. \n",
    "\n",
    "\n",
    "First let's make a Delaunay triangulation of all the good points, and delete all the triangles that are bigger than a certain cutoff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RUN THIS. No need to change anything. \n",
    "\n",
    "def isTriangleLargerThan(triangle, cutoff=0.006):\n",
    "        pa, pb, pc = triangle\n",
    "        a = np.sqrt((pa[0] - pb[0])**2 + (pa[1] - pb[1])**2)\n",
    "        b = np.sqrt((pb[0] - pc[0])**2 + (pb[1] - pc[1])**2)\n",
    "        c = np.sqrt((pc[0] - pa[0])**2 + (pc[1] - pa[1])**2)\n",
    "        s = (a + b + c) / 2.0\n",
    "        area = np.sqrt(s * (s - a) * (s - b) * (s - c))\n",
    "        circum_r = a * b * c / (4.0 * area)\n",
    "        return circum_r < cutoff\n",
    "\n",
    "allpoints = dfGood[['ra', 'decl']].values\n",
    "# coords = copy.copy(allpoints)\n",
    "tri = Delaunay(allpoints)\n",
    "triangles = allpoints[tri.simplices]\n",
    "\n",
    "\n",
    "idx = [isTriangleLargerThan(triangle) for triangle in triangles]\n",
    "\n",
    "# Take note of these options for our discussion tomorrow on MapReduce\n",
    "# idx = map(isTriangleLargerThan, triangles)\n",
    "# onlySmallTriangles = filter(isTriangleLargerThan, triangles)\n",
    "\n",
    "onlySmallTriangles = tri.simplices[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a collection of triangles. Each triangle is made up of 3 vertices, but it is also made up of 3 edges. Lets convert this list of triangles into a bag of edges. There will be duplicates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "edges = np.vstack((onlySmallTriangles[:,0:2], onlySmallTriangles[:,1:3], onlySmallTriangles[:,[0,2]]))\n",
    "edges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to find the boundary of the whole structure.  How do we know if an edge is an outer edge or inner edge? Outer edges  will only appear once in the bag. MOST of the edges were part of 2 triangles and will appear twice. \n",
    "\n",
    "_We don't care about direction. So edge 4 -> 8 is the same as 8 -> 4_\n",
    "\n",
    "Find all the edges that only appear ONCE in this bag of edges. There are many ways to do this. In the spirit of SQL, this sounds like a: `... GROUP BY HAVING COUNT(*) = 1`.  Pandas will do that for you. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfEdges = pd.DataFrame(edges)\n",
    "\n",
    "# COMPLETE\n",
    "\n",
    "singleEdges = #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Shapely can turn a bag of edges into a polygon. \n",
    "# If you haven't installed shapely, do that now.\n",
    "\n",
    "from shapely.geometry import MultiLineString\n",
    "lines = MultiLineString([((line[0][0], line[0][1]),\n",
    "                          (line[1][0], line[1,1])) for line in allpoints[singleEdges]])\n",
    "polygonList = list(polygonize(lines))\n",
    "print(\"The polygon's area is: %s\" % (polygonList[0].area))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE: This procedure measured euclidean area rather than solid angle, why is that OK here for this dataset?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
