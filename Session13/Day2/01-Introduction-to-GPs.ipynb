{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to Gaussian Processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"figure.figsize\"] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Gaussian Distribution\n",
    "\n",
    "In this notebook, we'll go over the very basics of Gaussian Processes (GPs) and how to construct and draw samples from them. But first, let's review some stuff about the Gausssian distribution $-$ the familiar bell curve $-$ itself.\n",
    "\n",
    "We're probably all familiar with ``numpy``'s built-in ``np.random.randn()`` function, which draws a sample from the standard normal $\\mathcal{N}(0, 1)$, i.e., a Gaussian with zero mean and unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1)\n",
    "u = np.random.randn(100000)\n",
    "plt.hist(u, bins=50, density=1);\n",
    "plt.axvline(0, color=\"C1\", ls=\"--\")\n",
    "y = np.exp(-0.5) / (np.sqrt(2 * np.pi))\n",
    "plt.plot([-1, 1], [y, y], \"C1-\")\n",
    "plt.text(0.1, 0.05, r\"$\\mu$\", color=\"C1\", fontsize=20);\n",
    "plt.text(0.3, 0.26, r\"$\\sigma$\", color=\"C1\", fontsize=20);\n",
    "plt.text(-0.5, 0.26, r\"$\\sigma$\", color=\"C1\", fontsize=20);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To draw from a Gaussian distribution with a different mean $\\mu$, we simply *add* $\\mu$ to $u$, and to draw from a distribution with different variance $\\sigma^2$, we simply *multiply* $u$ by $\\sigma$. We can therefore draw from a Gaussian with mean (say) $10.0$ and variance $5.0^2$ by running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "u = np.random.randn(100000)\n",
    "plt.hist(10.0 + 5.0 * u, bins=50, density=1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As I mentioned in my lecture, in a single dimension, the probability density function for a Gaussian with mean $\\mu$ and variance $\\sigma^2$ is\n",
    "\n",
    "$$\n",
    "p(y | \\mu, \\sigma^2) = \n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n",
    "\\exp\\left[\n",
    "-\\frac{1}{2}\\left(\\frac{y - \\mu}{\\sigma}\\right)^2\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "In Exercise 1 below, we'll derive the expression for a Gaussian in $N$ dimensions. We'll do it for the case where there isn't any covariance across the $N$ dimensions, in which case the probability density is obtained by multiplying the expression above $N$ times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 1</h1>\n",
    "</div>\n",
    "\n",
    "Show (numerically) that the probability function for an uncorrelated Gaussian in $N$ dimensions\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \n",
    "\\prod_{n=0}^{N-1}\n",
    "\\frac{1}{\\sqrt{2\\pi\\sigma_n^2}}\n",
    "\\exp\\left[\n",
    "-\\frac{1}{2}\\left(\\frac{y_n - \\mu_n}{\\sigma_n}\\right)^2\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "can be written in vector form as\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \n",
    "\\frac{1}{(2\\pi)^N \\mathrm{det}(\\mathbf{\\Sigma})}\n",
    "\\exp\\left[\n",
    "-\\frac{1}{2}\n",
    "(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\n",
    "\\mathbf{\\Sigma}^{-1}\n",
    "(\\mathbf{y} - \\boldsymbol{\\mu})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma} =\n",
    "\\begin{pmatrix}\n",
    " \\sigma_0^2 & 0 & 0 \\\\\n",
    " 0 & \\sigma_1^2 & 0 \\\\\n",
    " 0 & 0 & \\ddots\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "is a diagonal covariance matrix. You can use the following random inputs to test the equivalence of the two expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 5\n",
    "y = np.array([1.0, 2.0, 0.5, 0.75, 1.1])\n",
    "mu = np.array([0.7, 1.5, 0.8, 0.7, 1.3])\n",
    "sigma = np.array([0.3, 0.5, 0.5, 0.4, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though we derived the expression for the probability of a multidimensional Gaussian assuming no covariance between the different dimensions, the expression in Exercise 1 is general. In the most general case, the covariance matrix is allowed to have off-diagonal elements:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma} =\n",
    "\\begin{pmatrix}\n",
    " \\mathrm{Var(x_0)} & \\mathrm{Cov(x_0, x_1)} & \\cdots & \\mathrm{Cov(x_0, x_N)} \\\\ \\\\\n",
    " \\mathrm{Cov(x_1, x_0)} & \\mathrm{Var(x_1)} & \\cdots & \\mathrm{Cov(x_1, x_N)} \\\\ \\\\\n",
    " \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\\\\n",
    " \\mathrm{Cov(x_N, x_0)} & \\mathrm{Cov(x_N, x_1)} & \\cdots & \\mathrm{Var(x_N)}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Here, $\\mathrm{Var(x_i)}$ is the variance, or square of the standard deviation, along the $i^\\mathrm{th}$ dimension, and $\\mathrm{Cov(x_i, x_j)}$ is the covariance $-$ a measure of how two variables vary jointly $-$ between the $i^\\mathrm{th}$ and $j^\\mathrm{th}$ dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's hard to picture a multivariate Gaussian, but it helps to imagine what the contours of equal probability (or equipotential surfaces) look like. For a two dimensional standard normal with no covariance ($\\mathbf{\\Sigma} = \\mathbf{I}$), these are just circles. As I add to the off-diagonal terms of the matrix, I introduce a preferred direction in the space, since now the dimensions are correlated. The circle becomes an ellipse whose elongation increases with the covariance. In the general case of a multidimensional Gaussian, the contours form a multidimensional ellipsoid, elongated by different amounts along different dimensions.\n",
    "\n",
    "**So.** How do we draw samples from a multivariate Gaussian given $\\mathbf{\\Sigma}$? We use ``np.random.multivariate_normal``."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function ``draw_from_gaussian(mu, S, ndraws=1)`` that returns ``ndraws`` samples from a multivariate Gaussian with mean ``mu`` and covariance matrix ``S``. The shape of the output is ``(ndraws, ndim)`` where ``ndim`` is the dimension of the Gaussian:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_from_gaussian(mu, S, ndraws=1):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate gaussian\n",
    "    specified by covariance ``S`` and mean ``mu``.\n",
    "    \n",
    "    \"\"\"\n",
    "    return np.random.multivariate_normal(mu, S, (ndraws,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 2</h1>\n",
    "</div>\n",
    "\n",
    "Use the function above to draw ``10,000`` samples from a zero-mean Gaussian with covariance\n",
    "\n",
    "$$\n",
    "\\mathbf{\\Sigma} =\n",
    "\\begin{pmatrix}\n",
    " 1 & 0.5 \\\\\n",
    " 0.5 & 1\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Plot the \"corner\" plot for your ``samples`` using the ``corner`` package (``!pip install corner``):\n",
    "\n",
    "```python\n",
    "from corner import corner\n",
    "fig = corner(samples);\n",
    "```\n",
    "\n",
    "Vary the terms in the covariance matrix (recalling that it must be symmetric!) to get a sense of how they affect the joint distribution of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametrizing the covariance\n",
    "\n",
    "As the number of dimensions increases, and in particular as it becomes *infinite* (which it will, when we get to actual Gaussian Processes), it's no longer convenient to describe the covariance in terms of every single entry in the covariance matrix. Instead, it's useful to introduce the notion of a *kernel*, a function of just a few parameters that describes the overall structure of the covariance matrix.\n",
    "\n",
    "In general, the covariance matrix can have *any* structure, but quite often in timeseries analysis *data points close to each other in time are strongly correlated*. This is true whether your timeseries contains photometric variability from a rotating star, the rise and fall of a supernova light curve, a gravitational wave signal, or PSF shape changes due to temperature fluctuations on the detector. This is because each of these processes have characteristic timescales over which they operate, and on timescales shorter than that, all measurements you make are likely to be close to relatively each other. But if you wait long enough, what your system will do down the line will be pretty decoupled from what it is doing right now.\n",
    "\n",
    "So we can imagine defining a kernel that looks something like this: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><img src=\"kernel.png\"></img></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where the covariance peaks at zero time lag and drops smoothly to zero as the time lag increases. This particular kernel is extremely useful and has a name: the *Squared Exponential Kernel*, defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    k(t_i, t_j) = A^2 \\exp\\left(-\\frac{\\left(t_i - t_j\\right)^2}{2l^2}\\right)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a function ``ExpSquaredCovariance`` that returns the covariance matrix described by the squared exponential kernel for a timeseries ``t`` given an amplitude ``A`` and a timescale ``l``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpSquaredCovariance(t, A=1.0, l=1.0, tprime=None):\n",
    "    \"\"\"\n",
    "    Return the ``N x M`` exponential squared\n",
    "    covariance matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    if tprime is None:\n",
    "        tprime = t\n",
    "    TPrime, T = np.meshgrid(tprime, t)\n",
    "    return A ** 2 * np.exp(-0.5 * (T - TPrime) ** 2 / l ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about the `tprime` keyword for now -- we'll use it later in Exercise 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 3</h1>\n",
    "</div>\n",
    "\n",
    "For ``t = np.linspace(0, 2, 11)``, draw ``10,000`` samples from the Gaussian described by the covariance matrix above and plot the corresponding corner plot, labeling each subplot with the time point it represents. Comment on the structure of the plot and the correlations among the different dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Gaussians to Gaussian Processes\n",
    "\n",
    "So far we've been taking about Gaussians *distributions* and how they can jointly model a few random variables at a time. But what happens as the dimensionality of the Gaussian increases and eventually becomes infinite? The Gaussian no longer represents a collection of random variables, but instead the behaviour of a continuous function. And that's exactly what a Gaussian Process is: a distribution over functions with infinitely many points. We can't technically model a function with infinitely many points on a computer, but we can investigate what happens as the number of points becomes very large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 4</h1>\n",
    "</div>\n",
    "\n",
    "Construct a ``1000 x 1000`` covariance matrix using the squared exponential kernel for a timeseries spanning 10 time units, and plot an image of it using ``plt.imshow``. Comment on the structure of the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we draw ``30`` samples from the zero-mean Gaussian described by the covariance matrix you plotted in the previous exercise. These are shown as the transluscent black curves. The mean of the process is the solid blue line, and the 1 and 2 sigma levels are indicated with the blue shading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(0, 10, 1000)\n",
    "mu = np.zeros_like(t)\n",
    "S = ExpSquaredCovariance(t, A=1.0, l=1.0)\n",
    "np.random.seed(1)\n",
    "for i in range(30):\n",
    "    samples = draw_from_gaussian(mu, S)\n",
    "    plt.plot(t, samples.T, color=\"k\", alpha=0.1)\n",
    "\n",
    "mu_ = np.zeros_like(t)\n",
    "plt.plot(t, mu_, color=\"C0\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"GP\")\n",
    "std_ = np.sqrt(S.diagonal())\n",
    "plt.fill_between(t, mu_ - std_, mu_ + std_, color=\"C0\", alpha=0.3);\n",
    "plt.fill_between(t, mu_ - 2 * std_, mu_ + 2 * std_, color=\"C0\", alpha=0.15);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These curves are samples from the (infinitely large) family of functions described by your GP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 5</h1>\n",
    "</div>\n",
    "\n",
    "In the code above, vary the hyperparameters of the GP, $A$ and $l$, and comment on how they change the behavior of the GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditioning on data\n",
    "\n",
    "If you made it this far, you have actually implemented your own Gaussian Process! But it's still pretty simple, because so far we are drawing only from our *prior*: we know the Gaussian has a certain covariance and a certain mean (zero, in the case above), but nothing else about how it's supposed to behave. To apply this to a real dataset, we need to *condition* the GP on observations.\n",
    "\n",
    "Say I make two perfectly noise-free observations:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y(t = 2.5) &= 1.0 \\\\\n",
    "    y(t = 7.5) &= -1.0\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Again, $y$ could be anything: the flux received from a star, the strain due to a gravitational wave, etc. I want to draw new samples from my GP, but not just *any* samples: I explicitly want to draw functions that go through those two data points. One (very inefficient) way to do this is to draw a *ton* of samples and discard any samples that don't agree with the data. (In this particular case, that would be literally impossible, since I know $y(t)$ at those two points *exactly*: I will never draw at random a function whose value is exactly what I want it to be. But note that *rejection sampling* can actually be useful in other cases. Anyways.)\n",
    "\n",
    "A *better* way to do this is to consider the *joint distribution* of the observed data $y(t)$ (usually called the \"training set\") and the points where I'm trying to predict the value of the function $y_\\star(t_\\star)$ (the \"test set\"):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    " y \\\\\n",
    " y_\\star\n",
    "\\end{pmatrix}\n",
    "&=\n",
    "\\mathcal{N}\n",
    "\\left[\n",
    "\\mathbf{0},\n",
    "\\begin{pmatrix}\n",
    " \\mathbf{\\Sigma}(t, t) & \\mathbf{\\Sigma}(t, t_\\star)\\\\\n",
    " \\mathbf{\\Sigma}(t_\\star, t) & \\mathbf{\\Sigma}(t_\\star, t_\\star)\n",
    "\\end{pmatrix}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\mathbf{\\Sigma}(t, t)$ is the $N_\\mathrm{train} \\times N_\\mathrm{train}$ covariance matrix evaluated at all pairs of training points, $\\mathbf{\\Sigma}(t_\\star, t_\\star)$ is the $N_\\mathrm{test} \\times N_\\mathrm{test}$ covariance matrix evaluated at all pairs of test points, and the remaining two entries are the (rectangular) covariance matrices evaluated at all pairs of (test, training) points.\n",
    "\n",
    "Given this joint distribution, we can compute the *conditional* distribution of $y_\\star$ given $y$ with some linear algebra (for a derivation, see Appendix A.2 of [Rasmussen & Williams (2006)](http://www.gaussianprocess.org/gpml/chapters/RW.pdf) and references therein):\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y_\\star | y \\sim \\mathcal{N}\\left( \n",
    "    \\mathbf{\\Sigma}(t_\\star, t) \\mathbf{\\Sigma}(t, t)^{-1}y,\n",
    "    \\mathbf{\\Sigma}(t_\\star, t_\\star) - \n",
    "        \\mathbf{\\Sigma}(t_\\star, t) \\mathbf{\\Sigma}(t, t)^{-1} \\mathbf{\\Sigma}(t, t_\\star)\n",
    "\\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "In other words, given $y$, the distribution for $y_\\star$ is *still Gaussian*, but this time with mean equal to\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{\\Sigma}(t_\\star, t) \\mathbf{\\Sigma}(t, t)^{-1}y\n",
    "\\end{align}\n",
    "$$\n",
    "and covariance\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathbf{\\Sigma}(t_\\star, t_\\star) - \\mathbf{\\Sigma}(t_\\star, t) \\mathbf{\\Sigma}(t, t)^{-1} \\mathbf{\\Sigma}(t, t_\\star)\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 6</h1>\n",
    "</div>\n",
    "\n",
    "Convince yourself that when $t_\\star = t$, the mean and covariance of the GP for $y_\\star$ give you what you'd expect. When $t_\\star \\neq t$, what can you say about the variance of the GP after versus before data was collected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we define a function ``compute_gp`` that returns the mean and covariance of the Gaussian process at the test points (``y_test(t_test)``) conditioned on the values at the training points (``y_train(t_train)``). The ``**kwargs`` are for additional optional keyword arguments passed directly to the kernel (in this simple case, ``A`` and ``l``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gp(t_train, y_train, t_test, sigma=0, **kwargs):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute the required matrices\n",
    "    Stt = ExpSquaredCovariance(t_train, **kwargs)\n",
    "    Stt += sigma ** 2 * np.eye(Stt.shape[0])\n",
    "    Spp = ExpSquaredCovariance(t_test, **kwargs)\n",
    "    Spt = ExpSquaredCovariance(t_test, tprime=t_train, **kwargs)\n",
    "\n",
    "    # Compute the mean and covariance of the GP\n",
    "    mu = np.dot(Spt, np.linalg.solve(Stt, y_train))\n",
    "    S = Spp - np.dot(Spt, np.linalg.solve(Stt, Spt.T))\n",
    "    \n",
    "    return mu, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't worry about ``sigma`` yet -- we'll discuss this below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 7</h1>\n",
    "</div>\n",
    "\n",
    "Given\n",
    "\n",
    "```python\n",
    "t_train = np.array([2.5, 7.5])\n",
    "y_train = np.array([1.0, -1.0])\n",
    "t_test = np.linspace(0, 10, 1000)\n",
    "```\n",
    "\n",
    "compute the mean and covariance of the GP. Draw ``30`` samples from it as we did above. Plot the mean function and shade the 1- and 2-$\\sigma$ levels. Comment on the behavior of the GP. Does the GP correctly predict the training points? How does the variance behave close to and far from the training points? How does the GP behave very far from the training points?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we'll do regarding drawing from a GP is to account for noise in the observations. If the measurement error on the data points is Gaussian and uncorrelated (white), as we usually assume it to be, we simply add a term to the joint distribution of the training and test data:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\begin{pmatrix}\n",
    " y \\\\\n",
    " y_\\star\n",
    "\\end{pmatrix}\n",
    "&=\n",
    "\\mathcal{N}\n",
    "\\left[\n",
    "\\mathbf{0},\n",
    "\\begin{pmatrix}\n",
    " \\mathbf{\\Sigma}(t, t) + \\sigma_n^2 \\mathbf{I} & \\mathbf{\\Sigma}(t, t_\\star)\\\\\n",
    " \\mathbf{\\Sigma}(t_\\star, t) & \\mathbf{\\Sigma}(t_\\star, t_\\star)\n",
    "\\end{pmatrix}\n",
    "\\right]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\sigma_n$ is the standard deviation for the $n^\\mathrm{th}$ data point in the training set and $\\mathbf{I}$ is the identiy matrix. If you look at our function ``compute_gp`` above, you'll see that it accepts a ``sigma`` keyword, and that it adds its square to the $\\mathbf{\\Sigma}(t, t)$ term in the covariance, as shown above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 8</h1>\n",
    "</div>\n",
    "\n",
    "Re-plot the figure from **Exercise 7**, this time given an observational uncertainty $\\sigma = 0.25$ at both training points. Comment on the new behavior of the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GP optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous section, we learned how to construct and sample from a simple GP. This is useful for making predictions, i.e., interpolating or extrapolating based on the data you measured. But the true power of GPs comes from their application to *regression* and *inference*: given a dataset $D$ and a model $M(\\theta)$, what are the values of the model parameters $\\theta$ that are consistent with $D$?\n",
    "\n",
    "A very common use of GPs is to model things you don't have an explicit physical model for, so quite often they are used to model \"nuisances\" in the dataset. But just because you don't care about these nuisances doesn't mean they don't affect your inference: in fact, unmodelled correlated noise can often lead to strong biases in the parameter values you infer. In this notebook, we'll learn how to compute likelihoods of Gaussian Processes so that we can *marginalize* over the nuisance parameters (given suitable priors) and obtain unbiased estimates for the physical parameters we care about."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to the definition of the probability density function for a multivariate Gaussian:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{y} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = \n",
    "\\frac{1}{(2\\pi)^N \\mathrm{det}(\\mathbf{\\Sigma})}\n",
    "\\exp\\left[\n",
    "-\\frac{1}{2}\n",
    "(\\mathbf{y} - \\boldsymbol{\\mu})^\\top\n",
    "\\mathbf{\\Sigma}^{-1}\n",
    "(\\mathbf{y} - \\boldsymbol{\\mu})\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "It's usually easier to work in log-probability space since probabilities can typically be *very* small. Let's therefore take the log of this:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\ln p(\\mathbf{y} | \\boldsymbol{\\mu}, \\mathbf{\\Sigma}) = -\\frac{1}{2}(\\mathbf{y} - \\boldsymbol{\\mu})^\\top \\mathbf{\\Sigma}^{-1} (\\mathbf{y} - \\boldsymbol{\\mu}) - \\frac{1}{2}\\ln \\mathrm{det}\\,\\mathbf{\\Sigma} - \\frac{N}{2} \\ln 2\\pi\n",
    "\\end{align}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define a function to compute this log probability given our Squared Exponential covariance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln_gp_likelihood(t, y, sigma=0, A=1.0, l=1.0):\n",
    "    \"\"\"\n",
    "    Return the log of the GP likelihood for a datatset y(t)\n",
    "    with uncertainties sigma, modeled with a Squared Exponential\n",
    "    Kernel with amplitude A and lengthscale l.\n",
    "    \n",
    "    \"\"\"\n",
    "    # The covariance and its determinant\n",
    "    npts = len(t)\n",
    "    K = ExpSquaredCovariance(t, A=A, l=l) + sigma ** 2 * np.eye(npts)\n",
    "    \n",
    "    # The log marginal likelihood\n",
    "    log_like = -0.5 * np.dot(y.T, np.linalg.solve(K, y))\n",
    "    log_like -= 0.5 * np.linalg.slogdet(K)[1]\n",
    "    log_like -= 0.5 * npts * np.log(2 * np.pi)\n",
    "    \n",
    "    return log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is the probability of our data conditioned on our model, it is often referred to as a *likelihood*. Specifically, it is a *marginal likelihood*, since it's actually the likelihood of the data *marginalized* (integrated) over all the infinitely many curves in the family of functions defined by our GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 9</h1>\n",
    "</div>\n",
    "\n",
    "Consider the following dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "t, y, sigma = np.loadtxt(\"data/sample_data.txt\", unpack=True)\n",
    "plt.plot(t, y, \"k.\", alpha=0.5, ms=3)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log GP likelihood for this data, conditioned on different values of the lengthscale of the GP. Specifically, compute it for each of the following values:\n",
    "\n",
    "```python\n",
    "l = np.linspace(0.2, 0.4, 300)\n",
    "```\n",
    "\n",
    "Assume you know the amplitude `A` to be unity (the default), and use an observational uncertainty `sigma` of `0.05`. Then plot the likelihood as a function of $l$. To compute the likelihood from the log likelihood, do\n",
    "\n",
    "```python\n",
    "like = np.exp(lnlike - np.max(lnlike))\n",
    "```\n",
    "\n",
    "This will normalize things so that the maximum likelihood is unity. (If you don't do this, you might run into floating point underflow errors, since the numbers we're dealing with are extremely small!)\n",
    "\n",
    "Comment on your results. The true timescale of the dataset is $l = 0.3$. Were you able to correctly infer that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just performed your first GP inference problem! It was quite simple, since we assumed the data could be solely modeled with a GP. In practice, we usually have a base model (usually something that depends on physics, like a transit model) whose parameters we want to learn about; the GP is included as an additional model to capture the nuisance, correlated noise that's standing in the way of our measurement. We'll consider a problem like that in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference with a GP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timeseries below was generated by a linear function of time, $y(t)= mt + b$. In addition to observational uncertainty $\\sigma$ (white noise), there is a fair bit of correlated (red) noise, which we will assume is well described\n",
    "by the squared exponential covariance with a certain (unknown) amplitude $A$ and timescale $l$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t, y, sigma = np.loadtxt(\"data/sample_data_line.txt\", unpack=True)\n",
    "m_true, b_true, A_true, l_true = np.loadtxt(\"data/sample_data_line_truths.txt\", unpack=True)\n",
    "plt.errorbar(t, y, yerr=sigma, fmt=\"k.\", label=\"observed\")\n",
    "plt.plot(t, m_true * t + b_true, color=\"C0\", label=\"truth\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"data\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to estimate the values of $m$ and $b$, the slope and intercept of the line, respectively. Initially, we are going to **assume there is no correlated noise.** Our model for the $n^\\mathrm{th}$ datapoint is thus\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    y_n \\sim \\mathcal{N}(m t_n + b, \\sigma_n\\mathbf{I})\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "and the probability of the data given the model can be computed by calling our GP log-likelihood function:\n",
    "\n",
    "```python\n",
    "def lnprob(params):\n",
    "    m, b = params\n",
    "    model = m * t + b\n",
    "    return ln_gp_likelihood(t, y - model, sigma, A=0, l=1)\n",
    "```\n",
    "\n",
    "Note, importantly, that we are passing the **residual vector**, $y - (mt + b)$, to the GP, since above we coded up a zero-mean Gaussian process. We are therefore using the GP to model the **residuals** of the data after applying our physical model (the equation of the line).\n",
    "\n",
    "To estimate the values of $m$ and $b$ we could generate a fine grid in those two parameters and compute the likelihood at every point, as we did above. But since we'll soon be fitting for four parameters (in the next part), we might as well upgrade our inference scheme and use the ``emcee`` package to do Markov Chain Monte Carlo (MCMC). If you haven't used ``emcee`` before, check out the first few tutorials on the [documentation page](https://emcee.readthedocs.io/en/latest/). The basic setup for the problem is this:\n",
    "\n",
    "```python\n",
    "import emcee\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, nburn)   # nburn = 500 should do\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, nsteps);            # nsteps = 1000 should do\n",
    "```\n",
    "\n",
    "where ``nwalkers`` is the number of walkers (something like 20 or 30 is fine), ``ndim`` is the number of dimensions (2 in this case), and ``lnprob`` is the log-probability function for the data given the model. Finally, ``p0`` is a list of starting positions for each of the walkers. We're going to pick eyeballed values for $m$ and $b$, then add a small random number to each to generate different initial positions for each walker. This will initialize all walkers in a ball centered on some point, and as the chain progresses they'll diffuse out and begin to explore the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnprob(p):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    m, b = p\n",
    "    if (m < 0) or (m > 10):\n",
    "        return -np.inf\n",
    "    elif (b < 0) or (b > 30):\n",
    "        return -np.inf\n",
    "    model = m * t + b\n",
    "    lnlike = ln_gp_likelihood(t, y - model, sigma, A=0, l=1)\n",
    "    return lnlike"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "print(\"Using emcee version {0}\".format(emcee.__version__))\n",
    "\n",
    "initial = [4.0, 15.0]\n",
    "ndim = len(initial)\n",
    "nwalkers = 32\n",
    "p0 = initial + 1e-3 * np.random.randn(nwalkers, ndim)\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, lnprob)\n",
    "\n",
    "print(\"Running burn-in...\")\n",
    "p0, _, _ = sampler.run_mcmc(p0, 500, progress=True)\n",
    "sampler.reset()\n",
    "\n",
    "print(\"Running production...\")\n",
    "sampler.run_mcmc(p0, 1000, progress=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the chain finishes running, we're going to plot several draws from it on top of the data. We'll also plot the **true** line that generated the dataset (given by the variables ``m_true`` and ``b_true``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "plt.errorbar(t, y, yerr=sigma, fmt=\".k\", capsize=0)\n",
    "\n",
    "# The positions where the prediction should be computed\n",
    "x = np.linspace(0, 10, 500)\n",
    "\n",
    "# Plot 24 posterior samples\n",
    "samples = sampler.flatchain\n",
    "for s in samples[np.random.randint(len(samples), size=24)]:\n",
    "    m, b = s\n",
    "    model = m * x + b\n",
    "    plt.plot(x, model, color=\"#4682b4\", alpha=0.3)\n",
    "\n",
    "# Plot the truth\n",
    "plt.plot(x, m_true * x + b_true, \"C1\", label=\"truth\")\n",
    "    \n",
    "plt.ylabel(\"data\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.title(\"fit assuming uncorrelated noise\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also plot the corner plot to see how well we inferred the slope and the intercept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "labels = [\"slope\", \"intercept\"]\n",
    "truths = [m_true, b_true]\n",
    "corner.corner(sampler.flatchain, truths=truths, labels=labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great! Our slope is actually OK, but our intercept is quite wrong -- we underestimated it by more than 5 sigma!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise 10</h1>\n",
    "</div>\n",
    "\n",
    "This time, let's actually model the correlated noise. Re-define the ``lnprob`` function above to accept four parameters (slope, intercept, amplitude, and timescale). Restrict the fit to reasonable values of the amplitude (say $0 < A < 10$ and the lengthscale ($0 < l < 10$) by returning `-np.inf` from your log prior function for samples outside that range.\n",
    "\n",
    "You'll probably want to run your chains for a bit longer this time, too. As before, plot some posterior samples for the line, as well as the corner plot. How did you do this time? Is there any bias in your inferred values? How does the variance compare to the previous estimate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
