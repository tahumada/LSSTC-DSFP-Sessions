{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast GP implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import rcParams\n",
    "rcParams[\"figure.dpi\"] = 100\n",
    "rcParams[\"figure.figsize\"] = 12, 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking GP codes\n",
    "Implemented the right way, GPs can be super fast! Let's compare the time it takes to evaluate our GP likelihood and the time it takes to evaluate the likelihood computed with the snazzy ``george`` and ``celerite`` packages. We'll learn how to use both along the way. Let's create a large, fake dataset for these tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(0)\n",
    "t = np.linspace(0, 10, 10000)\n",
    "y = np.random.randn(10000)\n",
    "sigma = np.ones(10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ExpSquaredCovariance(t, A=1.0, l=1.0, tprime=None):\n",
    "    \"\"\"\n",
    "    Return the ``N x M`` exponential squared\n",
    "    covariance matrix.\n",
    "    \n",
    "    \"\"\"\n",
    "    if tprime is None:\n",
    "        tprime = t\n",
    "    TPrime, T = np.meshgrid(tprime, t)\n",
    "    return A ** 2 * np.exp(-0.5 * (T - TPrime) ** 2 / l ** 2)\n",
    "\n",
    "\n",
    "def ln_gp_likelihood(t, y, sigma=0, A=1.0, l=1.0):\n",
    "    \"\"\"\n",
    "    Return the log of the GP likelihood for a datatset y(t)\n",
    "    with uncertainties sigma, modeled with a Squared Exponential\n",
    "    Kernel with amplitude A and lengthscale l.\n",
    "    \n",
    "    \"\"\"\n",
    "    # The covariance and its determinant\n",
    "    npts = len(t)\n",
    "    K = ExpSquaredCovariance(t, A=A, l=l) + sigma ** 2 * np.eye(npts)\n",
    "    \n",
    "    # The log marginal likelihood\n",
    "    log_like = -0.5 * np.dot(y.T, np.linalg.solve(K, y))\n",
    "    log_like -= 0.5 * np.linalg.slogdet(K)[1]\n",
    "    log_like -= 0.5 * npts * np.log(2 * np.pi)\n",
    "    \n",
    "    return log_like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to evaluate the GP likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ln_gp_likelihood(t, y, sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### george"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time how long it takes to do the same operation using the ``george`` package (``pip install george``).\n",
    "\n",
    "The kernel we'll use is\n",
    "\n",
    "```python\n",
    "kernel = amp ** 2 * george.kernels.ExpSquaredKernel(tau ** 2)\n",
    "```\n",
    "\n",
    "where ``amp = 1`` and ``tau = 1`` in this case.\n",
    "\n",
    "To instantiate a GP using ``george``, simply run\n",
    "\n",
    "```python\n",
    "gp = george.GP(kernel)\n",
    "```\n",
    "\n",
    "The ``george`` package pre-computes a lot of matrices that are re-used in different operations, so before anything else, we'll ask it to compute the GP model for our timeseries:\n",
    "\n",
    "```python\n",
    "gp.compute(t, sigma)\n",
    "```\n",
    "\n",
    "Note that we've only given it the time array and the uncertainties, so as long as those remain the same, you don't have to re-compute anything. This will save you a lot of time in the long run!\n",
    "\n",
    "Finally, the log likelihood is given by ``gp.log_likelihood(y)``.\n",
    "\n",
    "How do the speeds compare? Did you get the same value of the likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import george"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kernel = george.kernels.ExpSquaredKernel(1.0)\n",
    "gp = george.GP(kernel)\n",
    "gp.compute(t, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "print(gp.log_likelihood(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``george`` also offers a fancy GP solver called the HODLR solver, which makes some approximations that dramatically speed up the matrix algebra. Let's instantiate the GP object again by passing the keyword ``solver=george.HODLRSolver`` and re-compute the log likelihood. How long did that take? Did we get the same value for the log likelihood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gp = george.GP(kernel, solver=george.HODLRSolver)\n",
    "gp.compute(t, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gp.log_likelihood(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### celerite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ``george`` package is super useful for GP modeling, and I recommend you read over the [docs and examples](https://george.readthedocs.io/en/latest/). It implements several different [kernels](https://george.readthedocs.io/en/latest/user/kernels/) that come in handy in different situations, and it has support for multi-dimensional GPs. But if all you care about are GPs in one dimension (in this case, we're only doing GPs in the time domain, so we're good), then ``celerite`` is what it's all about:\n",
    "\n",
    "```bash\n",
    "pip install celerite\n",
    "```\n",
    "\n",
    "Check out the [docs](https://celerite.readthedocs.io/en/stable/) here, as well as several tutorials. There is also a [paper](https://arxiv.org/abs/1703.09710) that discusses the math behind ``celerite``. The basic idea is that for certain families of kernels, there exist **extremely efficient** methods of factorizing the covariance matrices. Whereas GP fitting typically scales with the number of datapoints $N$ as $N^3$, ``celerite`` is able to do everything in order $N$ (!!!) This is a **huge** advantage, especially for datasets with tens or hundreds of thousands of data points. Using ``george`` or any homebuilt GP model for datasets larger than about ``10,000`` points is simply intractable, but with ``celerite`` you can do it in a breeze.\n",
    "\n",
    "Next we repeat the timing tests, but this time using ``celerite``. Note that the Exponential Squared Kernel is not available in ``celerite``, because it doesn't have the special form needed to make its factorization fast. Instead, we'll use the ``Matern 3/2`` kernel, which is qualitatively similar and can be approximated quite well in terms of the ``celerite`` basis functions:\n",
    "\n",
    "```python\n",
    "kernel = celerite.terms.Matern32Term(np.log(1), np.log(1))\n",
    "```\n",
    "\n",
    "Note that ``celerite`` accepts the **log** of the amplitude and the **log** of the timescale. Other than this, we can compute the likelihood using the same syntax as ``george``.\n",
    "\n",
    "How much faster did it run? Is the value of the likelihood different from what you found above? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import celerite\n",
    "from celerite import terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "kernel = terms.Matern32Term(np.log(1), np.log(1))\n",
    "gp = celerite.GP(kernel)\n",
    "gp.compute(t, sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gp.log_likelihood(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #D6EAF8; border-left: 15px solid #2E86C1;\">\n",
    "    <h1 style=\"line-height:2.5em; margin-left:1em;\">Exercise (the one and only)</h1>\n",
    "</div>\n",
    "\n",
    "Let's use what we've learned about GPs in a real application: fitting an exoplanet transit model in the presence of correlated noise.\n",
    "\n",
    "Here is a (fictitious) light curve for a star with a transiting planet: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t, y, yerr = np.loadtxt(\"data/sample_transit.txt\", unpack=True)\n",
    "plt.errorbar(t, y, yerr=yerr, fmt=\".k\", capsize=0)\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"relative flux\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a transit visible to the eye at $t = 0$, which (say) is when you'd expect the planet to transit if its orbit were perfectly periodic. However, a recent paper claims that the planet shows transit timing variations, which are indicative of a second, perturbing planet in the system, and that a transit at $t = 0$ can be ruled out at 3 $\\sigma$. **Your task is to verify this claim.**\n",
    "\n",
    "Assume you have no prior information on the planet other than the transit occurs in the observation window, the depth of the transit is somewhere in the range $(0, 1)$, and the transit duration is somewhere between $0.1$ and $1$ day. You don't know the exact process generating the noise, but you are certain that there's correlated noise in the dataset, so you'll have to pick a reasonable kernel and estimate its hyperparameters.\n",
    "\n",
    "\n",
    "Fit the transit with a simple inverted Gaussian with three free parameters:\n",
    "\n",
    "```python\n",
    "def transit_shape(depth, t0, dur):\n",
    "    return -depth * np.exp(-0.5 * (t - t0) ** 2 / (0.2 * dur) ** 2)\n",
    "```\n",
    "\n",
    "*HINT: I borrowed heavily from [this tutorial](https://celerite.readthedocs.io/en/stable/tutorials/modeling/) in the celerite documentation, so you might want to take a look at it...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
